{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>712.000000</td>\n",
       "      <td>696.0</td>\n",
       "      <td>65.440519</td>\n",
       "      <td>240.0</td>\n",
       "      <td>0.703359</td>\n",
       "      <td>-0.246387</td>\n",
       "      <td>...</td>\n",
       "      <td>712.000000</td>\n",
       "      <td>696.0</td>\n",
       "      <td>65.707979</td>\n",
       "      <td>240.0</td>\n",
       "      <td>0.699256</td>\n",
       "      <td>-0.277091</td>\n",
       "      <td>783.821985</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>712.000000</td>\n",
       "      <td>691.0</td>\n",
       "      <td>47.300458</td>\n",
       "      <td>158.0</td>\n",
       "      <td>0.884010</td>\n",
       "      <td>-0.332635</td>\n",
       "      <td>...</td>\n",
       "      <td>712.000000</td>\n",
       "      <td>691.0</td>\n",
       "      <td>47.483330</td>\n",
       "      <td>156.0</td>\n",
       "      <td>0.894198</td>\n",
       "      <td>-0.355280</td>\n",
       "      <td>380.188159</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>13.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1096.250000</td>\n",
       "      <td>1098.0</td>\n",
       "      <td>8.150920</td>\n",
       "      <td>28.0</td>\n",
       "      <td>-0.156675</td>\n",
       "      <td>-0.642487</td>\n",
       "      <td>...</td>\n",
       "      <td>1096.000000</td>\n",
       "      <td>1098.0</td>\n",
       "      <td>8.426150</td>\n",
       "      <td>28.0</td>\n",
       "      <td>-0.160466</td>\n",
       "      <td>-0.801230</td>\n",
       "      <td>808.359965</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1169.714286</td>\n",
       "      <td>1168.0</td>\n",
       "      <td>4.463000</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.642283</td>\n",
       "      <td>-0.722722</td>\n",
       "      <td>...</td>\n",
       "      <td>1169.428571</td>\n",
       "      <td>1168.0</td>\n",
       "      <td>4.237828</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.827427</td>\n",
       "      <td>-0.140248</td>\n",
       "      <td>1150.133430</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>585.333333</td>\n",
       "      <td>586.0</td>\n",
       "      <td>2.890598</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.196283</td>\n",
       "      <td>0.113400</td>\n",
       "      <td>...</td>\n",
       "      <td>585.333333</td>\n",
       "      <td>586.0</td>\n",
       "      <td>3.155243</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.354663</td>\n",
       "      <td>-0.484056</td>\n",
       "      <td>198.042444</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8343</th>\n",
       "      <td>0.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>619.466667</td>\n",
       "      <td>592.0</td>\n",
       "      <td>139.659522</td>\n",
       "      <td>480.0</td>\n",
       "      <td>0.480046</td>\n",
       "      <td>-0.842798</td>\n",
       "      <td>...</td>\n",
       "      <td>619.466667</td>\n",
       "      <td>594.0</td>\n",
       "      <td>139.562109</td>\n",
       "      <td>482.0</td>\n",
       "      <td>0.472791</td>\n",
       "      <td>-0.825913</td>\n",
       "      <td>468.155165</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8344</th>\n",
       "      <td>3.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>510.777778</td>\n",
       "      <td>512.0</td>\n",
       "      <td>7.091118</td>\n",
       "      <td>30.0</td>\n",
       "      <td>-0.059891</td>\n",
       "      <td>-0.269851</td>\n",
       "      <td>...</td>\n",
       "      <td>510.888889</td>\n",
       "      <td>512.0</td>\n",
       "      <td>6.870944</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.081059</td>\n",
       "      <td>-0.433829</td>\n",
       "      <td>309.210006</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>19.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8345</th>\n",
       "      <td>1.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1090.250000</td>\n",
       "      <td>1088.0</td>\n",
       "      <td>9.769212</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.213875</td>\n",
       "      <td>-1.152589</td>\n",
       "      <td>...</td>\n",
       "      <td>1090.250000</td>\n",
       "      <td>1088.0</td>\n",
       "      <td>8.742854</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.436427</td>\n",
       "      <td>-0.891096</td>\n",
       "      <td>940.155678</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8346</th>\n",
       "      <td>3.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>370.320000</td>\n",
       "      <td>370.0</td>\n",
       "      <td>3.133305</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.358316</td>\n",
       "      <td>-0.885061</td>\n",
       "      <td>...</td>\n",
       "      <td>370.400000</td>\n",
       "      <td>370.0</td>\n",
       "      <td>3.098387</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.051640</td>\n",
       "      <td>-0.703333</td>\n",
       "      <td>856.813033</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8347</th>\n",
       "      <td>1.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1037.000000</td>\n",
       "      <td>1037.0</td>\n",
       "      <td>28.372522</td>\n",
       "      <td>94.0</td>\n",
       "      <td>-0.267952</td>\n",
       "      <td>-0.810638</td>\n",
       "      <td>...</td>\n",
       "      <td>1037.000000</td>\n",
       "      <td>1037.0</td>\n",
       "      <td>28.705400</td>\n",
       "      <td>94.0</td>\n",
       "      <td>-0.237050</td>\n",
       "      <td>-0.876580</td>\n",
       "      <td>495.494523</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8348 rows × 124 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0     1    2     3            4       5           6      7         8  \\\n",
       "0     0.0  87.0  1.0  14.0   712.000000   696.0   65.440519  240.0  0.703359   \n",
       "1     2.0  49.0  1.0  13.0   712.000000   691.0   47.300458  158.0  0.884010   \n",
       "2     1.0  86.0  1.0   9.0  1096.250000  1098.0    8.150920   28.0 -0.156675   \n",
       "3     1.0  70.0  1.0   8.0  1169.714286  1168.0    4.463000   14.0  0.642283   \n",
       "4     3.0  61.0  0.0  16.0   585.333333   586.0    2.890598   12.0  0.196283   \n",
       "...   ...   ...  ...   ...          ...     ...         ...    ...       ...   \n",
       "8343  0.0  72.0  1.0  16.0   619.466667   592.0  139.659522  480.0  0.480046   \n",
       "8344  3.0  23.0  0.0  19.0   510.777778   512.0    7.091118   30.0 -0.059891   \n",
       "8345  1.0  51.0  0.0   9.0  1090.250000  1088.0    9.769212   30.0  0.213875   \n",
       "8346  3.0  45.0  0.0  26.0   370.320000   370.0    3.133305   10.0  0.358316   \n",
       "8347  1.0  28.0  1.0   9.0  1037.000000  1037.0   28.372522   94.0 -0.267952   \n",
       "\n",
       "             9  ...          114     115         116    117       118  \\\n",
       "0    -0.246387  ...   712.000000   696.0   65.707979  240.0  0.699256   \n",
       "1    -0.332635  ...   712.000000   691.0   47.483330  156.0  0.894198   \n",
       "2    -0.642487  ...  1096.000000  1098.0    8.426150   28.0 -0.160466   \n",
       "3    -0.722722  ...  1169.428571  1168.0    4.237828   14.0  0.827427   \n",
       "4     0.113400  ...   585.333333   586.0    3.155243   12.0  0.354663   \n",
       "...        ...  ...          ...     ...         ...    ...       ...   \n",
       "8343 -0.842798  ...   619.466667   594.0  139.562109  482.0  0.472791   \n",
       "8344 -0.269851  ...   510.888889   512.0    6.870944   28.0  0.081059   \n",
       "8345 -1.152589  ...  1090.250000  1088.0    8.742854   28.0  0.436427   \n",
       "8346 -0.885061  ...   370.400000   370.0    3.098387   12.0  0.051640   \n",
       "8347 -0.810638  ...  1037.000000  1037.0   28.705400   94.0 -0.237050   \n",
       "\n",
       "           119          120       121   122   123  \n",
       "0    -0.277091   783.821985  0.785714   9.0   6.0  \n",
       "1    -0.355280   380.188159  1.000000  13.0  12.0  \n",
       "2    -0.801230   808.359965  1.000000   8.0   6.0  \n",
       "3    -0.140248  1150.133430  1.000000   8.0   7.0  \n",
       "4    -0.484056   198.042444  1.000000  16.0  15.0  \n",
       "...        ...          ...       ...   ...   ...  \n",
       "8343 -0.825913   468.155165  1.000000  16.0   8.0  \n",
       "8344 -0.433829   309.210006  1.000000  19.0  18.0  \n",
       "8345 -0.891096   940.155678  1.000000   9.0   8.0  \n",
       "8346 -0.703333   856.813033  0.461538   1.0   6.0  \n",
       "8347 -0.876580   495.494523  1.000000   9.0   8.0  \n",
       "\n",
       "[8348 rows x 124 columns]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"../data_train_125ft.csv\")\n",
    "df_train.drop(columns=[\"Unnamed: 0\"],inplace=True)\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1091.750000</td>\n",
       "      <td>1091.0</td>\n",
       "      <td>22.280877</td>\n",
       "      <td>66.0</td>\n",
       "      <td>0.493022</td>\n",
       "      <td>-0.931472</td>\n",
       "      <td>...</td>\n",
       "      <td>1092.000000</td>\n",
       "      <td>1091.0</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>66.0</td>\n",
       "      <td>0.521225</td>\n",
       "      <td>-0.913087</td>\n",
       "      <td>373.995302</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1115.000000</td>\n",
       "      <td>1106.0</td>\n",
       "      <td>59.841457</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.777480</td>\n",
       "      <td>-0.423745</td>\n",
       "      <td>...</td>\n",
       "      <td>1115.000000</td>\n",
       "      <td>1107.0</td>\n",
       "      <td>60.728906</td>\n",
       "      <td>192.0</td>\n",
       "      <td>0.772436</td>\n",
       "      <td>-0.395166</td>\n",
       "      <td>477.271172</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>806.000000</td>\n",
       "      <td>804.0</td>\n",
       "      <td>8.045326</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.293282</td>\n",
       "      <td>-0.392690</td>\n",
       "      <td>...</td>\n",
       "      <td>806.000000</td>\n",
       "      <td>804.0</td>\n",
       "      <td>7.722458</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.198976</td>\n",
       "      <td>-0.896193</td>\n",
       "      <td>277.972651</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>458.300000</td>\n",
       "      <td>458.0</td>\n",
       "      <td>1.705872</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.192180</td>\n",
       "      <td>-0.770964</td>\n",
       "      <td>...</td>\n",
       "      <td>458.300000</td>\n",
       "      <td>458.0</td>\n",
       "      <td>1.926136</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.368600</td>\n",
       "      <td>-0.876527</td>\n",
       "      <td>361.998395</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>941.555556</td>\n",
       "      <td>1054.0</td>\n",
       "      <td>220.140920</td>\n",
       "      <td>628.0</td>\n",
       "      <td>-1.482314</td>\n",
       "      <td>0.419095</td>\n",
       "      <td>...</td>\n",
       "      <td>1059.250000</td>\n",
       "      <td>1059.0</td>\n",
       "      <td>10.341059</td>\n",
       "      <td>34.0</td>\n",
       "      <td>-0.061463</td>\n",
       "      <td>-0.843856</td>\n",
       "      <td>586.261896</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2083</th>\n",
       "      <td>3.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>485.789474</td>\n",
       "      <td>486.0</td>\n",
       "      <td>3.104817</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-0.415255</td>\n",
       "      <td>-0.874744</td>\n",
       "      <td>...</td>\n",
       "      <td>485.789474</td>\n",
       "      <td>486.0</td>\n",
       "      <td>2.966106</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-0.496663</td>\n",
       "      <td>-0.494835</td>\n",
       "      <td>798.662084</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2084</th>\n",
       "      <td>1.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1060.250000</td>\n",
       "      <td>1056.0</td>\n",
       "      <td>14.677789</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.273677</td>\n",
       "      <td>-1.204916</td>\n",
       "      <td>...</td>\n",
       "      <td>1060.250000</td>\n",
       "      <td>1056.0</td>\n",
       "      <td>13.872184</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.231747</td>\n",
       "      <td>-1.352375</td>\n",
       "      <td>603.272611</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2085</th>\n",
       "      <td>1.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1070.250000</td>\n",
       "      <td>1076.0</td>\n",
       "      <td>33.082284</td>\n",
       "      <td>98.0</td>\n",
       "      <td>-0.016678</td>\n",
       "      <td>-1.320021</td>\n",
       "      <td>...</td>\n",
       "      <td>1070.250000</td>\n",
       "      <td>1075.0</td>\n",
       "      <td>32.686962</td>\n",
       "      <td>96.0</td>\n",
       "      <td>0.030170</td>\n",
       "      <td>-1.329810</td>\n",
       "      <td>692.057509</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2086</th>\n",
       "      <td>1.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1021.750000</td>\n",
       "      <td>1018.0</td>\n",
       "      <td>7.171994</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.898849</td>\n",
       "      <td>-0.581729</td>\n",
       "      <td>...</td>\n",
       "      <td>1021.750000</td>\n",
       "      <td>1020.0</td>\n",
       "      <td>7.101936</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.839867</td>\n",
       "      <td>-0.281413</td>\n",
       "      <td>456.052143</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2087</th>\n",
       "      <td>1.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1079.500000</td>\n",
       "      <td>1089.0</td>\n",
       "      <td>20.826666</td>\n",
       "      <td>54.0</td>\n",
       "      <td>-0.460865</td>\n",
       "      <td>-1.486785</td>\n",
       "      <td>...</td>\n",
       "      <td>1079.750000</td>\n",
       "      <td>1087.0</td>\n",
       "      <td>21.574001</td>\n",
       "      <td>60.0</td>\n",
       "      <td>-0.405116</td>\n",
       "      <td>-1.328357</td>\n",
       "      <td>324.178086</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2088 rows × 124 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0     1    2     3            4       5           6      7         8  \\\n",
       "0     1.0  32.0  0.0   9.0  1091.750000  1091.0   22.280877   66.0  0.493022   \n",
       "1     1.0  17.0  0.0   9.0  1115.000000  1106.0   59.841457  186.0  0.777480   \n",
       "2     2.0  64.0  0.0  12.0   806.000000   804.0    8.045326   30.0  0.293282   \n",
       "3     3.0  61.0  0.0  21.0   458.300000   458.0    1.705872    6.0  0.192180   \n",
       "4     1.0  70.0  1.0  10.0   941.555556  1054.0  220.140920  628.0 -1.482314   \n",
       "...   ...   ...  ...   ...          ...     ...         ...    ...       ...   \n",
       "2083  3.0  71.0  0.0  20.0   485.789474   486.0    3.104817   10.0 -0.415255   \n",
       "2084  1.0  51.0  1.0   9.0  1060.250000  1056.0   14.677789   44.0  0.273677   \n",
       "2085  1.0  61.0  1.0   9.0  1070.250000  1076.0   33.082284   98.0 -0.016678   \n",
       "2086  1.0  74.0  1.0   9.0  1021.750000  1018.0    7.171994   22.0  0.898849   \n",
       "2087  1.0  63.0  0.0   9.0  1079.500000  1089.0   20.826666   54.0 -0.460865   \n",
       "\n",
       "             9  ...          114     115        116    117       118  \\\n",
       "0    -0.931472  ...  1092.000000  1091.0  22.000000   66.0  0.521225   \n",
       "1    -0.423745  ...  1115.000000  1107.0  60.728906  192.0  0.772436   \n",
       "2    -0.392690  ...   806.000000   804.0   7.722458   26.0  0.198976   \n",
       "3    -0.770964  ...   458.300000   458.0   1.926136    6.0  0.368600   \n",
       "4     0.419095  ...  1059.250000  1059.0  10.341059   34.0 -0.061463   \n",
       "...        ...  ...          ...     ...        ...    ...       ...   \n",
       "2083 -0.874744  ...   485.789474   486.0   2.966106   10.0 -0.496663   \n",
       "2084 -1.204916  ...  1060.250000  1056.0  13.872184   40.0  0.231747   \n",
       "2085 -1.320021  ...  1070.250000  1075.0  32.686962   96.0  0.030170   \n",
       "2086 -0.581729  ...  1021.750000  1020.0   7.101936   24.0  0.839867   \n",
       "2087 -1.486785  ...  1079.750000  1087.0  21.574001   60.0 -0.405116   \n",
       "\n",
       "           119         120  121   122   123  \n",
       "0    -0.913087  373.995302  1.0   9.0   7.0  \n",
       "1    -0.395166  477.271172  1.0   9.0   8.0  \n",
       "2    -0.896193  277.972651  1.0  12.0  11.0  \n",
       "3    -0.876527  361.998395  1.0   0.0  20.0  \n",
       "4    -0.843856  586.261896  1.0   9.0   7.0  \n",
       "...        ...         ...  ...   ...   ...  \n",
       "2083 -0.494835  798.662084  1.0  10.0  19.0  \n",
       "2084 -1.352375  603.272611  1.0   9.0   8.0  \n",
       "2085 -1.329810  692.057509  1.0   9.0   4.0  \n",
       "2086 -0.281413  456.052143  1.0   9.0   8.0  \n",
       "2087 -1.328357  324.178086  1.0   9.0   8.0  \n",
       "\n",
       "[2088 rows x 124 columns]"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv(\"../data_test_125ft.csv\")\n",
    "df_test.drop(columns=[\"Unnamed: 0\"],inplace=True)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = df_train.iloc[:,1:].values\n",
    "y_train = df_train.iloc[:,0].values\n",
    "x_test = df_test.iloc[:,1:].values\n",
    "y_test = df_test.iloc[:,0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = MinMaxScaler()\n",
    "x_train = scale.fit_transform(x_train)\n",
    "x_test = scale.fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (4174, 123)\n",
      "Vallidation: (4174, 123)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train , test_size=0.5, shuffle=True, stratify=y_train, random_state=119)\n",
    "print(f\"Train: {x_train.shape}\")\n",
    "print(f\"Vallidation: {x_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier(criterion= 'log_loss', max_depth= 5, max_features= 'sqrt', n_estimators= 1000)\n",
    "ab_clf = AdaBoostClassifier(algorithm= 'SAMME.R', learning_rate= 0.1, n_estimators= 50)\n",
    "knn_clf = KNeighborsClassifier(algorithm= 'auto', n_neighbors= 5, p= 1, weights= 'uniform')\n",
    "svc_clf = SVC(C= 100, gamma= 'scale', kernel= 'rbf', probability= True)\n",
    "xgb_clf = XGBClassifier(gamma= 0,learning_rate= 0.1,max_depth= 5,min_child_weight= 1,n_estimators= 1000)\n",
    "dt_clf = DecisionTreeClassifier(criterion= 'entropy',max_depth= 5,max_features= 'sqrt',splitter= 'best')\n",
    "lgb_clf = LGBMClassifier(boosting = 'gbdt', data_sample_strategy= 'goss', estimators=50, learning_rate = 0.1, objective= 'multiclass')\n",
    "cb_clf = CatBoostClassifier(iterations = 10, learning_rate= 0.1)\n",
    "gb_clf = GradientBoostingClassifier(criterion='squared_error', learning_rate = 0.1, loss= 'log_loss', n_estimators=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-5 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-5 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-5 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-5 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-5 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-5 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-5 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-5 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-5 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-5 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-5 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-5 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-5 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(criterion=&#x27;entropy&#x27;, max_depth=5, max_features=&#x27;sqrt&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;DecisionTreeClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.tree.DecisionTreeClassifier.html\">?<span>Documentation for DecisionTreeClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>DecisionTreeClassifier(criterion=&#x27;entropy&#x27;, max_depth=5, max_features=&#x27;sqrt&#x27;)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier(criterion='entropy', max_depth=5, max_features='sqrt')"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Huấn luyện các mô hình con\n",
    "rf_clf.fit(x_train,y_train)\n",
    "ab_clf.fit(x_train, y_train)\n",
    "knn_clf.fit(x_train, y_train)\n",
    "svc_clf.fit(x_train, y_train)\n",
    "xgb_clf.fit(x_train, y_train)\n",
    "dt_clf.fit(x_train,y_train)\n",
    "# lgb_clf.fit(x_train, y_train)\n",
    "# cb_clf.fit(x_train, y_train)\n",
    "# gb_clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dự đoán trên tập huấn luyện để tạo đặc trưng mới cho mô hình blending\n",
    "X_train_meta = np.column_stack((\n",
    "    rf_clf.predict_proba(x_val),\n",
    "    ab_clf.predict_proba(x_val),\n",
    "    knn_clf.predict_proba(x_val),\n",
    "    svc_clf.predict_proba(x_val),\n",
    "    xgb_clf.predict_proba(x_val),\n",
    "    dt_clf.predict_proba(x_val),\n",
    "    # lgb_clf.predict_proba(x_val),\n",
    "    # cb_clf.predict_proba(x_val)\n",
    "    # gb_clf.predict_proba(x_val)\n",
    "))\n",
    "# Dự đoán trên tập kiểm tra để tạo đặc trưng mới cho mô hình blending\n",
    "X_test_meta = np.column_stack((\n",
    "    rf_clf.predict_proba(x_test),\n",
    "    ab_clf.predict_proba(x_test),\n",
    "    knn_clf.predict_proba(x_test),\n",
    "    svc_clf.predict_proba(x_test),\n",
    "    xgb_clf.predict_proba(x_test),\n",
    "    dt_clf.predict_proba(x_test),\n",
    "    # lgb_clf.predict_proba(x_test),\n",
    "    # cb_clf.predict_proba(x_test)\n",
    "    # gb_clf.predict_proba(x_test)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_meta:(4174, 24)\n",
      "X_test_meta:(2088, 24)\n"
     ]
    }
   ],
   "source": [
    "print(f\"X_train_meta:{X_train_meta.shape}\")\n",
    "print(f\"X_test_meta:{X_test_meta.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 48 candidates, totalling 144 fits\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.001, loss=log_loss, n_estimators=100;, score=(train=0.371, test=0.371) total time=   6.8s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.001, loss=log_loss, n_estimators=100;, score=(train=0.371, test=0.372) total time=   6.8s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.001, loss=log_loss, n_estimators=100;, score=(train=0.371, test=0.371) total time=   6.6s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.001, loss=log_loss, n_estimators=200;, score=(train=0.941, test=0.920) total time=  13.4s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.001, loss=log_loss, n_estimators=200;, score=(train=0.943, test=0.937) total time=  13.6s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.001, loss=log_loss, n_estimators=200;, score=(train=0.953, test=0.924) total time=  13.3s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.001, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.001, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.001, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.001, loss=deviance, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.001, loss=deviance, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.001, loss=deviance, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.001, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.001, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.001, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.001, loss=exponential, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.001, loss=exponential, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.001, loss=exponential, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.01, loss=log_loss, n_estimators=100;, score=(train=0.967, test=0.945) total time=   6.5s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.01, loss=log_loss, n_estimators=100;, score=(train=0.966, test=0.950) total time=   6.5s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.01, loss=log_loss, n_estimators=100;, score=(train=0.967, test=0.945) total time=   6.6s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.01, loss=log_loss, n_estimators=200;, score=(train=0.971, test=0.947) total time=  14.2s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.01, loss=log_loss, n_estimators=200;, score=(train=0.967, test=0.950) total time=  13.5s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.01, loss=log_loss, n_estimators=200;, score=(train=0.970, test=0.949) total time=  13.2s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.01, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.01, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.01, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.01, loss=exponential, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.01, loss=exponential, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.01, loss=exponential, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.1, loss=log_loss, n_estimators=100;, score=(train=0.997, test=0.944) total time=   6.6s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.1, loss=log_loss, n_estimators=100;, score=(train=0.996, test=0.949) total time=   6.6s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.1, loss=log_loss, n_estimators=100;, score=(train=0.996, test=0.942) total time=   6.5s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.1, loss=log_loss, n_estimators=200;, score=(train=1.000, test=0.945) total time=  13.4s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.1, loss=log_loss, n_estimators=200;, score=(train=1.000, test=0.947) total time=  13.8s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.1, loss=log_loss, n_estimators=200;, score=(train=1.000, test=0.939) total time=  13.6s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.1, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.1, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.1, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.1, loss=exponential, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.1, loss=exponential, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.1, loss=exponential, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=1, loss=log_loss, n_estimators=100;, score=(train=1.000, test=0.940) total time=   6.9s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=1, loss=log_loss, n_estimators=100;, score=(train=1.000, test=0.945) total time=   6.9s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=1, loss=log_loss, n_estimators=100;, score=(train=0.986, test=0.922) total time=   6.8s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=1, loss=log_loss, n_estimators=200;, score=(train=1.000, test=0.943) total time=  11.8s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=1, loss=log_loss, n_estimators=200;, score=(train=1.000, test=0.945) total time=  10.8s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=1, loss=log_loss, n_estimators=200;, score=(train=1.000, test=0.932) total time=  12.2s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=1, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=1, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=1, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=1, loss=deviance, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=1, loss=deviance, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=1, loss=deviance, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=1, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=1, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=1, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=1, loss=exponential, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=1, loss=exponential, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=1, loss=exponential, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.001, loss=log_loss, n_estimators=100;, score=(train=0.371, test=0.371) total time=   6.7s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.001, loss=log_loss, n_estimators=100;, score=(train=0.371, test=0.372) total time=   6.6s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.001, loss=log_loss, n_estimators=100;, score=(train=0.371, test=0.371) total time=   6.6s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.001, loss=log_loss, n_estimators=200;, score=(train=0.941, test=0.920) total time=  13.2s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.001, loss=log_loss, n_estimators=200;, score=(train=0.943, test=0.937) total time=  13.4s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.001, loss=log_loss, n_estimators=200;, score=(train=0.953, test=0.924) total time=  13.1s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.001, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.001, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.001, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.001, loss=deviance, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.001, loss=deviance, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.001, loss=deviance, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.001, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.001, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.001, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.001, loss=exponential, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.001, loss=exponential, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.001, loss=exponential, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.01, loss=log_loss, n_estimators=100;, score=(train=0.967, test=0.945) total time=   6.4s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.01, loss=log_loss, n_estimators=100;, score=(train=0.966, test=0.950) total time=   6.8s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.01, loss=log_loss, n_estimators=100;, score=(train=0.967, test=0.945) total time=   6.6s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.01, loss=log_loss, n_estimators=200;, score=(train=0.971, test=0.948) total time=  13.4s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.01, loss=log_loss, n_estimators=200;, score=(train=0.967, test=0.950) total time=  13.3s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.01, loss=log_loss, n_estimators=200;, score=(train=0.970, test=0.949) total time=  13.4s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.01, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.01, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.01, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.01, loss=deviance, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.01, loss=deviance, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.01, loss=deviance, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.01, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.01, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.01, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.01, loss=exponential, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.01, loss=exponential, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.01, loss=exponential, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.1, loss=log_loss, n_estimators=100;, score=(train=0.997, test=0.945) total time=   6.8s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.1, loss=log_loss, n_estimators=100;, score=(train=0.996, test=0.948) total time=   6.7s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.1, loss=log_loss, n_estimators=100;, score=(train=0.996, test=0.942) total time=   6.5s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.1, loss=log_loss, n_estimators=200;, score=(train=1.000, test=0.945) total time=   9.2s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.1, loss=log_loss, n_estimators=200;, score=(train=1.000, test=0.948) total time=   8.2s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.1, loss=log_loss, n_estimators=200;, score=(train=1.000, test=0.942) total time=   8.4s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.1, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.1, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.1, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.1, loss=deviance, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.1, loss=deviance, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.1, loss=deviance, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.1, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.1, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.1, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.1, loss=exponential, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.1, loss=exponential, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.1, loss=exponential, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=1, loss=log_loss, n_estimators=100;, score=(train=1.000, test=0.940) total time=   4.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=1, loss=log_loss, n_estimators=100;, score=(train=1.000, test=0.942) total time=   4.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=1, loss=log_loss, n_estimators=100;, score=(train=0.999, test=0.937) total time=   3.9s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=1, loss=log_loss, n_estimators=200;, score=(train=1.000, test=0.938) total time=   7.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=1, loss=log_loss, n_estimators=200;, score=(train=1.000, test=0.944) total time=   7.9s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=1, loss=log_loss, n_estimators=200;, score=(train=0.999, test=0.939) total time=   7.5s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=1, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=1, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=1, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=1, loss=deviance, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=1, loss=deviance, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=1, loss=deviance, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=1, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=1, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=1, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=1, loss=exponential, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=1, loss=exponential, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=1, loss=exponential, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_validation.py:542: FitFailedWarning: \n",
      "96 fits failed out of a total of 144.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "48 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_validation.py\", line 890, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\base.py\", line 1344, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of GradientBoostingClassifier must be a str among {'exponential', 'log_loss'}. Got 'deviance' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "48 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_validation.py\", line 890, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\base.py\", line 1351, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\ensemble\\_gb.py\", line 673, in fit\n",
      "    self._loss = self._get_loss(sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\ensemble\\_gb.py\", line 1537, in _get_loss\n",
      "    raise ValueError(\n",
      "ValueError: loss='exponential' is only suitable for a binary classification problem, you have n_classes=4. Please use loss='log_loss' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the test scores are non-finite: [0.37110695 0.9269302         nan        nan        nan        nan\n",
      " 0.94705358 0.94873069        nan        nan        nan        nan\n",
      " 0.94513684 0.94369868        nan        nan        nan        nan\n",
      " 0.93579208 0.94010483        nan        nan        nan        nan\n",
      " 0.37110695 0.92716984        nan        nan        nan        nan\n",
      " 0.94705358 0.94897015        nan        nan        nan        nan\n",
      " 0.9451365  0.9446574         nan        nan        nan        nan\n",
      " 0.93962608 0.9403455         nan        nan        nan        nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the train scores are non-finite: [0.37110688 0.94585476        nan        nan        nan        nan\n",
      " 0.96645905 0.96921434        nan        nan        nan        nan\n",
      " 0.99616682 1.                nan        nan        nan        nan\n",
      " 0.99520901 0.99988023        nan        nan        nan        nan\n",
      " 0.37110688 0.94585476        nan        nan        nan        nan\n",
      " 0.96645905 0.96921434        nan        nan        nan        nan\n",
      " 0.99616682 1.                nan        nan        nan        nan\n",
      " 0.99964068 0.99976045        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "model = GradientBoostingClassifier()\n",
    "params = {\n",
    "    'loss':['log_loss', 'deviance', 'exponential'],\n",
    "    'learning_rate':[0.001,0.01,0.1,1],\n",
    "    'n_estimators':[100,200],\n",
    "    'criterion':['friedman_mse', 'squared_error']\n",
    "}\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=params, cv=3, verbose=5, return_train_score=True,refit=True)\n",
    "grid_model = grid_search.fit(X_train_meta,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_test = grid_model.predict(X_test_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'squared_error',\n",
       " 'learning_rate': 0.01,\n",
       " 'loss': 'log_loss',\n",
       " 'n_estimators': 200}"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9489701515765003"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_model.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay,multilabel_confusion_matrix,f1_score,precision_score,accuracy_score,recall_score,precision_recall_fscore_support\n",
    "def evaluation_test(y,y_pred):\n",
    "    cm = confusion_matrix(y,y_pred)\n",
    "    disp = ConfusionMatrixDisplay(cm,display_labels=['AFIB','SB','SR','GSVT'])\n",
    "    disp.plot()\n",
    "    plt.show()\n",
    "    n_classes = len(cm)\n",
    "    result = []\n",
    "    for c in range(n_classes):\n",
    "        tp = cm[c,c]\n",
    "        fp = sum(cm[:,c]) - cm[c,c]\n",
    "        fn = sum(cm[c,:]) - cm[c,c]\n",
    "        tn = sum(np.delete(sum(cm)-cm[c,:],c))\n",
    "        acc = (tp+tn) / (tp+fn+tn+fp)\n",
    "        recall = tp/(tp+fn)\n",
    "        precision = tp/(tp+fp)\n",
    "        specificity = tn/(tn+fp)\n",
    "        f1_score = 2*((precision*recall)/(precision+recall))\n",
    "        if c+1 == 1:\n",
    "            Rhythm = 'AFIB'\n",
    "        elif c+1 == 2:\n",
    "            Rhythm = 'SB'\n",
    "        elif c+1 == 3:\n",
    "            Rhythm = 'SR'\n",
    "        else:\n",
    "            Rhythm = 'GSVT'\n",
    "        result.append([Rhythm,acc,recall,precision,f1_score,specificity])\n",
    "    p_macro,r_macro,f_macro,support_macro = precision_recall_fscore_support(y,y_pred,average='macro')\n",
    "    p_micro,r_micro,f_micro,support_micro = precision_recall_fscore_support(y,y_pred,average='micro')\n",
    "    p_weighted,r_weighted,f_weighted,support_weighted = precision_recall_fscore_support(y,y_pred,average='weighted')\n",
    "    result.append(['macro avg',None,f_macro,p_macro,r_macro,None])\n",
    "    result.append(['micro avg',None,f_micro,p_micro,r_micro,None])\n",
    "    result.append(['weighted avg',None,f_weighted,p_weighted,r_weighted,None])\n",
    "    return result\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rhythm Group</th>\n",
       "      <th>ACC</th>\n",
       "      <th>F1-score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>specificity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AFIB</td>\n",
       "      <td>0.961686</td>\n",
       "      <td>0.880734</td>\n",
       "      <td>0.932039</td>\n",
       "      <td>0.905660</td>\n",
       "      <td>0.983051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SB</td>\n",
       "      <td>0.961207</td>\n",
       "      <td>0.971613</td>\n",
       "      <td>0.927340</td>\n",
       "      <td>0.948960</td>\n",
       "      <td>0.955065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SR</td>\n",
       "      <td>0.952107</td>\n",
       "      <td>0.855856</td>\n",
       "      <td>0.913462</td>\n",
       "      <td>0.883721</td>\n",
       "      <td>0.978102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GSVT</td>\n",
       "      <td>0.967912</td>\n",
       "      <td>0.939954</td>\n",
       "      <td>0.908482</td>\n",
       "      <td>0.923950</td>\n",
       "      <td>0.975227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>macro avg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.915573</td>\n",
       "      <td>0.920331</td>\n",
       "      <td>0.912039</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>micro avg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.921456</td>\n",
       "      <td>0.921456</td>\n",
       "      <td>0.921456</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>weighted avg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.920859</td>\n",
       "      <td>0.921459</td>\n",
       "      <td>0.921456</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rhythm Group       ACC  F1-score  Precision    Recall  specificity\n",
       "0          AFIB  0.961686  0.880734   0.932039  0.905660     0.983051\n",
       "1            SB  0.961207  0.971613   0.927340  0.948960     0.955065\n",
       "2            SR  0.952107  0.855856   0.913462  0.883721     0.978102\n",
       "3          GSVT  0.967912  0.939954   0.908482  0.923950     0.975227\n",
       "4     macro avg       NaN  0.915573   0.920331  0.912039          NaN\n",
       "5     micro avg       NaN  0.921456   0.921456  0.921456          NaN\n",
       "6  weighted avg       NaN  0.920859   0.921459  0.921456          NaN"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_test = evaluation_test(y_test,result_test)\n",
    "df_evaluation_test = pd.DataFrame(data=evaluation_test,columns=[\"Rhythm Group\",\"ACC\",\"F1-score\",\"Precision\",\"Recall\",\"specificity\"])\n",
    "df_evaluation_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_evaluation_test.to_csv(\"../Result/Blending_GB_125ft.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
