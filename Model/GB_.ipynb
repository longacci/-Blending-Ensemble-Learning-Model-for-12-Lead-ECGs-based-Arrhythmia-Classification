{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>712.000000</td>\n",
       "      <td>696.0</td>\n",
       "      <td>65.440519</td>\n",
       "      <td>240.0</td>\n",
       "      <td>0.703359</td>\n",
       "      <td>-0.246387</td>\n",
       "      <td>...</td>\n",
       "      <td>712.000000</td>\n",
       "      <td>696.0</td>\n",
       "      <td>65.707979</td>\n",
       "      <td>240.0</td>\n",
       "      <td>0.699256</td>\n",
       "      <td>-0.277091</td>\n",
       "      <td>783.821985</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>712.000000</td>\n",
       "      <td>691.0</td>\n",
       "      <td>47.300458</td>\n",
       "      <td>158.0</td>\n",
       "      <td>0.884010</td>\n",
       "      <td>-0.332635</td>\n",
       "      <td>...</td>\n",
       "      <td>712.000000</td>\n",
       "      <td>691.0</td>\n",
       "      <td>47.483330</td>\n",
       "      <td>156.0</td>\n",
       "      <td>0.894198</td>\n",
       "      <td>-0.355280</td>\n",
       "      <td>380.188159</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>13.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1096.250000</td>\n",
       "      <td>1098.0</td>\n",
       "      <td>8.150920</td>\n",
       "      <td>28.0</td>\n",
       "      <td>-0.156675</td>\n",
       "      <td>-0.642487</td>\n",
       "      <td>...</td>\n",
       "      <td>1096.000000</td>\n",
       "      <td>1098.0</td>\n",
       "      <td>8.426150</td>\n",
       "      <td>28.0</td>\n",
       "      <td>-0.160466</td>\n",
       "      <td>-0.801230</td>\n",
       "      <td>808.359965</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1169.714286</td>\n",
       "      <td>1168.0</td>\n",
       "      <td>4.463000</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.642283</td>\n",
       "      <td>-0.722722</td>\n",
       "      <td>...</td>\n",
       "      <td>1169.428571</td>\n",
       "      <td>1168.0</td>\n",
       "      <td>4.237828</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.827427</td>\n",
       "      <td>-0.140248</td>\n",
       "      <td>1150.133430</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>585.333333</td>\n",
       "      <td>586.0</td>\n",
       "      <td>2.890598</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.196283</td>\n",
       "      <td>0.113400</td>\n",
       "      <td>...</td>\n",
       "      <td>585.333333</td>\n",
       "      <td>586.0</td>\n",
       "      <td>3.155243</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.354663</td>\n",
       "      <td>-0.484056</td>\n",
       "      <td>198.042444</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8343</th>\n",
       "      <td>0.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>619.466667</td>\n",
       "      <td>592.0</td>\n",
       "      <td>139.659522</td>\n",
       "      <td>480.0</td>\n",
       "      <td>0.480046</td>\n",
       "      <td>-0.842798</td>\n",
       "      <td>...</td>\n",
       "      <td>619.466667</td>\n",
       "      <td>594.0</td>\n",
       "      <td>139.562109</td>\n",
       "      <td>482.0</td>\n",
       "      <td>0.472791</td>\n",
       "      <td>-0.825913</td>\n",
       "      <td>468.155165</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8344</th>\n",
       "      <td>3.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>510.777778</td>\n",
       "      <td>512.0</td>\n",
       "      <td>7.091118</td>\n",
       "      <td>30.0</td>\n",
       "      <td>-0.059891</td>\n",
       "      <td>-0.269851</td>\n",
       "      <td>...</td>\n",
       "      <td>510.888889</td>\n",
       "      <td>512.0</td>\n",
       "      <td>6.870944</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.081059</td>\n",
       "      <td>-0.433829</td>\n",
       "      <td>309.210006</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>19.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8345</th>\n",
       "      <td>1.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1090.250000</td>\n",
       "      <td>1088.0</td>\n",
       "      <td>9.769212</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.213875</td>\n",
       "      <td>-1.152589</td>\n",
       "      <td>...</td>\n",
       "      <td>1090.250000</td>\n",
       "      <td>1088.0</td>\n",
       "      <td>8.742854</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.436427</td>\n",
       "      <td>-0.891096</td>\n",
       "      <td>940.155678</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8346</th>\n",
       "      <td>3.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>370.320000</td>\n",
       "      <td>370.0</td>\n",
       "      <td>3.133305</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.358316</td>\n",
       "      <td>-0.885061</td>\n",
       "      <td>...</td>\n",
       "      <td>370.400000</td>\n",
       "      <td>370.0</td>\n",
       "      <td>3.098387</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.051640</td>\n",
       "      <td>-0.703333</td>\n",
       "      <td>856.813033</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8347</th>\n",
       "      <td>1.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1037.000000</td>\n",
       "      <td>1037.0</td>\n",
       "      <td>28.372522</td>\n",
       "      <td>94.0</td>\n",
       "      <td>-0.267952</td>\n",
       "      <td>-0.810638</td>\n",
       "      <td>...</td>\n",
       "      <td>1037.000000</td>\n",
       "      <td>1037.0</td>\n",
       "      <td>28.705400</td>\n",
       "      <td>94.0</td>\n",
       "      <td>-0.237050</td>\n",
       "      <td>-0.876580</td>\n",
       "      <td>495.494523</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8348 rows × 124 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0     1    2     3            4       5           6      7         8  \\\n",
       "0     0.0  87.0  1.0  14.0   712.000000   696.0   65.440519  240.0  0.703359   \n",
       "1     2.0  49.0  1.0  13.0   712.000000   691.0   47.300458  158.0  0.884010   \n",
       "2     1.0  86.0  1.0   9.0  1096.250000  1098.0    8.150920   28.0 -0.156675   \n",
       "3     1.0  70.0  1.0   8.0  1169.714286  1168.0    4.463000   14.0  0.642283   \n",
       "4     3.0  61.0  0.0  16.0   585.333333   586.0    2.890598   12.0  0.196283   \n",
       "...   ...   ...  ...   ...          ...     ...         ...    ...       ...   \n",
       "8343  0.0  72.0  1.0  16.0   619.466667   592.0  139.659522  480.0  0.480046   \n",
       "8344  3.0  23.0  0.0  19.0   510.777778   512.0    7.091118   30.0 -0.059891   \n",
       "8345  1.0  51.0  0.0   9.0  1090.250000  1088.0    9.769212   30.0  0.213875   \n",
       "8346  3.0  45.0  0.0  26.0   370.320000   370.0    3.133305   10.0  0.358316   \n",
       "8347  1.0  28.0  1.0   9.0  1037.000000  1037.0   28.372522   94.0 -0.267952   \n",
       "\n",
       "             9  ...          114     115         116    117       118  \\\n",
       "0    -0.246387  ...   712.000000   696.0   65.707979  240.0  0.699256   \n",
       "1    -0.332635  ...   712.000000   691.0   47.483330  156.0  0.894198   \n",
       "2    -0.642487  ...  1096.000000  1098.0    8.426150   28.0 -0.160466   \n",
       "3    -0.722722  ...  1169.428571  1168.0    4.237828   14.0  0.827427   \n",
       "4     0.113400  ...   585.333333   586.0    3.155243   12.0  0.354663   \n",
       "...        ...  ...          ...     ...         ...    ...       ...   \n",
       "8343 -0.842798  ...   619.466667   594.0  139.562109  482.0  0.472791   \n",
       "8344 -0.269851  ...   510.888889   512.0    6.870944   28.0  0.081059   \n",
       "8345 -1.152589  ...  1090.250000  1088.0    8.742854   28.0  0.436427   \n",
       "8346 -0.885061  ...   370.400000   370.0    3.098387   12.0  0.051640   \n",
       "8347 -0.810638  ...  1037.000000  1037.0   28.705400   94.0 -0.237050   \n",
       "\n",
       "           119          120       121   122   123  \n",
       "0    -0.277091   783.821985  0.785714   9.0   6.0  \n",
       "1    -0.355280   380.188159  1.000000  13.0  12.0  \n",
       "2    -0.801230   808.359965  1.000000   8.0   6.0  \n",
       "3    -0.140248  1150.133430  1.000000   8.0   7.0  \n",
       "4    -0.484056   198.042444  1.000000  16.0  15.0  \n",
       "...        ...          ...       ...   ...   ...  \n",
       "8343 -0.825913   468.155165  1.000000  16.0   8.0  \n",
       "8344 -0.433829   309.210006  1.000000  19.0  18.0  \n",
       "8345 -0.891096   940.155678  1.000000   9.0   8.0  \n",
       "8346 -0.703333   856.813033  0.461538   1.0   6.0  \n",
       "8347 -0.876580   495.494523  1.000000   9.0   8.0  \n",
       "\n",
       "[8348 rows x 124 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../df_new_mean_train.csv\")\n",
    "df.drop(columns=[\"Unnamed: 0\"],inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = df.iloc[:,1:].values\n",
    "y_train = df.iloc[:,0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (4174, 123)\n",
      "Vallidation: (4174, 123)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train , test_size=0.5, shuffle=True, stratify=y_train, random_state=119)\n",
    "print(f\"Train: {x_train.shape}\")\n",
    "print(f\"Vallidation: {x_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = x_train\n",
    "y = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sc = MinMaxScaler()\n",
    "X = sc.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "def confusion_matrix_scorer(clf=None,X=None, y=None,y_pred=None,y_prob=None):\n",
    "    if clf != None:\n",
    "        y_pred = clf.predict(X)\n",
    "        y_prob = clf.predict_proba(X)[:,1]\n",
    "    cm = confusion_matrix(y,y_pred)\n",
    "    acc_arr = []\n",
    "    precision_arr = []\n",
    "    recall_arr = []\n",
    "    specificity_arr = []\n",
    "    f1_arr = []\n",
    "    for c in range(0,len(cm)):\n",
    "        c = len(cm)-1\n",
    "        tp = cm[c,c]\n",
    "        fp = sum(cm[:,c]) - cm[c,c]\n",
    "        fn = sum(cm[c,:]) - cm[c,c]\n",
    "        tn = sum(np.delete(sum(cm)-cm[c,:],c))\n",
    "        acc_arr.append((tp+tn) / (tp+fn+tn+fp))\n",
    "        recall_arr.append(tp/(tp+fn))\n",
    "        precision_arr.append(tp/(tp+fp))\n",
    "        specificity_arr.append(tn/(tn+fp))\n",
    "        f1_arr.append(2*(((tp/(tp+fp))*(tp/(tp+fn)))/((tp/(tp+fp))+(tp/(tp+fn)))))\n",
    "    return {'acc':np.nanmean(acc_arr),'precision':np.nanmean(precision_arr),'specificity':np.nanmean(specificity_arr),'recall':np.nanmean(recall_arr),'f1_score':np.nanmean(f1_arr)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 48 candidates, totalling 144 fits\n",
      "[CV] END criterion=friedman_mse, learning_rate=0.001, loss=log_loss, n_estimators=100; total time=  22.0s\n",
      "[CV] END criterion=friedman_mse, learning_rate=0.001, loss=log_loss, n_estimators=100; total time=  22.2s\n",
      "[CV] END criterion=friedman_mse, learning_rate=0.001, loss=log_loss, n_estimators=100; total time=  21.8s\n",
      "[CV] END criterion=friedman_mse, learning_rate=0.001, loss=log_loss, n_estimators=200; total time=  44.2s\n",
      "[CV] END criterion=friedman_mse, learning_rate=0.001, loss=log_loss, n_estimators=200; total time=  53.0s\n",
      "[CV] END criterion=friedman_mse, learning_rate=0.001, loss=log_loss, n_estimators=200; total time=  53.0s\n",
      "[CV] END criterion=friedman_mse, learning_rate=0.001, loss=deviance, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, learning_rate=0.001, loss=deviance, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, learning_rate=0.001, loss=deviance, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, learning_rate=0.001, loss=deviance, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, learning_rate=0.001, loss=deviance, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, learning_rate=0.001, loss=deviance, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, learning_rate=0.001, loss=exponential, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, learning_rate=0.001, loss=exponential, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, learning_rate=0.001, loss=exponential, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, learning_rate=0.001, loss=exponential, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, learning_rate=0.001, loss=exponential, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, learning_rate=0.001, loss=exponential, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, learning_rate=0.01, loss=log_loss, n_estimators=100; total time=  21.5s\n",
      "[CV] END criterion=friedman_mse, learning_rate=0.01, loss=log_loss, n_estimators=100; total time=  21.3s\n",
      "[CV] END criterion=friedman_mse, learning_rate=0.01, loss=log_loss, n_estimators=100; total time=  21.3s\n",
      "[CV] END criterion=friedman_mse, learning_rate=0.01, loss=log_loss, n_estimators=200; total time=  58.8s\n",
      "[CV] END criterion=friedman_mse, learning_rate=0.01, loss=log_loss, n_estimators=200; total time= 1.1min\n",
      "[CV] END criterion=friedman_mse, learning_rate=0.01, loss=log_loss, n_estimators=200; total time= 1.0min\n",
      "[CV] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, learning_rate=0.01, loss=exponential, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, learning_rate=0.01, loss=exponential, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, learning_rate=0.01, loss=exponential, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, learning_rate=0.01, loss=exponential, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, learning_rate=0.01, loss=exponential, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, learning_rate=0.01, loss=exponential, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, learning_rate=0.1, loss=log_loss, n_estimators=100; total time=  30.9s\n",
      "[CV] END criterion=friedman_mse, learning_rate=0.1, loss=log_loss, n_estimators=100; total time=  31.0s\n",
      "[CV] END criterion=friedman_mse, learning_rate=0.1, loss=log_loss, n_estimators=100; total time=  31.2s\n",
      "[CV] END criterion=friedman_mse, learning_rate=0.1, loss=log_loss, n_estimators=200; total time= 1.0min\n",
      "[CV] END criterion=friedman_mse, learning_rate=0.1, loss=log_loss, n_estimators=200; total time= 1.0min\n",
      "[CV] END criterion=friedman_mse, learning_rate=0.1, loss=log_loss, n_estimators=200; total time= 1.1min\n",
      "[CV] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, learning_rate=0.1, loss=exponential, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, learning_rate=0.1, loss=exponential, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, learning_rate=0.1, loss=exponential, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, learning_rate=0.1, loss=exponential, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, learning_rate=0.1, loss=exponential, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, learning_rate=0.1, loss=exponential, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, learning_rate=1, loss=log_loss, n_estimators=100; total time=  31.6s\n",
      "[CV] END criterion=friedman_mse, learning_rate=1, loss=log_loss, n_estimators=100; total time=  31.4s\n",
      "[CV] END criterion=friedman_mse, learning_rate=1, loss=log_loss, n_estimators=100; total time=  31.5s\n",
      "[CV] END criterion=friedman_mse, learning_rate=1, loss=log_loss, n_estimators=200; total time= 1.0min\n",
      "[CV] END criterion=friedman_mse, learning_rate=1, loss=log_loss, n_estimators=200; total time=  54.5s\n",
      "[CV] END criterion=friedman_mse, learning_rate=1, loss=log_loss, n_estimators=200; total time= 1.3min\n",
      "[CV] END criterion=friedman_mse, learning_rate=1, loss=deviance, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, learning_rate=1, loss=deviance, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, learning_rate=1, loss=deviance, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, learning_rate=1, loss=deviance, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, learning_rate=1, loss=deviance, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, learning_rate=1, loss=deviance, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, learning_rate=1, loss=exponential, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, learning_rate=1, loss=exponential, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, learning_rate=1, loss=exponential, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, learning_rate=1, loss=exponential, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, learning_rate=1, loss=exponential, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, learning_rate=1, loss=exponential, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=0.001, loss=log_loss, n_estimators=100; total time=  37.0s\n",
      "[CV] END criterion=squared_error, learning_rate=0.001, loss=log_loss, n_estimators=100; total time=  37.0s\n",
      "[CV] END criterion=squared_error, learning_rate=0.001, loss=log_loss, n_estimators=100; total time=  37.1s\n",
      "[CV] END criterion=squared_error, learning_rate=0.001, loss=log_loss, n_estimators=200; total time= 1.2min\n",
      "[CV] END criterion=squared_error, learning_rate=0.001, loss=log_loss, n_estimators=200; total time= 1.2min\n",
      "[CV] END criterion=squared_error, learning_rate=0.001, loss=log_loss, n_estimators=200; total time= 1.3min\n",
      "[CV] END criterion=squared_error, learning_rate=0.001, loss=deviance, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=0.001, loss=deviance, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=0.001, loss=deviance, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=0.001, loss=deviance, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=0.001, loss=deviance, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=0.001, loss=deviance, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=0.001, loss=exponential, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=0.001, loss=exponential, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=0.001, loss=exponential, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=0.001, loss=exponential, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=0.001, loss=exponential, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=0.001, loss=exponential, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=0.01, loss=log_loss, n_estimators=100; total time=  36.8s\n",
      "[CV] END criterion=squared_error, learning_rate=0.01, loss=log_loss, n_estimators=100; total time=  36.8s\n",
      "[CV] END criterion=squared_error, learning_rate=0.01, loss=log_loss, n_estimators=100; total time=  37.1s\n",
      "[CV] END criterion=squared_error, learning_rate=0.01, loss=log_loss, n_estimators=200; total time= 1.2min\n",
      "[CV] END criterion=squared_error, learning_rate=0.01, loss=log_loss, n_estimators=200; total time= 1.2min\n",
      "[CV] END criterion=squared_error, learning_rate=0.01, loss=log_loss, n_estimators=200; total time= 1.1min\n",
      "[CV] END criterion=squared_error, learning_rate=0.01, loss=deviance, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=0.01, loss=deviance, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=0.01, loss=deviance, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=0.01, loss=deviance, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=0.01, loss=deviance, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=0.01, loss=deviance, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=0.01, loss=exponential, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=0.01, loss=exponential, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=0.01, loss=exponential, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=0.01, loss=exponential, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=0.01, loss=exponential, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=0.01, loss=exponential, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=0.1, loss=log_loss, n_estimators=100; total time=  33.2s\n",
      "[CV] END criterion=squared_error, learning_rate=0.1, loss=log_loss, n_estimators=100; total time=  33.0s\n",
      "[CV] END criterion=squared_error, learning_rate=0.1, loss=log_loss, n_estimators=100; total time=  33.6s\n",
      "[CV] END criterion=squared_error, learning_rate=0.1, loss=log_loss, n_estimators=200; total time= 1.1min\n",
      "[CV] END criterion=squared_error, learning_rate=0.1, loss=log_loss, n_estimators=200; total time= 1.1min\n",
      "[CV] END criterion=squared_error, learning_rate=0.1, loss=log_loss, n_estimators=200; total time= 1.1min\n",
      "[CV] END criterion=squared_error, learning_rate=0.1, loss=deviance, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=0.1, loss=deviance, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=0.1, loss=deviance, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=0.1, loss=deviance, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=0.1, loss=deviance, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=0.1, loss=deviance, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=0.1, loss=exponential, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=0.1, loss=exponential, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=0.1, loss=exponential, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=0.1, loss=exponential, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=0.1, loss=exponential, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=0.1, loss=exponential, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=1, loss=log_loss, n_estimators=100; total time=  32.8s\n",
      "[CV] END criterion=squared_error, learning_rate=1, loss=log_loss, n_estimators=100; total time=  32.9s\n",
      "[CV] END criterion=squared_error, learning_rate=1, loss=log_loss, n_estimators=100; total time=  33.0s\n",
      "[CV] END criterion=squared_error, learning_rate=1, loss=log_loss, n_estimators=200; total time= 1.1min\n",
      "[CV] END criterion=squared_error, learning_rate=1, loss=log_loss, n_estimators=200; total time=  55.1s\n",
      "[CV] END criterion=squared_error, learning_rate=1, loss=log_loss, n_estimators=200; total time= 1.1min\n",
      "[CV] END criterion=squared_error, learning_rate=1, loss=deviance, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=1, loss=deviance, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=1, loss=deviance, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=1, loss=deviance, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=1, loss=deviance, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=1, loss=deviance, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=1, loss=exponential, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=1, loss=exponential, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=1, loss=exponential, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=1, loss=exponential, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=1, loss=exponential, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=1, loss=exponential, n_estimators=200; total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_validation.py:542: FitFailedWarning: \n",
      "96 fits failed out of a total of 144.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "48 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_validation.py\", line 890, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\base.py\", line 1344, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of GradientBoostingClassifier must be a str among {'exponential', 'log_loss'}. Got 'deviance' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "48 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_validation.py\", line 890, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\base.py\", line 1351, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\ensemble\\_gb.py\", line 673, in fit\n",
      "    self._loss = self._get_loss(sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\ensemble\\_gb.py\", line 1537, in _get_loss\n",
      "    raise ValueError(\n",
      "ValueError: loss='exponential' is only suitable for a binary classification problem, you have n_classes=4. Please use loss='log_loss' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the test scores are non-finite: [0.37110678 0.87590414        nan        nan        nan        nan\n",
      " 0.93100195 0.93818947        nan        nan        nan        nan\n",
      " 0.94130456 0.94226242        nan        nan        nan        nan\n",
      " 0.93531659 0.93795138        nan        nan        nan        nan\n",
      " 0.37110678 0.87614361        nan        nan        nan        nan\n",
      " 0.93100195 0.93818947        nan        nan        nan        nan\n",
      " 0.94130456 0.94322079        nan        nan        nan        nan\n",
      " 0.93723316 0.93843152        nan        nan        nan        nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the train scores are non-finite: [0.37110683 0.88967262        nan        nan        nan        nan\n",
      " 0.95004804 0.96070947        nan        nan        nan        nan\n",
      " 0.99868243 1.                nan        nan        nan        nan\n",
      " 0.99964068 0.99952086        nan        nan        nan        nan\n",
      " 0.37110683 0.88967262        nan        nan        nan        nan\n",
      " 0.95004804 0.96082924        nan        nan        nan        nan\n",
      " 0.99880221 1.                nan        nan        nan        nan\n",
      " 0.99952086 0.99952086        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "model = GradientBoostingClassifier()\n",
    "params = {\n",
    "    'loss':['log_loss', 'deviance', 'exponential'],\n",
    "    'learning_rate':[0.001,0.01,0.1,1],\n",
    "    'n_estimators':[100,200],\n",
    "    'criterion':['friedman_mse', 'squared_error']\n",
    "}\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=params, cv=3, verbose=2, return_train_score=True,refit=True)\n",
    "grid_model = grid_search.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'squared_error',\n",
       " 'learning_rate': 0.1,\n",
       " 'loss': 'log_loss',\n",
       " 'n_estimators': 200}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9432207871621342"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_model.best_score_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
