{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_predict,StratifiedShuffleSplit,cross_validate,cross_validate,ShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>712.000000</td>\n",
       "      <td>696.0</td>\n",
       "      <td>65.440519</td>\n",
       "      <td>240.0</td>\n",
       "      <td>0.703359</td>\n",
       "      <td>-0.246387</td>\n",
       "      <td>...</td>\n",
       "      <td>712.000000</td>\n",
       "      <td>696.0</td>\n",
       "      <td>65.707979</td>\n",
       "      <td>240.0</td>\n",
       "      <td>0.699256</td>\n",
       "      <td>-0.277091</td>\n",
       "      <td>783.821985</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>712.000000</td>\n",
       "      <td>691.0</td>\n",
       "      <td>47.300458</td>\n",
       "      <td>158.0</td>\n",
       "      <td>0.884010</td>\n",
       "      <td>-0.332635</td>\n",
       "      <td>...</td>\n",
       "      <td>712.000000</td>\n",
       "      <td>691.0</td>\n",
       "      <td>47.483330</td>\n",
       "      <td>156.0</td>\n",
       "      <td>0.894198</td>\n",
       "      <td>-0.355280</td>\n",
       "      <td>380.188159</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>13.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1096.250000</td>\n",
       "      <td>1098.0</td>\n",
       "      <td>8.150920</td>\n",
       "      <td>28.0</td>\n",
       "      <td>-0.156675</td>\n",
       "      <td>-0.642487</td>\n",
       "      <td>...</td>\n",
       "      <td>1096.000000</td>\n",
       "      <td>1098.0</td>\n",
       "      <td>8.426150</td>\n",
       "      <td>28.0</td>\n",
       "      <td>-0.160466</td>\n",
       "      <td>-0.801230</td>\n",
       "      <td>808.359965</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1169.714286</td>\n",
       "      <td>1168.0</td>\n",
       "      <td>4.463000</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.642283</td>\n",
       "      <td>-0.722722</td>\n",
       "      <td>...</td>\n",
       "      <td>1169.428571</td>\n",
       "      <td>1168.0</td>\n",
       "      <td>4.237828</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.827427</td>\n",
       "      <td>-0.140248</td>\n",
       "      <td>1150.133430</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>585.333333</td>\n",
       "      <td>586.0</td>\n",
       "      <td>2.890598</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.196283</td>\n",
       "      <td>0.113400</td>\n",
       "      <td>...</td>\n",
       "      <td>585.333333</td>\n",
       "      <td>586.0</td>\n",
       "      <td>3.155243</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.354663</td>\n",
       "      <td>-0.484056</td>\n",
       "      <td>198.042444</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8343</th>\n",
       "      <td>0.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>619.466667</td>\n",
       "      <td>592.0</td>\n",
       "      <td>139.659522</td>\n",
       "      <td>480.0</td>\n",
       "      <td>0.480046</td>\n",
       "      <td>-0.842798</td>\n",
       "      <td>...</td>\n",
       "      <td>619.466667</td>\n",
       "      <td>594.0</td>\n",
       "      <td>139.562109</td>\n",
       "      <td>482.0</td>\n",
       "      <td>0.472791</td>\n",
       "      <td>-0.825913</td>\n",
       "      <td>468.155165</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8344</th>\n",
       "      <td>3.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>510.777778</td>\n",
       "      <td>512.0</td>\n",
       "      <td>7.091118</td>\n",
       "      <td>30.0</td>\n",
       "      <td>-0.059891</td>\n",
       "      <td>-0.269851</td>\n",
       "      <td>...</td>\n",
       "      <td>510.888889</td>\n",
       "      <td>512.0</td>\n",
       "      <td>6.870944</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.081059</td>\n",
       "      <td>-0.433829</td>\n",
       "      <td>309.210006</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>19.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8345</th>\n",
       "      <td>1.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1090.250000</td>\n",
       "      <td>1088.0</td>\n",
       "      <td>9.769212</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.213875</td>\n",
       "      <td>-1.152589</td>\n",
       "      <td>...</td>\n",
       "      <td>1090.250000</td>\n",
       "      <td>1088.0</td>\n",
       "      <td>8.742854</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.436427</td>\n",
       "      <td>-0.891096</td>\n",
       "      <td>940.155678</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8346</th>\n",
       "      <td>3.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>370.320000</td>\n",
       "      <td>370.0</td>\n",
       "      <td>3.133305</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.358316</td>\n",
       "      <td>-0.885061</td>\n",
       "      <td>...</td>\n",
       "      <td>370.400000</td>\n",
       "      <td>370.0</td>\n",
       "      <td>3.098387</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.051640</td>\n",
       "      <td>-0.703333</td>\n",
       "      <td>856.813033</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8347</th>\n",
       "      <td>1.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1037.000000</td>\n",
       "      <td>1037.0</td>\n",
       "      <td>28.372522</td>\n",
       "      <td>94.0</td>\n",
       "      <td>-0.267952</td>\n",
       "      <td>-0.810638</td>\n",
       "      <td>...</td>\n",
       "      <td>1037.000000</td>\n",
       "      <td>1037.0</td>\n",
       "      <td>28.705400</td>\n",
       "      <td>94.0</td>\n",
       "      <td>-0.237050</td>\n",
       "      <td>-0.876580</td>\n",
       "      <td>495.494523</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8348 rows × 124 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0     1    2     3            4       5           6      7         8  \\\n",
       "0     0.0  87.0  1.0  14.0   712.000000   696.0   65.440519  240.0  0.703359   \n",
       "1     2.0  49.0  1.0  13.0   712.000000   691.0   47.300458  158.0  0.884010   \n",
       "2     1.0  86.0  1.0   9.0  1096.250000  1098.0    8.150920   28.0 -0.156675   \n",
       "3     1.0  70.0  1.0   8.0  1169.714286  1168.0    4.463000   14.0  0.642283   \n",
       "4     3.0  61.0  0.0  16.0   585.333333   586.0    2.890598   12.0  0.196283   \n",
       "...   ...   ...  ...   ...          ...     ...         ...    ...       ...   \n",
       "8343  0.0  72.0  1.0  16.0   619.466667   592.0  139.659522  480.0  0.480046   \n",
       "8344  3.0  23.0  0.0  19.0   510.777778   512.0    7.091118   30.0 -0.059891   \n",
       "8345  1.0  51.0  0.0   9.0  1090.250000  1088.0    9.769212   30.0  0.213875   \n",
       "8346  3.0  45.0  0.0  26.0   370.320000   370.0    3.133305   10.0  0.358316   \n",
       "8347  1.0  28.0  1.0   9.0  1037.000000  1037.0   28.372522   94.0 -0.267952   \n",
       "\n",
       "             9  ...          114     115         116    117       118  \\\n",
       "0    -0.246387  ...   712.000000   696.0   65.707979  240.0  0.699256   \n",
       "1    -0.332635  ...   712.000000   691.0   47.483330  156.0  0.894198   \n",
       "2    -0.642487  ...  1096.000000  1098.0    8.426150   28.0 -0.160466   \n",
       "3    -0.722722  ...  1169.428571  1168.0    4.237828   14.0  0.827427   \n",
       "4     0.113400  ...   585.333333   586.0    3.155243   12.0  0.354663   \n",
       "...        ...  ...          ...     ...         ...    ...       ...   \n",
       "8343 -0.842798  ...   619.466667   594.0  139.562109  482.0  0.472791   \n",
       "8344 -0.269851  ...   510.888889   512.0    6.870944   28.0  0.081059   \n",
       "8345 -1.152589  ...  1090.250000  1088.0    8.742854   28.0  0.436427   \n",
       "8346 -0.885061  ...   370.400000   370.0    3.098387   12.0  0.051640   \n",
       "8347 -0.810638  ...  1037.000000  1037.0   28.705400   94.0 -0.237050   \n",
       "\n",
       "           119          120       121   122   123  \n",
       "0    -0.277091   783.821985  0.785714   9.0   6.0  \n",
       "1    -0.355280   380.188159  1.000000  13.0  12.0  \n",
       "2    -0.801230   808.359965  1.000000   8.0   6.0  \n",
       "3    -0.140248  1150.133430  1.000000   8.0   7.0  \n",
       "4    -0.484056   198.042444  1.000000  16.0  15.0  \n",
       "...        ...          ...       ...   ...   ...  \n",
       "8343 -0.825913   468.155165  1.000000  16.0   8.0  \n",
       "8344 -0.433829   309.210006  1.000000  19.0  18.0  \n",
       "8345 -0.891096   940.155678  1.000000   9.0   8.0  \n",
       "8346 -0.703333   856.813033  0.461538   1.0   6.0  \n",
       "8347 -0.876580   495.494523  1.000000   9.0   8.0  \n",
       "\n",
       "[8348 rows x 124 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"../data_train_125ft.csv\")\n",
    "df_train.drop(columns=[\"Unnamed: 0\"],inplace=True)\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = df_train.iloc[:,1:].values\n",
    "y_train = df_train.iloc[:,0].values\n",
    "# x_train = x_train[fti_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8348"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = MinMaxScaler()\n",
    "x_train = scale.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1091.750000</td>\n",
       "      <td>1091.0</td>\n",
       "      <td>22.280877</td>\n",
       "      <td>66.0</td>\n",
       "      <td>0.493022</td>\n",
       "      <td>-0.931472</td>\n",
       "      <td>...</td>\n",
       "      <td>1092.000000</td>\n",
       "      <td>1091.0</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>66.0</td>\n",
       "      <td>0.521225</td>\n",
       "      <td>-0.913087</td>\n",
       "      <td>373.995302</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1115.000000</td>\n",
       "      <td>1106.0</td>\n",
       "      <td>59.841457</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.777480</td>\n",
       "      <td>-0.423745</td>\n",
       "      <td>...</td>\n",
       "      <td>1115.000000</td>\n",
       "      <td>1107.0</td>\n",
       "      <td>60.728906</td>\n",
       "      <td>192.0</td>\n",
       "      <td>0.772436</td>\n",
       "      <td>-0.395166</td>\n",
       "      <td>477.271172</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>806.000000</td>\n",
       "      <td>804.0</td>\n",
       "      <td>8.045326</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.293282</td>\n",
       "      <td>-0.392690</td>\n",
       "      <td>...</td>\n",
       "      <td>806.000000</td>\n",
       "      <td>804.0</td>\n",
       "      <td>7.722458</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.198976</td>\n",
       "      <td>-0.896193</td>\n",
       "      <td>277.972651</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>458.300000</td>\n",
       "      <td>458.0</td>\n",
       "      <td>1.705872</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.192180</td>\n",
       "      <td>-0.770964</td>\n",
       "      <td>...</td>\n",
       "      <td>458.300000</td>\n",
       "      <td>458.0</td>\n",
       "      <td>1.926136</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.368600</td>\n",
       "      <td>-0.876527</td>\n",
       "      <td>361.998395</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>941.555556</td>\n",
       "      <td>1054.0</td>\n",
       "      <td>220.140920</td>\n",
       "      <td>628.0</td>\n",
       "      <td>-1.482314</td>\n",
       "      <td>0.419095</td>\n",
       "      <td>...</td>\n",
       "      <td>1059.250000</td>\n",
       "      <td>1059.0</td>\n",
       "      <td>10.341059</td>\n",
       "      <td>34.0</td>\n",
       "      <td>-0.061463</td>\n",
       "      <td>-0.843856</td>\n",
       "      <td>586.261896</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2083</th>\n",
       "      <td>3.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>485.789474</td>\n",
       "      <td>486.0</td>\n",
       "      <td>3.104817</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-0.415255</td>\n",
       "      <td>-0.874744</td>\n",
       "      <td>...</td>\n",
       "      <td>485.789474</td>\n",
       "      <td>486.0</td>\n",
       "      <td>2.966106</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-0.496663</td>\n",
       "      <td>-0.494835</td>\n",
       "      <td>798.662084</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2084</th>\n",
       "      <td>1.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1060.250000</td>\n",
       "      <td>1056.0</td>\n",
       "      <td>14.677789</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.273677</td>\n",
       "      <td>-1.204916</td>\n",
       "      <td>...</td>\n",
       "      <td>1060.250000</td>\n",
       "      <td>1056.0</td>\n",
       "      <td>13.872184</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.231747</td>\n",
       "      <td>-1.352375</td>\n",
       "      <td>603.272611</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2085</th>\n",
       "      <td>1.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1070.250000</td>\n",
       "      <td>1076.0</td>\n",
       "      <td>33.082284</td>\n",
       "      <td>98.0</td>\n",
       "      <td>-0.016678</td>\n",
       "      <td>-1.320021</td>\n",
       "      <td>...</td>\n",
       "      <td>1070.250000</td>\n",
       "      <td>1075.0</td>\n",
       "      <td>32.686962</td>\n",
       "      <td>96.0</td>\n",
       "      <td>0.030170</td>\n",
       "      <td>-1.329810</td>\n",
       "      <td>692.057509</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2086</th>\n",
       "      <td>1.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1021.750000</td>\n",
       "      <td>1018.0</td>\n",
       "      <td>7.171994</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.898849</td>\n",
       "      <td>-0.581729</td>\n",
       "      <td>...</td>\n",
       "      <td>1021.750000</td>\n",
       "      <td>1020.0</td>\n",
       "      <td>7.101936</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.839867</td>\n",
       "      <td>-0.281413</td>\n",
       "      <td>456.052143</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2087</th>\n",
       "      <td>1.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1079.500000</td>\n",
       "      <td>1089.0</td>\n",
       "      <td>20.826666</td>\n",
       "      <td>54.0</td>\n",
       "      <td>-0.460865</td>\n",
       "      <td>-1.486785</td>\n",
       "      <td>...</td>\n",
       "      <td>1079.750000</td>\n",
       "      <td>1087.0</td>\n",
       "      <td>21.574001</td>\n",
       "      <td>60.0</td>\n",
       "      <td>-0.405116</td>\n",
       "      <td>-1.328357</td>\n",
       "      <td>324.178086</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2088 rows × 124 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0     1    2     3            4       5           6      7         8  \\\n",
       "0     1.0  32.0  0.0   9.0  1091.750000  1091.0   22.280877   66.0  0.493022   \n",
       "1     1.0  17.0  0.0   9.0  1115.000000  1106.0   59.841457  186.0  0.777480   \n",
       "2     2.0  64.0  0.0  12.0   806.000000   804.0    8.045326   30.0  0.293282   \n",
       "3     3.0  61.0  0.0  21.0   458.300000   458.0    1.705872    6.0  0.192180   \n",
       "4     1.0  70.0  1.0  10.0   941.555556  1054.0  220.140920  628.0 -1.482314   \n",
       "...   ...   ...  ...   ...          ...     ...         ...    ...       ...   \n",
       "2083  3.0  71.0  0.0  20.0   485.789474   486.0    3.104817   10.0 -0.415255   \n",
       "2084  1.0  51.0  1.0   9.0  1060.250000  1056.0   14.677789   44.0  0.273677   \n",
       "2085  1.0  61.0  1.0   9.0  1070.250000  1076.0   33.082284   98.0 -0.016678   \n",
       "2086  1.0  74.0  1.0   9.0  1021.750000  1018.0    7.171994   22.0  0.898849   \n",
       "2087  1.0  63.0  0.0   9.0  1079.500000  1089.0   20.826666   54.0 -0.460865   \n",
       "\n",
       "             9  ...          114     115        116    117       118  \\\n",
       "0    -0.931472  ...  1092.000000  1091.0  22.000000   66.0  0.521225   \n",
       "1    -0.423745  ...  1115.000000  1107.0  60.728906  192.0  0.772436   \n",
       "2    -0.392690  ...   806.000000   804.0   7.722458   26.0  0.198976   \n",
       "3    -0.770964  ...   458.300000   458.0   1.926136    6.0  0.368600   \n",
       "4     0.419095  ...  1059.250000  1059.0  10.341059   34.0 -0.061463   \n",
       "...        ...  ...          ...     ...        ...    ...       ...   \n",
       "2083 -0.874744  ...   485.789474   486.0   2.966106   10.0 -0.496663   \n",
       "2084 -1.204916  ...  1060.250000  1056.0  13.872184   40.0  0.231747   \n",
       "2085 -1.320021  ...  1070.250000  1075.0  32.686962   96.0  0.030170   \n",
       "2086 -0.581729  ...  1021.750000  1020.0   7.101936   24.0  0.839867   \n",
       "2087 -1.486785  ...  1079.750000  1087.0  21.574001   60.0 -0.405116   \n",
       "\n",
       "           119         120  121   122   123  \n",
       "0    -0.913087  373.995302  1.0   9.0   7.0  \n",
       "1    -0.395166  477.271172  1.0   9.0   8.0  \n",
       "2    -0.896193  277.972651  1.0  12.0  11.0  \n",
       "3    -0.876527  361.998395  1.0   0.0  20.0  \n",
       "4    -0.843856  586.261896  1.0   9.0   7.0  \n",
       "...        ...         ...  ...   ...   ...  \n",
       "2083 -0.494835  798.662084  1.0  10.0  19.0  \n",
       "2084 -1.352375  603.272611  1.0   9.0   8.0  \n",
       "2085 -1.329810  692.057509  1.0   9.0   4.0  \n",
       "2086 -0.281413  456.052143  1.0   9.0   8.0  \n",
       "2087 -1.328357  324.178086  1.0   9.0   8.0  \n",
       "\n",
       "[2088 rows x 124 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv(\"../data_test_125ft.csv\")\n",
    "df_test.drop(columns=[\"Unnamed: 0\"],inplace=True)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = df_test.iloc[:,1:].values\n",
    "y_test = df_test.iloc[:,0].values\n",
    "x_test = x_test[fti_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = scale.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create based model\n",
    "level0 = list()\n",
    "level0.append(('RF', RandomForestClassifier(criterion='log_loss', max_depth=5, max_features='sqrt', n_estimators=50)))\n",
    "level0.append(('AB', AdaBoostClassifier(algorithm='SAMME.R', learning_rate=0.01, n_estimators=1000)))\n",
    "level0.append(('CB', CatBoostClassifier(iterations= 300, learning_rate=0.1)))\n",
    "level0.append(('GB', GradientBoostingClassifier(criterion= 'squared_error',learning_rate= 0.1,loss= 'log_loss',n_estimators= 200)))\n",
    "level0.append(('XGB', XGBClassifier(gamma=0, learning_rate = 0.1, max_depth= 3, min_child_weight= 1, n_estimators= 1000)))\n",
    "level0.append(('LGB', LGBMClassifier(boosting= 'gbdt', data_sample_strategy= 'goss', estimators= 50, learning_rate=0.1, objective= 'multiclass')))\n",
    "\n",
    "\n",
    "level1 = list()\n",
    "level1.append(('LR', LogisticRegression(C= 10, max_iter= 200, penalty= 'l2', solver= 'saga')))\n",
    "level1.append(('SVM', SVC(C= 10, gamma= 'scale', kernel= 'rbf', probability= True)))\n",
    "level1.append(('DT', DecisionTreeClassifier(criterion= 'gini',max_depth= 5,max_features= 'sqrt',splitter= 'best')))\n",
    "level1.append(('KNN', KNeighborsClassifier(algorithm= 'auto', n_neighbors= 6, p= 1, weights= 'distance')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: RF\n",
      "----------------\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [50, 8348]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m----------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m train_model \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i,(train_index, test_index) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(kf\u001b[38;5;241m.\u001b[39msplit(x_train, y_train)):\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_split.py:367\u001b[0m, in \u001b[0;36m_BaseKFold.split\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msplit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, groups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    344\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generate indices to split data into training and test set.\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \n\u001b[0;32m    346\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    365\u001b[0m \u001b[38;5;124;03m        The testing set indices for that split.\u001b[39;00m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 367\u001b[0m     X, y, groups \u001b[38;5;241m=\u001b[39m \u001b[43mindexable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    368\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(X)\n\u001b[0;32m    369\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_splits \u001b[38;5;241m>\u001b[39m n_samples:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\utils\\validation.py:476\u001b[0m, in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[0;32m    458\u001b[0m \n\u001b[0;32m    459\u001b[0m \u001b[38;5;124;03mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    472\u001b[0m \u001b[38;5;124;03m    sparse matrix, or dataframe) or `None`.\u001b[39;00m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    475\u001b[0m result \u001b[38;5;241m=\u001b[39m [_make_indexable(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m iterables]\n\u001b[1;32m--> 476\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\utils\\validation.py:430\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    428\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    429\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 430\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    431\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    432\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    433\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [50, 8348]"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold,StratifiedKFold,StratifiedShuffleSplit,StratifiedGroupKFold\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "kf = StratifiedKFold(n_splits=5,random_state=None)\n",
    "\n",
    "train_news = np.empty((x_train.shape[0], 0))\n",
    "test_news = np.empty((x_test.shape[0], 0))\n",
    "\n",
    "for name, model in level0:\n",
    "    print(f\"Model: {name}\")\n",
    "    print(\"----------------\")\n",
    "    train_model = []\n",
    "    for i,(train_index, test_index) in enumerate(kf.split(x_train, y_train)):\n",
    "        print(f\"Fold {i}\")\n",
    "        print(f\"train: {len(train_index)}\")\n",
    "        print(f\"test: {len(test_index)}\")\n",
    "        # Train model in 4/5\n",
    "        model.fit(x_train[train_index], y_train[train_index])\n",
    "        # predict model in 1/5\n",
    "        y_pred = model.predict(x_train[test_index])\n",
    "        train_model.extend(y_pred)\n",
    "    # Predict on test set\n",
    "    model.fit(x_train, y_train)\n",
    "    test_model = model.predict(x_test)\n",
    "    # Add column to news data\n",
    "    train_news = np.column_stack((train_news, np.array(train_model).reshape(-1, 1)))\n",
    "    test_news = np.column_stack((test_news, np.array(test_model).reshape(-1, 1)))\n",
    "    print(\"-------Done-------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LR\n",
      "----------------\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "-------Done-------\n",
      "Model: SVM\n",
      "----------------\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "-------Done-------\n",
      "Model: DT\n",
      "----------------\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "-------Done-------\n",
      "Model: KNN\n",
      "----------------\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "-------Done-------\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "train_news_final = np.empty((train_news.shape[0], 0))\n",
    "test_news_final = np.empty((test_news.shape[0], 0))\n",
    "\n",
    "for name, model in level1:\n",
    "    print(f\"Model: {name}\")\n",
    "    print(\"----------------\")\n",
    "    train_model = []\n",
    "    for i,(train_index, test_index) in enumerate(kf.split(train_news, y_train)):\n",
    "        print(f\"Fold {i}\")\n",
    "        # Train model in 4/5\n",
    "        model.fit(train_news[train_index], y_train[train_index])\n",
    "        # predict model in 1/5\n",
    "        y_pred = model.predict(train_news[test_index])\n",
    "        train_model.extend(y_pred)\n",
    "    # Predict on test set\n",
    "    model.fit(train_news, y_train)\n",
    "    test_model = model.predict(test_news)\n",
    "    # Add column to news data\n",
    "    train_news_final = np.column_stack((train_news_final, np.array(train_model).reshape(-1, 1)))\n",
    "    test_news_final = np.column_stack((test_news_final, np.array(test_model).reshape(-1, 1)))\n",
    "    print(\"-------Done-------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_news_final = pd.read_csv(\"../train_news_final.csv\")\n",
    "# train_news_final.drop(columns=['Unnamed: 0'],inplace=True)\n",
    "# test_news_final = pd.read_csv(\"../test_news_final.csv\")\n",
    "# test_news_final.drop(columns=['Unnamed: 0'],inplace=True)\n",
    "# train_news_final = train_news_final.to_numpy()\n",
    "# test_news_final = test_news_final.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n",
      "[CV 1/5] END algorithm=SAMME, learning_rate=0.01, n_estimators=10;, score=0.578 total time=   0.0s\n",
      "[CV 2/5] END algorithm=SAMME, learning_rate=0.01, n_estimators=10;, score=0.557 total time=   0.0s\n",
      "[CV 3/5] END algorithm=SAMME, learning_rate=0.01, n_estimators=10;, score=0.526 total time=   0.0s\n",
      "[CV 4/5] END algorithm=SAMME, learning_rate=0.01, n_estimators=10;, score=0.539 total time=   0.0s\n",
      "[CV 5/5] END algorithm=SAMME, learning_rate=0.01, n_estimators=10;, score=0.557 total time=   0.0s\n",
      "[CV 1/5] END algorithm=SAMME, learning_rate=0.01, n_estimators=50;, score=0.578 total time=   0.1s\n",
      "[CV 2/5] END algorithm=SAMME, learning_rate=0.01, n_estimators=50;, score=0.557 total time=   0.1s\n",
      "[CV 3/5] END algorithm=SAMME, learning_rate=0.01, n_estimators=50;, score=0.527 total time=   0.1s\n",
      "[CV 4/5] END algorithm=SAMME, learning_rate=0.01, n_estimators=50;, score=0.539 total time=   0.1s\n",
      "[CV 5/5] END algorithm=SAMME, learning_rate=0.01, n_estimators=50;, score=0.556 total time=   0.1s\n",
      "[CV 1/5] END algorithm=SAMME, learning_rate=0.01, n_estimators=100;, score=0.558 total time=   0.3s\n",
      "[CV 2/5] END algorithm=SAMME, learning_rate=0.01, n_estimators=100;, score=0.545 total time=   0.3s\n",
      "[CV 3/5] END algorithm=SAMME, learning_rate=0.01, n_estimators=100;, score=0.685 total time=   0.3s\n",
      "[CV 4/5] END algorithm=SAMME, learning_rate=0.01, n_estimators=100;, score=0.711 total time=   0.3s\n",
      "[CV 5/5] END algorithm=SAMME, learning_rate=0.01, n_estimators=100;, score=0.555 total time=   0.3s\n",
      "[CV 1/5] END algorithm=SAMME, learning_rate=0.01, n_estimators=1000;, score=0.941 total time=   3.8s\n",
      "[CV 2/5] END algorithm=SAMME, learning_rate=0.01, n_estimators=1000;, score=0.891 total time=   3.8s\n",
      "[CV 3/5] END algorithm=SAMME, learning_rate=0.01, n_estimators=1000;, score=0.841 total time=   3.8s\n",
      "[CV 4/5] END algorithm=SAMME, learning_rate=0.01, n_estimators=1000;, score=0.886 total time=   3.8s\n",
      "[CV 5/5] END algorithm=SAMME, learning_rate=0.01, n_estimators=1000;, score=0.907 total time=   3.9s\n",
      "[CV 1/5] END algorithm=SAMME, learning_rate=0.1, n_estimators=10;, score=0.558 total time=   0.0s\n",
      "[CV 2/5] END algorithm=SAMME, learning_rate=0.1, n_estimators=10;, score=0.729 total time=   0.0s\n",
      "[CV 3/5] END algorithm=SAMME, learning_rate=0.1, n_estimators=10;, score=0.527 total time=   0.0s\n",
      "[CV 4/5] END algorithm=SAMME, learning_rate=0.1, n_estimators=10;, score=0.539 total time=   0.0s\n",
      "[CV 5/5] END algorithm=SAMME, learning_rate=0.1, n_estimators=10;, score=0.744 total time=   0.0s\n",
      "[CV 1/5] END algorithm=SAMME, learning_rate=0.1, n_estimators=50;, score=0.941 total time=   0.1s\n",
      "[CV 2/5] END algorithm=SAMME, learning_rate=0.1, n_estimators=50;, score=0.887 total time=   0.1s\n",
      "[CV 3/5] END algorithm=SAMME, learning_rate=0.1, n_estimators=50;, score=0.840 total time=   0.1s\n",
      "[CV 4/5] END algorithm=SAMME, learning_rate=0.1, n_estimators=50;, score=0.883 total time=   0.1s\n",
      "[CV 5/5] END algorithm=SAMME, learning_rate=0.1, n_estimators=50;, score=0.907 total time=   0.1s\n",
      "[CV 1/5] END algorithm=SAMME, learning_rate=0.1, n_estimators=100;, score=0.941 total time=   0.3s\n",
      "[CV 2/5] END algorithm=SAMME, learning_rate=0.1, n_estimators=100;, score=0.891 total time=   0.3s\n",
      "[CV 3/5] END algorithm=SAMME, learning_rate=0.1, n_estimators=100;, score=0.841 total time=   0.3s\n",
      "[CV 4/5] END algorithm=SAMME, learning_rate=0.1, n_estimators=100;, score=0.886 total time=   0.3s\n",
      "[CV 5/5] END algorithm=SAMME, learning_rate=0.1, n_estimators=100;, score=0.907 total time=   0.3s\n",
      "[CV 1/5] END algorithm=SAMME, learning_rate=0.1, n_estimators=1000;, score=0.941 total time=   3.2s\n",
      "[CV 2/5] END algorithm=SAMME, learning_rate=0.1, n_estimators=1000;, score=0.893 total time=   2.2s\n",
      "[CV 3/5] END algorithm=SAMME, learning_rate=0.1, n_estimators=1000;, score=0.841 total time=   2.2s\n",
      "[CV 4/5] END algorithm=SAMME, learning_rate=0.1, n_estimators=1000;, score=0.889 total time=   3.0s\n",
      "[CV 5/5] END algorithm=SAMME, learning_rate=0.1, n_estimators=1000;, score=0.907 total time=   2.8s\n",
      "[CV 1/5] END algorithm=SAMME, learning_rate=1, n_estimators=10;, score=0.746 total time=   0.0s\n",
      "[CV 2/5] END algorithm=SAMME, learning_rate=1, n_estimators=10;, score=0.887 total time=   0.0s\n",
      "[CV 3/5] END algorithm=SAMME, learning_rate=1, n_estimators=10;, score=0.841 total time=   0.0s\n",
      "[CV 4/5] END algorithm=SAMME, learning_rate=1, n_estimators=10;, score=0.883 total time=   0.0s\n",
      "[CV 5/5] END algorithm=SAMME, learning_rate=1, n_estimators=10;, score=0.899 total time=   0.0s\n",
      "[CV 1/5] END algorithm=SAMME, learning_rate=1, n_estimators=50;, score=0.941 total time=   0.0s\n",
      "[CV 2/5] END algorithm=SAMME, learning_rate=1, n_estimators=50;, score=0.893 total time=   0.0s\n",
      "[CV 3/5] END algorithm=SAMME, learning_rate=1, n_estimators=50;, score=0.841 total time=   0.0s\n",
      "[CV 4/5] END algorithm=SAMME, learning_rate=1, n_estimators=50;, score=0.889 total time=   0.0s\n",
      "[CV 5/5] END algorithm=SAMME, learning_rate=1, n_estimators=50;, score=0.907 total time=   0.0s\n",
      "[CV 1/5] END algorithm=SAMME, learning_rate=1, n_estimators=100;, score=0.943 total time=   0.2s\n",
      "[CV 2/5] END algorithm=SAMME, learning_rate=1, n_estimators=100;, score=0.893 total time=   0.1s\n",
      "[CV 3/5] END algorithm=SAMME, learning_rate=1, n_estimators=100;, score=0.842 total time=   0.1s\n",
      "[CV 4/5] END algorithm=SAMME, learning_rate=1, n_estimators=100;, score=0.889 total time=   0.1s\n",
      "[CV 5/5] END algorithm=SAMME, learning_rate=1, n_estimators=100;, score=0.907 total time=   0.1s\n",
      "[CV 1/5] END algorithm=SAMME, learning_rate=1, n_estimators=1000;, score=0.942 total time=   2.2s\n",
      "[CV 2/5] END algorithm=SAMME, learning_rate=1, n_estimators=1000;, score=0.892 total time=   2.3s\n",
      "[CV 3/5] END algorithm=SAMME, learning_rate=1, n_estimators=1000;, score=0.842 total time=   2.7s\n",
      "[CV 4/5] END algorithm=SAMME, learning_rate=1, n_estimators=1000;, score=0.889 total time=   2.4s\n",
      "[CV 5/5] END algorithm=SAMME, learning_rate=1, n_estimators=1000;, score=0.907 total time=   2.1s\n",
      "[CV 1/5] END algorithm=SAMME, learning_rate=10, n_estimators=10;, score=0.383 total time=   0.0s\n",
      "[CV 2/5] END algorithm=SAMME, learning_rate=10, n_estimators=10;, score=0.211 total time=   0.0s\n",
      "[CV 3/5] END algorithm=SAMME, learning_rate=10, n_estimators=10;, score=0.058 total time=   0.0s\n",
      "[CV 4/5] END algorithm=SAMME, learning_rate=10, n_estimators=10;, score=0.042 total time=   0.0s\n",
      "[CV 5/5] END algorithm=SAMME, learning_rate=10, n_estimators=10;, score=0.216 total time=   0.0s\n",
      "[CV 1/5] END algorithm=SAMME, learning_rate=10, n_estimators=50;, score=0.383 total time=   0.0s\n",
      "[CV 2/5] END algorithm=SAMME, learning_rate=10, n_estimators=50;, score=0.371 total time=   0.0s\n",
      "[CV 3/5] END algorithm=SAMME, learning_rate=10, n_estimators=50;, score=0.360 total time=   0.0s\n",
      "[CV 4/5] END algorithm=SAMME, learning_rate=10, n_estimators=50;, score=0.539 total time=   0.0s\n",
      "[CV 5/5] END algorithm=SAMME, learning_rate=10, n_estimators=50;, score=0.369 total time=   0.0s\n",
      "[CV 1/5] END algorithm=SAMME, learning_rate=10, n_estimators=100;, score=0.383 total time=   0.1s\n",
      "[CV 2/5] END algorithm=SAMME, learning_rate=10, n_estimators=100;, score=0.371 total time=   0.1s\n",
      "[CV 3/5] END algorithm=SAMME, learning_rate=10, n_estimators=100;, score=0.360 total time=   0.1s\n",
      "[CV 4/5] END algorithm=SAMME, learning_rate=10, n_estimators=100;, score=0.374 total time=   0.1s\n",
      "[CV 5/5] END algorithm=SAMME, learning_rate=10, n_estimators=100;, score=0.369 total time=   0.1s\n",
      "[CV 1/5] END algorithm=SAMME, learning_rate=10, n_estimators=1000;, score=0.383 total time=   2.1s\n",
      "[CV 2/5] END algorithm=SAMME, learning_rate=10, n_estimators=1000;, score=0.371 total time=   2.0s\n",
      "[CV 3/5] END algorithm=SAMME, learning_rate=10, n_estimators=1000;, score=0.528 total time=   2.0s\n",
      "[CV 4/5] END algorithm=SAMME, learning_rate=10, n_estimators=1000;, score=0.539 total time=   2.0s\n",
      "[CV 5/5] END algorithm=SAMME, learning_rate=10, n_estimators=1000;, score=0.369 total time=   2.0s\n",
      "[CV 1/5] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=10;, score=0.578 total time=   0.0s\n",
      "[CV 2/5] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=10;, score=0.559 total time=   0.0s\n",
      "[CV 3/5] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=10;, score=0.528 total time=   0.0s\n",
      "[CV 4/5] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=10;, score=0.539 total time=   0.0s\n",
      "[CV 5/5] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=10;, score=0.557 total time=   0.0s\n",
      "[CV 1/5] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=50;, score=0.763 total time=   0.0s\n",
      "[CV 2/5] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=50;, score=0.731 total time=   0.0s\n",
      "[CV 3/5] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=50;, score=0.687 total time=   0.0s\n",
      "[CV 4/5] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=50;, score=0.711 total time=   0.0s\n",
      "[CV 5/5] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=50;, score=0.745 total time=   0.0s\n",
      "[CV 1/5] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=100;, score=0.941 total time=   0.2s\n",
      "[CV 2/5] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=100;, score=0.893 total time=   0.2s\n",
      "[CV 3/5] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=100;, score=0.841 total time=   0.2s\n",
      "[CV 4/5] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=100;, score=0.888 total time=   0.2s\n",
      "[CV 5/5] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=100;, score=0.907 total time=   0.2s\n",
      "[CV 1/5] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=1000;, score=0.747 total time=   2.5s\n",
      "[CV 2/5] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=1000;, score=0.893 total time=   2.5s\n",
      "[CV 3/5] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=1000;, score=0.841 total time=   2.4s\n",
      "[CV 4/5] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=1000;, score=0.889 total time=   2.5s\n",
      "[CV 5/5] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=1000;, score=0.907 total time=   2.4s\n",
      "[CV 1/5] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=10;, score=0.941 total time=   0.0s\n",
      "[CV 2/5] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=10;, score=0.887 total time=   0.0s\n",
      "[CV 3/5] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=10;, score=0.841 total time=   0.0s\n",
      "[CV 4/5] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=10;, score=0.884 total time=   0.0s\n",
      "[CV 5/5] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=10;, score=0.907 total time=   0.0s\n",
      "[CV 1/5] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=50;, score=0.941 total time=   0.0s\n",
      "[CV 2/5] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=50;, score=0.893 total time=   0.0s\n",
      "[CV 3/5] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=50;, score=0.841 total time=   0.0s\n",
      "[CV 4/5] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=50;, score=0.888 total time=   0.0s\n",
      "[CV 5/5] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=50;, score=0.907 total time=   0.0s\n",
      "[CV 1/5] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=100;, score=0.941 total time=   0.2s\n",
      "[CV 2/5] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=100;, score=0.893 total time=   0.2s\n",
      "[CV 3/5] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=100;, score=0.841 total time=   0.2s\n",
      "[CV 4/5] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=100;, score=0.889 total time=   0.2s\n",
      "[CV 5/5] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=100;, score=0.907 total time=   0.2s\n",
      "[CV 1/5] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=1000;, score=0.941 total time=   2.5s\n",
      "[CV 2/5] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=1000;, score=0.893 total time=   2.5s\n",
      "[CV 3/5] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=1000;, score=0.842 total time=   2.4s\n",
      "[CV 4/5] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=1000;, score=0.888 total time=   2.6s\n",
      "[CV 5/5] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=1000;, score=0.907 total time=   2.8s\n",
      "[CV 1/5] END algorithm=SAMME.R, learning_rate=1, n_estimators=10;, score=0.941 total time=   0.0s\n",
      "[CV 2/5] END algorithm=SAMME.R, learning_rate=1, n_estimators=10;, score=0.889 total time=   0.0s\n",
      "[CV 3/5] END algorithm=SAMME.R, learning_rate=1, n_estimators=10;, score=0.840 total time=   0.0s\n",
      "[CV 4/5] END algorithm=SAMME.R, learning_rate=1, n_estimators=10;, score=0.888 total time=   0.0s\n",
      "[CV 5/5] END algorithm=SAMME.R, learning_rate=1, n_estimators=10;, score=0.905 total time=   0.0s\n",
      "[CV 1/5] END algorithm=SAMME.R, learning_rate=1, n_estimators=50;, score=0.941 total time=   0.0s\n",
      "[CV 2/5] END algorithm=SAMME.R, learning_rate=1, n_estimators=50;, score=0.889 total time=   0.0s\n",
      "[CV 3/5] END algorithm=SAMME.R, learning_rate=1, n_estimators=50;, score=0.840 total time=   0.0s\n",
      "[CV 4/5] END algorithm=SAMME.R, learning_rate=1, n_estimators=50;, score=0.888 total time=   0.0s\n",
      "[CV 5/5] END algorithm=SAMME.R, learning_rate=1, n_estimators=50;, score=0.906 total time=   0.1s\n",
      "[CV 1/5] END algorithm=SAMME.R, learning_rate=1, n_estimators=100;, score=0.941 total time=   0.2s\n",
      "[CV 2/5] END algorithm=SAMME.R, learning_rate=1, n_estimators=100;, score=0.890 total time=   0.2s\n",
      "[CV 3/5] END algorithm=SAMME.R, learning_rate=1, n_estimators=100;, score=0.840 total time=   0.2s\n",
      "[CV 4/5] END algorithm=SAMME.R, learning_rate=1, n_estimators=100;, score=0.887 total time=   0.2s\n",
      "[CV 5/5] END algorithm=SAMME.R, learning_rate=1, n_estimators=100;, score=0.907 total time=   0.2s\n",
      "[CV 1/5] END algorithm=SAMME.R, learning_rate=1, n_estimators=1000;, score=0.942 total time=   5.0s\n",
      "[CV 2/5] END algorithm=SAMME.R, learning_rate=1, n_estimators=1000;, score=0.892 total time=   4.8s\n",
      "[CV 3/5] END algorithm=SAMME.R, learning_rate=1, n_estimators=1000;, score=0.841 total time=   4.7s\n",
      "[CV 4/5] END algorithm=SAMME.R, learning_rate=1, n_estimators=1000;, score=0.886 total time=   5.0s\n",
      "[CV 5/5] END algorithm=SAMME.R, learning_rate=1, n_estimators=1000;, score=0.905 total time=   3.9s\n",
      "[CV 1/5] END algorithm=SAMME.R, learning_rate=10, n_estimators=10;, score=0.398 total time=   0.0s\n",
      "[CV 2/5] END algorithm=SAMME.R, learning_rate=10, n_estimators=10;, score=0.212 total time=   0.0s\n",
      "[CV 3/5] END algorithm=SAMME.R, learning_rate=10, n_estimators=10;, score=0.208 total time=   0.0s\n",
      "[CV 4/5] END algorithm=SAMME.R, learning_rate=10, n_estimators=10;, score=0.207 total time=   0.0s\n",
      "[CV 5/5] END algorithm=SAMME.R, learning_rate=10, n_estimators=10;, score=0.208 total time=   0.0s\n",
      "[CV 1/5] END algorithm=SAMME.R, learning_rate=10, n_estimators=50;, score=0.763 total time=   0.0s\n",
      "[CV 2/5] END algorithm=SAMME.R, learning_rate=10, n_estimators=50;, score=0.559 total time=   0.0s\n",
      "[CV 3/5] END algorithm=SAMME.R, learning_rate=10, n_estimators=50;, score=0.528 total time=   0.0s\n",
      "[CV 4/5] END algorithm=SAMME.R, learning_rate=10, n_estimators=50;, score=0.532 total time=   0.0s\n",
      "[CV 5/5] END algorithm=SAMME.R, learning_rate=10, n_estimators=50;, score=0.552 total time=   0.1s\n",
      "[CV 1/5] END algorithm=SAMME.R, learning_rate=10, n_estimators=100;, score=0.763 total time=   0.5s\n",
      "[CV 2/5] END algorithm=SAMME.R, learning_rate=10, n_estimators=100;, score=0.557 total time=   0.5s\n",
      "[CV 3/5] END algorithm=SAMME.R, learning_rate=10, n_estimators=100;, score=0.528 total time=   0.4s\n",
      "[CV 4/5] END algorithm=SAMME.R, learning_rate=10, n_estimators=100;, score=0.532 total time=   0.5s\n",
      "[CV 5/5] END algorithm=SAMME.R, learning_rate=10, n_estimators=100;, score=0.552 total time=   0.2s\n",
      "[CV 1/5] END algorithm=SAMME.R, learning_rate=10, n_estimators=1000;, score=0.763 total time=   4.9s\n",
      "[CV 2/5] END algorithm=SAMME.R, learning_rate=10, n_estimators=1000;, score=0.557 total time=   5.0s\n",
      "[CV 3/5] END algorithm=SAMME.R, learning_rate=10, n_estimators=1000;, score=0.528 total time=   4.9s\n",
      "[CV 4/5] END algorithm=SAMME.R, learning_rate=10, n_estimators=1000;, score=0.532 total time=   5.3s\n",
      "[CV 5/5] END algorithm=SAMME.R, learning_rate=10, n_estimators=1000;, score=0.552 total time=   4.4s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-3 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-3 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-3 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-3 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-3 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-3 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-3 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-3 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-3 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>AdaBoostClassifier(algorithm=&#x27;SAMME&#x27;, learning_rate=1, n_estimators=100)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;AdaBoostClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.ensemble.AdaBoostClassifier.html\">?<span>Documentation for AdaBoostClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>AdaBoostClassifier(algorithm=&#x27;SAMME&#x27;, learning_rate=1, n_estimators=100)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME', learning_rate=1, n_estimators=100)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "meta_model = AdaBoostClassifier()\n",
    "params = {\n",
    "    'n_estimators': [10,50,100,1000],\n",
    "    'learning_rate': [0.01,0.1,1,10],\n",
    "    'algorithm': ['SAMME', 'SAMME.R'],\n",
    "}\n",
    "GS = GridSearchCV(estimator=meta_model, param_grid=params, cv=5, verbose=5)\n",
    "GS.fit(train_news_final, y_train)\n",
    "GS.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = GS.best_estimator_\n",
    "y_pred = best_model.predict(test_news_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay,multilabel_confusion_matrix,f1_score,precision_score,accuracy_score,recall_score,precision_recall_fscore_support\n",
    "def evaluation_test(y,y_pred):\n",
    "    cm = confusion_matrix(y,y_pred)\n",
    "    disp = ConfusionMatrixDisplay(cm,display_labels=['AFIB','SB','SR','GSVT'])\n",
    "    disp.plot()\n",
    "    plt.show()\n",
    "    n_classes = len(cm)\n",
    "    result = []\n",
    "    for c in range(n_classes):\n",
    "        tp = cm[c,c]\n",
    "        fp = sum(cm[:,c]) - cm[c,c]\n",
    "        fn = sum(cm[c,:]) - cm[c,c]\n",
    "        tn = sum(np.delete(sum(cm)-cm[c,:],c))\n",
    "        acc = (tp+tn) / (tp+fn+tn+fp)\n",
    "        recall = tp/(tp+fn)\n",
    "        precision = tp/(tp+fp)\n",
    "        specificity = tn/(tn+fp)\n",
    "        f1_score = 2*((precision*recall)/(precision+recall))\n",
    "        if c+1 == 1:\n",
    "            Rhythm = 'AFIB'\n",
    "        elif c+1 == 2:\n",
    "            Rhythm = 'SB'\n",
    "        elif c+1 == 3:\n",
    "            Rhythm = 'SR'\n",
    "        else:\n",
    "            Rhythm = 'GSVT'\n",
    "        result.append([Rhythm,acc,recall,precision,f1_score,specificity])\n",
    "    p_macro,r_macro,f_macro,support_macro = precision_recall_fscore_support(y,y_pred,average='macro')\n",
    "    p_micro,r_micro,f_micro,support_micro = precision_recall_fscore_support(y,y_pred,average='micro')\n",
    "    p_weighted,r_weighted,f_weighted,support_weighted = precision_recall_fscore_support(y,y_pred,average='weighted')\n",
    "    result.append(['macro avg',None,f_macro,p_macro,r_macro,None])\n",
    "    result.append(['micro avg',None,f_micro,p_micro,r_micro,None])\n",
    "    result.append(['weighted avg',None,f_weighted,p_weighted,r_weighted,None])\n",
    "    return result\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rhythm Group</th>\n",
       "      <th>ACC</th>\n",
       "      <th>F1-score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>specificity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AFIB</td>\n",
       "      <td>0.963602</td>\n",
       "      <td>0.910550</td>\n",
       "      <td>0.914747</td>\n",
       "      <td>0.912644</td>\n",
       "      <td>0.977603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SB</td>\n",
       "      <td>0.988506</td>\n",
       "      <td>0.984516</td>\n",
       "      <td>0.984516</td>\n",
       "      <td>0.984516</td>\n",
       "      <td>0.990861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SR</td>\n",
       "      <td>0.987069</td>\n",
       "      <td>0.970721</td>\n",
       "      <td>0.968539</td>\n",
       "      <td>0.969629</td>\n",
       "      <td>0.991484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GSVT</td>\n",
       "      <td>0.971743</td>\n",
       "      <td>0.933025</td>\n",
       "      <td>0.930876</td>\n",
       "      <td>0.931949</td>\n",
       "      <td>0.981873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>macro avg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.949684</td>\n",
       "      <td>0.949669</td>\n",
       "      <td>0.949703</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>micro avg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.955460</td>\n",
       "      <td>0.955460</td>\n",
       "      <td>0.955460</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>weighted avg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.955442</td>\n",
       "      <td>0.955426</td>\n",
       "      <td>0.955460</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rhythm Group       ACC  F1-score  Precision    Recall  specificity\n",
       "0          AFIB  0.963602  0.910550   0.914747  0.912644     0.977603\n",
       "1            SB  0.988506  0.984516   0.984516  0.984516     0.990861\n",
       "2            SR  0.987069  0.970721   0.968539  0.969629     0.991484\n",
       "3          GSVT  0.971743  0.933025   0.930876  0.931949     0.981873\n",
       "4     macro avg       NaN  0.949684   0.949669  0.949703          NaN\n",
       "5     micro avg       NaN  0.955460   0.955460  0.955460          NaN\n",
       "6  weighted avg       NaN  0.955442   0.955426  0.955460          NaN"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_test = evaluation_test(y_test,y_pred)\n",
    "df_evaluation_test = pd.DataFrame(data=evaluation_test,columns=[\"Rhythm Group\",\"ACC\",\"F1-score\",\"Precision\",\"Recall\",\"specificity\"])\n",
    "df_evaluation_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_evaluation_test.to_csv(\"../Result/Stacking_multipleClass_AB.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testdatasets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
