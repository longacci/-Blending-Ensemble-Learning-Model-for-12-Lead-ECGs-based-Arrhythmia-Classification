{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_predict,StratifiedShuffleSplit,cross_validate,cross_validate,ShuffleSplit\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_score = pd.read_csv(\"../Feature_selection/ft_important.csv\")\n",
    "# df_score.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "# df_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fti_index = df_score['index'].values\n",
    "# fti_index = fti_index.astype(dtype=str)\n",
    "# fti_index = np.insert(fti_index,0,'0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ranks = []\n",
    "# for i in range(len(list_feature)):\n",
    "#     arr = []\n",
    "#     for j in range(i+1):\n",
    "#         arr.append(list_feature[j])\n",
    "#     ranks.append(arr)\n",
    "# ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>245</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>450.600000</td>\n",
       "      <td>1.640000</td>\n",
       "      <td>450.600000</td>\n",
       "      <td>526.440000</td>\n",
       "      <td>446.500000</td>\n",
       "      <td>5097.950000</td>\n",
       "      <td>-8.700000</td>\n",
       "      <td>...</td>\n",
       "      <td>80062.392773</td>\n",
       "      <td>0.670011</td>\n",
       "      <td>-0.912051</td>\n",
       "      <td>0.018315</td>\n",
       "      <td>242.648806</td>\n",
       "      <td>21.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>-1.714518</td>\n",
       "      <td>3.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>357.000000</td>\n",
       "      <td>1.923077</td>\n",
       "      <td>357.076923</td>\n",
       "      <td>538.840237</td>\n",
       "      <td>356.076923</td>\n",
       "      <td>11891.224852</td>\n",
       "      <td>-1.307692</td>\n",
       "      <td>...</td>\n",
       "      <td>76406.068692</td>\n",
       "      <td>0.850813</td>\n",
       "      <td>-0.498918</td>\n",
       "      <td>-0.076761</td>\n",
       "      <td>974.391877</td>\n",
       "      <td>27.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>8.932387</td>\n",
       "      <td>-1.307692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>478.105263</td>\n",
       "      <td>1.041551</td>\n",
       "      <td>479.052632</td>\n",
       "      <td>345.839335</td>\n",
       "      <td>478.210526</td>\n",
       "      <td>2771.745152</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>...</td>\n",
       "      <td>85871.392384</td>\n",
       "      <td>0.489407</td>\n",
       "      <td>-0.973497</td>\n",
       "      <td>0.262626</td>\n",
       "      <td>25.275439</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>6.346053</td>\n",
       "      <td>0.526316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>587.066667</td>\n",
       "      <td>357.262222</td>\n",
       "      <td>587.866667</td>\n",
       "      <td>663.182222</td>\n",
       "      <td>584.533333</td>\n",
       "      <td>6410.382222</td>\n",
       "      <td>-0.800000</td>\n",
       "      <td>...</td>\n",
       "      <td>103785.759900</td>\n",
       "      <td>1.091541</td>\n",
       "      <td>-0.392591</td>\n",
       "      <td>-0.159420</td>\n",
       "      <td>9.601546</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>11.755690</td>\n",
       "      <td>-0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>900.600000</td>\n",
       "      <td>95.240000</td>\n",
       "      <td>900.600000</td>\n",
       "      <td>170.440000</td>\n",
       "      <td>905.000000</td>\n",
       "      <td>10025.000000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>...</td>\n",
       "      <td>81079.175405</td>\n",
       "      <td>0.800294</td>\n",
       "      <td>-0.779998</td>\n",
       "      <td>-0.343434</td>\n",
       "      <td>11.737973</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>-143.141574</td>\n",
       "      <td>2.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8479</th>\n",
       "      <td>0.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>612.800000</td>\n",
       "      <td>29298.560000</td>\n",
       "      <td>611.733333</td>\n",
       "      <td>28027.128889</td>\n",
       "      <td>614.933333</td>\n",
       "      <td>28417.528889</td>\n",
       "      <td>-5.866667</td>\n",
       "      <td>...</td>\n",
       "      <td>100877.375033</td>\n",
       "      <td>0.305497</td>\n",
       "      <td>-1.525075</td>\n",
       "      <td>-0.201948</td>\n",
       "      <td>2438.340543</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>24.802373</td>\n",
       "      <td>8.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8480</th>\n",
       "      <td>3.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>418.956522</td>\n",
       "      <td>2734.563327</td>\n",
       "      <td>418.956522</td>\n",
       "      <td>2523.780718</td>\n",
       "      <td>419.478261</td>\n",
       "      <td>8352.075614</td>\n",
       "      <td>-3.565217</td>\n",
       "      <td>...</td>\n",
       "      <td>81455.005289</td>\n",
       "      <td>0.448115</td>\n",
       "      <td>-1.260327</td>\n",
       "      <td>0.309823</td>\n",
       "      <td>32.322927</td>\n",
       "      <td>24.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>5.947163</td>\n",
       "      <td>-3.565217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8481</th>\n",
       "      <td>1.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1235.428571</td>\n",
       "      <td>700.244898</td>\n",
       "      <td>1235.714286</td>\n",
       "      <td>897.632653</td>\n",
       "      <td>1248.285714</td>\n",
       "      <td>15626.775510</td>\n",
       "      <td>16.857143</td>\n",
       "      <td>...</td>\n",
       "      <td>81763.393500</td>\n",
       "      <td>0.092227</td>\n",
       "      <td>-1.692873</td>\n",
       "      <td>0.599331</td>\n",
       "      <td>10.950366</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>18.149394</td>\n",
       "      <td>18.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8482</th>\n",
       "      <td>2.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>823.800000</td>\n",
       "      <td>4775.560000</td>\n",
       "      <td>824.000000</td>\n",
       "      <td>4056.000000</td>\n",
       "      <td>804.000000</td>\n",
       "      <td>11132.800000</td>\n",
       "      <td>-22.600000</td>\n",
       "      <td>...</td>\n",
       "      <td>66478.783260</td>\n",
       "      <td>1.024300</td>\n",
       "      <td>-0.596278</td>\n",
       "      <td>0.147300</td>\n",
       "      <td>335.316688</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>4.484999</td>\n",
       "      <td>-20.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8483</th>\n",
       "      <td>1.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1100.000000</td>\n",
       "      <td>3967.000000</td>\n",
       "      <td>1095.500000</td>\n",
       "      <td>6184.750000</td>\n",
       "      <td>1091.000000</td>\n",
       "      <td>5050.000000</td>\n",
       "      <td>-16.250000</td>\n",
       "      <td>...</td>\n",
       "      <td>55993.608000</td>\n",
       "      <td>0.410091</td>\n",
       "      <td>-1.525947</td>\n",
       "      <td>-0.070707</td>\n",
       "      <td>76.835189</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.645446</td>\n",
       "      <td>-18.285714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8484 rows × 255 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0     1    2            3             4            5             6  \\\n",
       "0     3.0  58.0  1.0   450.600000      1.640000   450.600000    526.440000   \n",
       "1     3.0  41.0  0.0   357.000000      1.923077   357.076923    538.840237   \n",
       "2     3.0  76.0  0.0   478.105263      1.041551   479.052632    345.839335   \n",
       "3     3.0  27.0  1.0   587.066667    357.262222   587.866667    663.182222   \n",
       "4     2.0  76.0  0.0   900.600000     95.240000   900.600000    170.440000   \n",
       "...   ...   ...  ...          ...           ...          ...           ...   \n",
       "8479  0.0  64.0  0.0   612.800000  29298.560000   611.733333  28027.128889   \n",
       "8480  3.0  75.0  0.0   418.956522   2734.563327   418.956522   2523.780718   \n",
       "8481  1.0  58.0  1.0  1235.428571    700.244898  1235.714286    897.632653   \n",
       "8482  2.0  75.0  1.0   823.800000   4775.560000   824.000000   4056.000000   \n",
       "8483  1.0  66.0  1.0  1100.000000   3967.000000  1095.500000   6184.750000   \n",
       "\n",
       "                7             8          9  ...            245       246  \\\n",
       "0      446.500000   5097.950000  -8.700000  ...   80062.392773  0.670011   \n",
       "1      356.076923  11891.224852  -1.307692  ...   76406.068692  0.850813   \n",
       "2      478.210526   2771.745152   0.315789  ...   85871.392384  0.489407   \n",
       "3      584.533333   6410.382222  -0.800000  ...  103785.759900  1.091541   \n",
       "4      905.000000  10025.000000   2.800000  ...   81079.175405  0.800294   \n",
       "...           ...           ...        ...  ...            ...       ...   \n",
       "8479   614.933333  28417.528889  -5.866667  ...  100877.375033  0.305497   \n",
       "8480   419.478261   8352.075614  -3.565217  ...   81455.005289  0.448115   \n",
       "8481  1248.285714  15626.775510  16.857143  ...   81763.393500  0.092227   \n",
       "8482   804.000000  11132.800000 -22.600000  ...   66478.783260  1.024300   \n",
       "8483  1091.000000   5050.000000 -16.250000  ...   55993.608000  0.410091   \n",
       "\n",
       "           247       248          249   250   251   252         253        254  \n",
       "0    -0.912051  0.018315   242.648806  21.0  21.0  21.0   -1.714518   3.600000  \n",
       "1    -0.498918 -0.076761   974.391877  27.0  27.0  27.0    8.932387  -1.307692  \n",
       "2    -0.973497  0.262626    25.275439  20.0  20.0  20.0    6.346053   0.526316  \n",
       "3    -0.392591 -0.159420     9.601546  16.0  16.0  16.0   11.755690  -0.666667  \n",
       "4    -0.779998 -0.343434    11.737973  11.0  11.0  11.0 -143.141574   2.800000  \n",
       "...        ...       ...          ...   ...   ...   ...         ...        ...  \n",
       "8479 -1.525075 -0.201948  2438.340543  15.0  15.0  15.0   24.802373   8.571429  \n",
       "8480 -1.260327  0.309823    32.322927  24.0  24.0  24.0    5.947163  -3.565217  \n",
       "8481 -1.692873  0.599331    10.950366   8.0   8.0   8.0   18.149394  18.571429  \n",
       "8482 -0.596278  0.147300   335.316688  11.0  11.0  11.0    4.484999 -20.400000  \n",
       "8483 -1.525947 -0.070707    76.835189   8.0   8.0   8.0    4.645446 -18.285714  \n",
       "\n",
       "[8484 rows x 255 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"../df_new_mean_train.csv\")\n",
    "df_train.drop(columns=[\"Unnamed: 0\"],inplace=True)\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1813"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train[df_train[\"0\"] == 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train = df_train[fti_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>245</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>450.600000</td>\n",
       "      <td>1.640000</td>\n",
       "      <td>450.600000</td>\n",
       "      <td>526.440000</td>\n",
       "      <td>446.500000</td>\n",
       "      <td>5097.950000</td>\n",
       "      <td>-8.700000</td>\n",
       "      <td>...</td>\n",
       "      <td>80062.392773</td>\n",
       "      <td>0.670011</td>\n",
       "      <td>-0.912051</td>\n",
       "      <td>0.018315</td>\n",
       "      <td>242.648806</td>\n",
       "      <td>21.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>-1.714518</td>\n",
       "      <td>3.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>357.000000</td>\n",
       "      <td>1.923077</td>\n",
       "      <td>357.076923</td>\n",
       "      <td>538.840237</td>\n",
       "      <td>356.076923</td>\n",
       "      <td>11891.224852</td>\n",
       "      <td>-1.307692</td>\n",
       "      <td>...</td>\n",
       "      <td>76406.068692</td>\n",
       "      <td>0.850813</td>\n",
       "      <td>-0.498918</td>\n",
       "      <td>-0.076761</td>\n",
       "      <td>974.391877</td>\n",
       "      <td>27.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>8.932387</td>\n",
       "      <td>-1.307692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>478.105263</td>\n",
       "      <td>1.041551</td>\n",
       "      <td>479.052632</td>\n",
       "      <td>345.839335</td>\n",
       "      <td>478.210526</td>\n",
       "      <td>2771.745152</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>...</td>\n",
       "      <td>85871.392384</td>\n",
       "      <td>0.489407</td>\n",
       "      <td>-0.973497</td>\n",
       "      <td>0.262626</td>\n",
       "      <td>25.275439</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>6.346053</td>\n",
       "      <td>0.526316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>587.066667</td>\n",
       "      <td>357.262222</td>\n",
       "      <td>587.866667</td>\n",
       "      <td>663.182222</td>\n",
       "      <td>584.533333</td>\n",
       "      <td>6410.382222</td>\n",
       "      <td>-0.800000</td>\n",
       "      <td>...</td>\n",
       "      <td>103785.759900</td>\n",
       "      <td>1.091541</td>\n",
       "      <td>-0.392591</td>\n",
       "      <td>-0.159420</td>\n",
       "      <td>9.601546</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>11.755690</td>\n",
       "      <td>-0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>900.600000</td>\n",
       "      <td>95.240000</td>\n",
       "      <td>900.600000</td>\n",
       "      <td>170.440000</td>\n",
       "      <td>905.000000</td>\n",
       "      <td>10025.000000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>...</td>\n",
       "      <td>81079.175405</td>\n",
       "      <td>0.800294</td>\n",
       "      <td>-0.779998</td>\n",
       "      <td>-0.343434</td>\n",
       "      <td>11.737973</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>-143.141574</td>\n",
       "      <td>2.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8479</th>\n",
       "      <td>0.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>612.800000</td>\n",
       "      <td>29298.560000</td>\n",
       "      <td>611.733333</td>\n",
       "      <td>28027.128889</td>\n",
       "      <td>614.933333</td>\n",
       "      <td>28417.528889</td>\n",
       "      <td>-5.866667</td>\n",
       "      <td>...</td>\n",
       "      <td>100877.375033</td>\n",
       "      <td>0.305497</td>\n",
       "      <td>-1.525075</td>\n",
       "      <td>-0.201948</td>\n",
       "      <td>2438.340543</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>24.802373</td>\n",
       "      <td>8.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8480</th>\n",
       "      <td>3.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>418.956522</td>\n",
       "      <td>2734.563327</td>\n",
       "      <td>418.956522</td>\n",
       "      <td>2523.780718</td>\n",
       "      <td>419.478261</td>\n",
       "      <td>8352.075614</td>\n",
       "      <td>-3.565217</td>\n",
       "      <td>...</td>\n",
       "      <td>81455.005289</td>\n",
       "      <td>0.448115</td>\n",
       "      <td>-1.260327</td>\n",
       "      <td>0.309823</td>\n",
       "      <td>32.322927</td>\n",
       "      <td>24.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>5.947163</td>\n",
       "      <td>-3.565217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8481</th>\n",
       "      <td>1.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1235.428571</td>\n",
       "      <td>700.244898</td>\n",
       "      <td>1235.714286</td>\n",
       "      <td>897.632653</td>\n",
       "      <td>1248.285714</td>\n",
       "      <td>15626.775510</td>\n",
       "      <td>16.857143</td>\n",
       "      <td>...</td>\n",
       "      <td>81763.393500</td>\n",
       "      <td>0.092227</td>\n",
       "      <td>-1.692873</td>\n",
       "      <td>0.599331</td>\n",
       "      <td>10.950366</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>18.149394</td>\n",
       "      <td>18.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8482</th>\n",
       "      <td>2.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>823.800000</td>\n",
       "      <td>4775.560000</td>\n",
       "      <td>824.000000</td>\n",
       "      <td>4056.000000</td>\n",
       "      <td>804.000000</td>\n",
       "      <td>11132.800000</td>\n",
       "      <td>-22.600000</td>\n",
       "      <td>...</td>\n",
       "      <td>66478.783260</td>\n",
       "      <td>1.024300</td>\n",
       "      <td>-0.596278</td>\n",
       "      <td>0.147300</td>\n",
       "      <td>335.316688</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>4.484999</td>\n",
       "      <td>-20.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8483</th>\n",
       "      <td>1.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1100.000000</td>\n",
       "      <td>3967.000000</td>\n",
       "      <td>1095.500000</td>\n",
       "      <td>6184.750000</td>\n",
       "      <td>1091.000000</td>\n",
       "      <td>5050.000000</td>\n",
       "      <td>-16.250000</td>\n",
       "      <td>...</td>\n",
       "      <td>55993.608000</td>\n",
       "      <td>0.410091</td>\n",
       "      <td>-1.525947</td>\n",
       "      <td>-0.070707</td>\n",
       "      <td>76.835189</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.645446</td>\n",
       "      <td>-18.285714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8484 rows × 255 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0     1    2            3             4            5             6  \\\n",
       "0     3.0  58.0  1.0   450.600000      1.640000   450.600000    526.440000   \n",
       "1     3.0  41.0  0.0   357.000000      1.923077   357.076923    538.840237   \n",
       "2     3.0  76.0  0.0   478.105263      1.041551   479.052632    345.839335   \n",
       "3     3.0  27.0  1.0   587.066667    357.262222   587.866667    663.182222   \n",
       "4     2.0  76.0  0.0   900.600000     95.240000   900.600000    170.440000   \n",
       "...   ...   ...  ...          ...           ...          ...           ...   \n",
       "8479  0.0  64.0  0.0   612.800000  29298.560000   611.733333  28027.128889   \n",
       "8480  3.0  75.0  0.0   418.956522   2734.563327   418.956522   2523.780718   \n",
       "8481  1.0  58.0  1.0  1235.428571    700.244898  1235.714286    897.632653   \n",
       "8482  2.0  75.0  1.0   823.800000   4775.560000   824.000000   4056.000000   \n",
       "8483  1.0  66.0  1.0  1100.000000   3967.000000  1095.500000   6184.750000   \n",
       "\n",
       "                7             8          9  ...            245       246  \\\n",
       "0      446.500000   5097.950000  -8.700000  ...   80062.392773  0.670011   \n",
       "1      356.076923  11891.224852  -1.307692  ...   76406.068692  0.850813   \n",
       "2      478.210526   2771.745152   0.315789  ...   85871.392384  0.489407   \n",
       "3      584.533333   6410.382222  -0.800000  ...  103785.759900  1.091541   \n",
       "4      905.000000  10025.000000   2.800000  ...   81079.175405  0.800294   \n",
       "...           ...           ...        ...  ...            ...       ...   \n",
       "8479   614.933333  28417.528889  -5.866667  ...  100877.375033  0.305497   \n",
       "8480   419.478261   8352.075614  -3.565217  ...   81455.005289  0.448115   \n",
       "8481  1248.285714  15626.775510  16.857143  ...   81763.393500  0.092227   \n",
       "8482   804.000000  11132.800000 -22.600000  ...   66478.783260  1.024300   \n",
       "8483  1091.000000   5050.000000 -16.250000  ...   55993.608000  0.410091   \n",
       "\n",
       "           247       248          249   250   251   252         253        254  \n",
       "0    -0.912051  0.018315   242.648806  21.0  21.0  21.0   -1.714518   3.600000  \n",
       "1    -0.498918 -0.076761   974.391877  27.0  27.0  27.0    8.932387  -1.307692  \n",
       "2    -0.973497  0.262626    25.275439  20.0  20.0  20.0    6.346053   0.526316  \n",
       "3    -0.392591 -0.159420     9.601546  16.0  16.0  16.0   11.755690  -0.666667  \n",
       "4    -0.779998 -0.343434    11.737973  11.0  11.0  11.0 -143.141574   2.800000  \n",
       "...        ...       ...          ...   ...   ...   ...         ...        ...  \n",
       "8479 -1.525075 -0.201948  2438.340543  15.0  15.0  15.0   24.802373   8.571429  \n",
       "8480 -1.260327  0.309823    32.322927  24.0  24.0  24.0    5.947163  -3.565217  \n",
       "8481 -1.692873  0.599331    10.950366   8.0   8.0   8.0   18.149394  18.571429  \n",
       "8482 -0.596278  0.147300   335.316688  11.0  11.0  11.0    4.484999 -20.400000  \n",
       "8483 -1.525947 -0.070707    76.835189   8.0   8.0   8.0    4.645446 -18.285714  \n",
       "\n",
       "[8484 rows x 255 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.replace([np.inf, -np.inf], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = df_train.iloc[:,1:].values    \n",
    "y_train = df_train.iloc[:,0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pca(x, n):\n",
    "    pca = PCA(n_components= n)\n",
    "    result = pca.fit_transform(x)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = MinMaxScaler()\n",
    "x_train = scale.fit_transform(x_train)\n",
    "# x_train = apply_pca(x_train, len(x_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>245</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1019.500000</td>\n",
       "      <td>1322.750000</td>\n",
       "      <td>1023.750000</td>\n",
       "      <td>1814.437500</td>\n",
       "      <td>1027.500000</td>\n",
       "      <td>13622.750000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>59369.005195</td>\n",
       "      <td>-0.395614</td>\n",
       "      <td>-1.405612</td>\n",
       "      <td>0.209280</td>\n",
       "      <td>147.535415</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>29.841092</td>\n",
       "      <td>4.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1070.500000</td>\n",
       "      <td>210.750000</td>\n",
       "      <td>1070.000000</td>\n",
       "      <td>220.000000</td>\n",
       "      <td>1091.000000</td>\n",
       "      <td>8003.000000</td>\n",
       "      <td>7.250000</td>\n",
       "      <td>...</td>\n",
       "      <td>14338.620840</td>\n",
       "      <td>1.581183</td>\n",
       "      <td>1.756293</td>\n",
       "      <td>-0.133602</td>\n",
       "      <td>8.406926</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.658428</td>\n",
       "      <td>7.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1089.750000</td>\n",
       "      <td>212.437500</td>\n",
       "      <td>1092.500000</td>\n",
       "      <td>780.750000</td>\n",
       "      <td>1089.750000</td>\n",
       "      <td>4948.437500</td>\n",
       "      <td>-2.750000</td>\n",
       "      <td>...</td>\n",
       "      <td>68352.620089</td>\n",
       "      <td>-0.017632</td>\n",
       "      <td>-1.643916</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>1.054053</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.429165</td>\n",
       "      <td>-2.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>442.476190</td>\n",
       "      <td>6.820862</td>\n",
       "      <td>441.619048</td>\n",
       "      <td>600.616780</td>\n",
       "      <td>440.380952</td>\n",
       "      <td>7582.331066</td>\n",
       "      <td>-4.857143</td>\n",
       "      <td>...</td>\n",
       "      <td>66255.006625</td>\n",
       "      <td>0.981365</td>\n",
       "      <td>-0.380309</td>\n",
       "      <td>0.179710</td>\n",
       "      <td>11.956238</td>\n",
       "      <td>22.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>4.172672</td>\n",
       "      <td>-4.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>702.307692</td>\n",
       "      <td>11491.597633</td>\n",
       "      <td>702.615385</td>\n",
       "      <td>11264.852071</td>\n",
       "      <td>702.307692</td>\n",
       "      <td>24367.289941</td>\n",
       "      <td>2.769231</td>\n",
       "      <td>...</td>\n",
       "      <td>65080.950533</td>\n",
       "      <td>0.923223</td>\n",
       "      <td>-0.621266</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.071816</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>6.103110</td>\n",
       "      <td>2.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2117</th>\n",
       "      <td>3.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>570.875000</td>\n",
       "      <td>71.484375</td>\n",
       "      <td>570.875000</td>\n",
       "      <td>430.484375</td>\n",
       "      <td>573.875000</td>\n",
       "      <td>4805.234375</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>...</td>\n",
       "      <td>79562.344747</td>\n",
       "      <td>0.584774</td>\n",
       "      <td>-1.051400</td>\n",
       "      <td>0.326923</td>\n",
       "      <td>76.133747</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>7.092578</td>\n",
       "      <td>-1.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2118</th>\n",
       "      <td>0.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>602.800000</td>\n",
       "      <td>15807.893333</td>\n",
       "      <td>604.266667</td>\n",
       "      <td>14172.728889</td>\n",
       "      <td>599.866667</td>\n",
       "      <td>14175.182222</td>\n",
       "      <td>2.266667</td>\n",
       "      <td>...</td>\n",
       "      <td>80598.806765</td>\n",
       "      <td>0.095878</td>\n",
       "      <td>-1.053186</td>\n",
       "      <td>-0.317312</td>\n",
       "      <td>51.143449</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2.935853</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2119</th>\n",
       "      <td>0.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1001.333333</td>\n",
       "      <td>13557.333333</td>\n",
       "      <td>1004.888889</td>\n",
       "      <td>12702.320988</td>\n",
       "      <td>1018.444444</td>\n",
       "      <td>8031.802469</td>\n",
       "      <td>6.888889</td>\n",
       "      <td>...</td>\n",
       "      <td>44918.959424</td>\n",
       "      <td>0.894128</td>\n",
       "      <td>-0.951440</td>\n",
       "      <td>0.178571</td>\n",
       "      <td>0.792483</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.692256</td>\n",
       "      <td>7.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2120</th>\n",
       "      <td>1.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1024.888889</td>\n",
       "      <td>1051.654321</td>\n",
       "      <td>1021.111111</td>\n",
       "      <td>2544.098765</td>\n",
       "      <td>1036.222222</td>\n",
       "      <td>2333.728395</td>\n",
       "      <td>20.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>1689.869824</td>\n",
       "      <td>0.208140</td>\n",
       "      <td>-1.684963</td>\n",
       "      <td>-0.157143</td>\n",
       "      <td>42.377293</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.571234</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2121</th>\n",
       "      <td>2.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>822.909091</td>\n",
       "      <td>187.173554</td>\n",
       "      <td>823.272727</td>\n",
       "      <td>670.016529</td>\n",
       "      <td>826.000000</td>\n",
       "      <td>12440.000000</td>\n",
       "      <td>4.727273</td>\n",
       "      <td>...</td>\n",
       "      <td>106370.986667</td>\n",
       "      <td>0.534113</td>\n",
       "      <td>-1.322970</td>\n",
       "      <td>-0.137652</td>\n",
       "      <td>6.667520</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>53.005012</td>\n",
       "      <td>4.727273</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2122 rows × 255 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0     1    2            3             4            5             6  \\\n",
       "0     1.0  44.0  1.0  1019.500000   1322.750000  1023.750000   1814.437500   \n",
       "1     1.0  64.0  0.0  1070.500000    210.750000  1070.000000    220.000000   \n",
       "2     1.0  69.0  1.0  1089.750000    212.437500  1092.500000    780.750000   \n",
       "3     0.0  68.0  0.0   442.476190      6.820862   441.619048    600.616780   \n",
       "4     0.0  86.0  0.0   702.307692  11491.597633   702.615385  11264.852071   \n",
       "...   ...   ...  ...          ...           ...          ...           ...   \n",
       "2117  3.0  40.0  1.0   570.875000     71.484375   570.875000    430.484375   \n",
       "2118  0.0  51.0  1.0   602.800000  15807.893333   604.266667  14172.728889   \n",
       "2119  0.0  77.0  1.0  1001.333333  13557.333333  1004.888889  12702.320988   \n",
       "2120  1.0  36.0  0.0  1024.888889   1051.654321  1021.111111   2544.098765   \n",
       "2121  2.0  58.0  0.0   822.909091    187.173554   823.272727    670.016529   \n",
       "\n",
       "                7             8          9  ...            245       246  \\\n",
       "0     1027.500000  13622.750000   4.000000  ...   59369.005195 -0.395614   \n",
       "1     1091.000000   8003.000000   7.250000  ...   14338.620840  1.581183   \n",
       "2     1089.750000   4948.437500  -2.750000  ...   68352.620089 -0.017632   \n",
       "3      440.380952   7582.331066  -4.857143  ...   66255.006625  0.981365   \n",
       "4      702.307692  24367.289941   2.769231  ...   65080.950533  0.923223   \n",
       "...           ...           ...        ...  ...            ...       ...   \n",
       "2117   573.875000   4805.234375   0.625000  ...   79562.344747  0.584774   \n",
       "2118   599.866667  14175.182222   2.266667  ...   80598.806765  0.095878   \n",
       "2119  1018.444444   8031.802469   6.888889  ...   44918.959424  0.894128   \n",
       "2120  1036.222222   2333.728395  20.666667  ...    1689.869824  0.208140   \n",
       "2121   826.000000  12440.000000   4.727273  ...  106370.986667  0.534113   \n",
       "\n",
       "           247       248         249   250   251   252        253        254  \n",
       "0    -1.405612  0.209280  147.535415   9.0   9.0   9.0  29.841092   4.750000  \n",
       "1     1.756293 -0.133602    8.406926   9.0   9.0   9.0   3.658428   7.250000  \n",
       "2    -1.643916 -0.100000    1.054053   9.0   9.0   9.0   5.429165  -2.250000  \n",
       "3    -0.380309  0.179710   11.956238  22.0  22.0  22.0   4.172672  -4.857143  \n",
       "4    -0.621266  0.000000    3.071816  13.0  13.0  13.0   6.103110   2.833333  \n",
       "...        ...       ...         ...   ...   ...   ...        ...        ...  \n",
       "2117 -1.051400  0.326923   76.133747  17.0  17.0  17.0   7.092578  -1.750000  \n",
       "2118 -1.053186 -0.317312   51.143449  17.0  17.0  17.0   2.935853   0.750000  \n",
       "2119 -0.951440  0.178571    0.792483  10.0  10.0  10.0   3.692256   7.111111  \n",
       "2120 -1.684963 -0.157143   42.377293  10.0  10.0  10.0   3.571234  20.000000  \n",
       "2121 -1.322970 -0.137652    6.667520  12.0  12.0  12.0  53.005012   4.727273  \n",
       "\n",
       "[2122 rows x 255 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv(\"../df_new_mean_test.csv\")\n",
    "df_test.drop(columns=[\"Unnamed: 0\"],inplace=True)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test = df_test[fti_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>245</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1019.500000</td>\n",
       "      <td>1322.750000</td>\n",
       "      <td>1023.750000</td>\n",
       "      <td>1814.437500</td>\n",
       "      <td>1027.500000</td>\n",
       "      <td>13622.750000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>59369.005195</td>\n",
       "      <td>-0.395614</td>\n",
       "      <td>-1.405612</td>\n",
       "      <td>0.209280</td>\n",
       "      <td>147.535415</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>29.841092</td>\n",
       "      <td>4.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1070.500000</td>\n",
       "      <td>210.750000</td>\n",
       "      <td>1070.000000</td>\n",
       "      <td>220.000000</td>\n",
       "      <td>1091.000000</td>\n",
       "      <td>8003.000000</td>\n",
       "      <td>7.250000</td>\n",
       "      <td>...</td>\n",
       "      <td>14338.620840</td>\n",
       "      <td>1.581183</td>\n",
       "      <td>1.756293</td>\n",
       "      <td>-0.133602</td>\n",
       "      <td>8.406926</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.658428</td>\n",
       "      <td>7.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1089.750000</td>\n",
       "      <td>212.437500</td>\n",
       "      <td>1092.500000</td>\n",
       "      <td>780.750000</td>\n",
       "      <td>1089.750000</td>\n",
       "      <td>4948.437500</td>\n",
       "      <td>-2.750000</td>\n",
       "      <td>...</td>\n",
       "      <td>68352.620089</td>\n",
       "      <td>-0.017632</td>\n",
       "      <td>-1.643916</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>1.054053</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.429165</td>\n",
       "      <td>-2.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>442.476190</td>\n",
       "      <td>6.820862</td>\n",
       "      <td>441.619048</td>\n",
       "      <td>600.616780</td>\n",
       "      <td>440.380952</td>\n",
       "      <td>7582.331066</td>\n",
       "      <td>-4.857143</td>\n",
       "      <td>...</td>\n",
       "      <td>66255.006625</td>\n",
       "      <td>0.981365</td>\n",
       "      <td>-0.380309</td>\n",
       "      <td>0.179710</td>\n",
       "      <td>11.956238</td>\n",
       "      <td>22.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>4.172672</td>\n",
       "      <td>-4.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>702.307692</td>\n",
       "      <td>11491.597633</td>\n",
       "      <td>702.615385</td>\n",
       "      <td>11264.852071</td>\n",
       "      <td>702.307692</td>\n",
       "      <td>24367.289941</td>\n",
       "      <td>2.769231</td>\n",
       "      <td>...</td>\n",
       "      <td>65080.950533</td>\n",
       "      <td>0.923223</td>\n",
       "      <td>-0.621266</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.071816</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>6.103110</td>\n",
       "      <td>2.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2117</th>\n",
       "      <td>3.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>570.875000</td>\n",
       "      <td>71.484375</td>\n",
       "      <td>570.875000</td>\n",
       "      <td>430.484375</td>\n",
       "      <td>573.875000</td>\n",
       "      <td>4805.234375</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>...</td>\n",
       "      <td>79562.344747</td>\n",
       "      <td>0.584774</td>\n",
       "      <td>-1.051400</td>\n",
       "      <td>0.326923</td>\n",
       "      <td>76.133747</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>7.092578</td>\n",
       "      <td>-1.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2118</th>\n",
       "      <td>0.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>602.800000</td>\n",
       "      <td>15807.893333</td>\n",
       "      <td>604.266667</td>\n",
       "      <td>14172.728889</td>\n",
       "      <td>599.866667</td>\n",
       "      <td>14175.182222</td>\n",
       "      <td>2.266667</td>\n",
       "      <td>...</td>\n",
       "      <td>80598.806765</td>\n",
       "      <td>0.095878</td>\n",
       "      <td>-1.053186</td>\n",
       "      <td>-0.317312</td>\n",
       "      <td>51.143449</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2.935853</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2119</th>\n",
       "      <td>0.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1001.333333</td>\n",
       "      <td>13557.333333</td>\n",
       "      <td>1004.888889</td>\n",
       "      <td>12702.320988</td>\n",
       "      <td>1018.444444</td>\n",
       "      <td>8031.802469</td>\n",
       "      <td>6.888889</td>\n",
       "      <td>...</td>\n",
       "      <td>44918.959424</td>\n",
       "      <td>0.894128</td>\n",
       "      <td>-0.951440</td>\n",
       "      <td>0.178571</td>\n",
       "      <td>0.792483</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.692256</td>\n",
       "      <td>7.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2120</th>\n",
       "      <td>1.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1024.888889</td>\n",
       "      <td>1051.654321</td>\n",
       "      <td>1021.111111</td>\n",
       "      <td>2544.098765</td>\n",
       "      <td>1036.222222</td>\n",
       "      <td>2333.728395</td>\n",
       "      <td>20.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>1689.869824</td>\n",
       "      <td>0.208140</td>\n",
       "      <td>-1.684963</td>\n",
       "      <td>-0.157143</td>\n",
       "      <td>42.377293</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.571234</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2121</th>\n",
       "      <td>2.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>822.909091</td>\n",
       "      <td>187.173554</td>\n",
       "      <td>823.272727</td>\n",
       "      <td>670.016529</td>\n",
       "      <td>826.000000</td>\n",
       "      <td>12440.000000</td>\n",
       "      <td>4.727273</td>\n",
       "      <td>...</td>\n",
       "      <td>106370.986667</td>\n",
       "      <td>0.534113</td>\n",
       "      <td>-1.322970</td>\n",
       "      <td>-0.137652</td>\n",
       "      <td>6.667520</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>53.005012</td>\n",
       "      <td>4.727273</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2122 rows × 255 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0     1    2            3             4            5             6  \\\n",
       "0     1.0  44.0  1.0  1019.500000   1322.750000  1023.750000   1814.437500   \n",
       "1     1.0  64.0  0.0  1070.500000    210.750000  1070.000000    220.000000   \n",
       "2     1.0  69.0  1.0  1089.750000    212.437500  1092.500000    780.750000   \n",
       "3     0.0  68.0  0.0   442.476190      6.820862   441.619048    600.616780   \n",
       "4     0.0  86.0  0.0   702.307692  11491.597633   702.615385  11264.852071   \n",
       "...   ...   ...  ...          ...           ...          ...           ...   \n",
       "2117  3.0  40.0  1.0   570.875000     71.484375   570.875000    430.484375   \n",
       "2118  0.0  51.0  1.0   602.800000  15807.893333   604.266667  14172.728889   \n",
       "2119  0.0  77.0  1.0  1001.333333  13557.333333  1004.888889  12702.320988   \n",
       "2120  1.0  36.0  0.0  1024.888889   1051.654321  1021.111111   2544.098765   \n",
       "2121  2.0  58.0  0.0   822.909091    187.173554   823.272727    670.016529   \n",
       "\n",
       "                7             8          9  ...            245       246  \\\n",
       "0     1027.500000  13622.750000   4.000000  ...   59369.005195 -0.395614   \n",
       "1     1091.000000   8003.000000   7.250000  ...   14338.620840  1.581183   \n",
       "2     1089.750000   4948.437500  -2.750000  ...   68352.620089 -0.017632   \n",
       "3      440.380952   7582.331066  -4.857143  ...   66255.006625  0.981365   \n",
       "4      702.307692  24367.289941   2.769231  ...   65080.950533  0.923223   \n",
       "...           ...           ...        ...  ...            ...       ...   \n",
       "2117   573.875000   4805.234375   0.625000  ...   79562.344747  0.584774   \n",
       "2118   599.866667  14175.182222   2.266667  ...   80598.806765  0.095878   \n",
       "2119  1018.444444   8031.802469   6.888889  ...   44918.959424  0.894128   \n",
       "2120  1036.222222   2333.728395  20.666667  ...    1689.869824  0.208140   \n",
       "2121   826.000000  12440.000000   4.727273  ...  106370.986667  0.534113   \n",
       "\n",
       "           247       248         249   250   251   252        253        254  \n",
       "0    -1.405612  0.209280  147.535415   9.0   9.0   9.0  29.841092   4.750000  \n",
       "1     1.756293 -0.133602    8.406926   9.0   9.0   9.0   3.658428   7.250000  \n",
       "2    -1.643916 -0.100000    1.054053   9.0   9.0   9.0   5.429165  -2.250000  \n",
       "3    -0.380309  0.179710   11.956238  22.0  22.0  22.0   4.172672  -4.857143  \n",
       "4    -0.621266  0.000000    3.071816  13.0  13.0  13.0   6.103110   2.833333  \n",
       "...        ...       ...         ...   ...   ...   ...        ...        ...  \n",
       "2117 -1.051400  0.326923   76.133747  17.0  17.0  17.0   7.092578  -1.750000  \n",
       "2118 -1.053186 -0.317312   51.143449  17.0  17.0  17.0   2.935853   0.750000  \n",
       "2119 -0.951440  0.178571    0.792483  10.0  10.0  10.0   3.692256   7.111111  \n",
       "2120 -1.684963 -0.157143   42.377293  10.0  10.0  10.0   3.571234  20.000000  \n",
       "2121 -1.322970 -0.137652    6.667520  12.0  12.0  12.0  53.005012   4.727273  \n",
       "\n",
       "[2122 rows x 255 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = df_test.iloc[:,1:].values\n",
    "y_test = df_test.iloc[:,0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = scale.transform(x_test)\n",
    "# x_test = apply_pca(x_test, len(x_test[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create based model\n",
    "level0 = list()\n",
    "level0.append(('RF', RandomForestClassifier(criterion='gini', max_depth=5, max_features='sqrt', n_estimators=1000)))\n",
    "level0.append(('AB', AdaBoostClassifier(algorithm='SAMME.R', learning_rate=0.1, n_estimators=100)))\n",
    "level0.append(('GB', GradientBoostingClassifier(criterion= 'friedman_mse',learning_rate= 0.1,loss= 'log_loss',n_estimators= 100)))\n",
    "level0.append(('XGB', XGBClassifier(gamma= 0.2,learning_rate= 0.1,max_depth= 4,min_child_weight= 1,n_estimators= 100)))\n",
    "level0.append(('LGB', LGBMClassifier(boosting= 'dart', data_sample_strategy= 'bagging', estimators= 50, learning_rate=0.1, objective= 'multiclass')))\n",
    "\n",
    "\n",
    "level1 = list()\n",
    "level1.append(('LR', LogisticRegression(C= 10, max_iter= 100, penalty= 'l1', solver= 'liblinear')))\n",
    "level1.append(('SVM', SVC(C= 100, gamma= 'scale', kernel= 'rbf', probability= True)))\n",
    "level1.append(('DT', DecisionTreeClassifier(criterion= 'entropy',max_depth= 5,max_features= 'sqrt',splitter= 'best')))\n",
    "level1.append(('KNN', KNeighborsClassifier(algorithm= 'auto', n_neighbors= 4, p= 1, weights= 'uniform')))\n",
    "level1.append(('CB', CatBoostClassifier(iterations= 200, learning_rate=0.1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: RF\n",
      "----------------\n",
      "Fold 0\n",
      "train: 7635\n",
      "test: 849\n",
      "Fold 1\n",
      "train: 7635\n",
      "test: 849\n",
      "Fold 2\n",
      "train: 7635\n",
      "test: 849\n",
      "Fold 3\n",
      "train: 7635\n",
      "test: 849\n",
      "Fold 4\n",
      "train: 7636\n",
      "test: 848\n",
      "Fold 5\n",
      "train: 7636\n",
      "test: 848\n",
      "Fold 6\n",
      "train: 7636\n",
      "test: 848\n",
      "Fold 7\n",
      "train: 7636\n",
      "test: 848\n",
      "Fold 8\n",
      "train: 7636\n",
      "test: 848\n",
      "Fold 9\n",
      "train: 7636\n",
      "test: 848\n",
      "-------Done-------\n",
      "Model: AB\n",
      "----------------\n",
      "Fold 0\n",
      "train: 7635\n",
      "test: 849\n",
      "Fold 1\n",
      "train: 7635\n",
      "test: 849\n",
      "Fold 2\n",
      "train: 7635\n",
      "test: 849\n",
      "Fold 3\n",
      "train: 7635\n",
      "test: 849\n",
      "Fold 4\n",
      "train: 7636\n",
      "test: 848\n",
      "Fold 5\n",
      "train: 7636\n",
      "test: 848\n",
      "Fold 6\n",
      "train: 7636\n",
      "test: 848\n",
      "Fold 7\n",
      "train: 7636\n",
      "test: 848\n",
      "Fold 8\n",
      "train: 7636\n",
      "test: 848\n",
      "Fold 9\n",
      "train: 7636\n",
      "test: 848\n",
      "-------Done-------\n",
      "Model: GB\n",
      "----------------\n",
      "Fold 0\n",
      "train: 7635\n",
      "test: 849\n",
      "Fold 1\n",
      "train: 7635\n",
      "test: 849\n",
      "Fold 2\n",
      "train: 7635\n",
      "test: 849\n",
      "Fold 3\n",
      "train: 7635\n",
      "test: 849\n",
      "Fold 4\n",
      "train: 7636\n",
      "test: 848\n",
      "Fold 5\n",
      "train: 7636\n",
      "test: 848\n",
      "Fold 6\n",
      "train: 7636\n",
      "test: 848\n",
      "Fold 7\n",
      "train: 7636\n",
      "test: 848\n",
      "Fold 8\n",
      "train: 7636\n",
      "test: 848\n",
      "Fold 9\n",
      "train: 7636\n",
      "test: 848\n",
      "-------Done-------\n",
      "Model: XGB\n",
      "----------------\n",
      "Fold 0\n",
      "train: 7635\n",
      "test: 849\n",
      "Fold 1\n",
      "train: 7635\n",
      "test: 849\n",
      "Fold 2\n",
      "train: 7635\n",
      "test: 849\n",
      "Fold 3\n",
      "train: 7635\n",
      "test: 849\n",
      "Fold 4\n",
      "train: 7636\n",
      "test: 848\n",
      "Fold 5\n",
      "train: 7636\n",
      "test: 848\n",
      "Fold 6\n",
      "train: 7636\n",
      "test: 848\n",
      "Fold 7\n",
      "train: 7636\n",
      "test: 848\n",
      "Fold 8\n",
      "train: 7636\n",
      "test: 848\n",
      "Fold 9\n",
      "train: 7636\n",
      "test: 848\n",
      "-------Done-------\n",
      "Model: LGB\n",
      "----------------\n",
      "Fold 0\n",
      "train: 7635\n",
      "test: 849\n",
      "[LightGBM] [Warning] Unknown parameter: estimators\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] Unknown parameter: estimators\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009476 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56241\n",
      "[LightGBM] [Info] Number of data points in the train set: 7635, number of used features: 254\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score -1.557752\n",
      "[LightGBM] [Info] Start training from score -1.003124\n",
      "[LightGBM] [Info] Start training from score -1.575318\n",
      "[LightGBM] [Info] Start training from score -1.533787\n",
      "[LightGBM] [Warning] Unknown parameter: estimators\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Fold 1\n",
      "train: 7635\n",
      "test: 849\n",
      "[LightGBM] [Warning] Unknown parameter: estimators\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] Unknown parameter: estimators\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011790 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56241\n",
      "[LightGBM] [Info] Number of data points in the train set: 7635, number of used features: 254\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score -1.558374\n",
      "[LightGBM] [Info] Start training from score -0.997781\n",
      "[LightGBM] [Info] Start training from score -1.570268\n",
      "[LightGBM] [Info] Start training from score -1.547235\n",
      "[LightGBM] [Warning] Unknown parameter: estimators\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Fold 2\n",
      "train: 7635\n",
      "test: 849\n",
      "[LightGBM] [Warning] Unknown parameter: estimators\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] Unknown parameter: estimators\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011982 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56238\n",
      "[LightGBM] [Info] Number of data points in the train set: 7635, number of used features: 254\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score -1.572790\n",
      "[LightGBM] [Info] Start training from score -0.996716\n",
      "[LightGBM] [Info] Start training from score -1.561490\n",
      "[LightGBM] [Info] Start training from score -1.543550\n",
      "[LightGBM] [Warning] Unknown parameter: estimators\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Fold 3\n",
      "train: 7635\n",
      "test: 849\n",
      "[LightGBM] [Warning] Unknown parameter: estimators\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] Unknown parameter: estimators\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008280 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56238\n",
      "[LightGBM] [Info] Number of data points in the train set: 7635, number of used features: 254\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score -1.566496\n",
      "[LightGBM] [Info] Start training from score -1.007777\n",
      "[LightGBM] [Info] Start training from score -1.549083\n",
      "[LightGBM] [Info] Start training from score -1.542937\n",
      "[LightGBM] [Warning] Unknown parameter: estimators\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Fold 4\n",
      "train: 7636\n",
      "test: 848\n",
      "[LightGBM] [Warning] Unknown parameter: estimators\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] Unknown parameter: estimators\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008944 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56241\n",
      "[LightGBM] [Info] Number of data points in the train set: 7636, number of used features: 254\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score -1.557261\n",
      "[LightGBM] [Info] Start training from score -1.000045\n",
      "[LightGBM] [Info] Start training from score -1.561621\n",
      "[LightGBM] [Info] Start training from score -1.552920\n",
      "[LightGBM] [Warning] Unknown parameter: estimators\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Fold 5\n",
      "train: 7636\n",
      "test: 848\n",
      "[LightGBM] [Warning] Unknown parameter: estimators\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] Unknown parameter: estimators\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008684 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56241\n",
      "[LightGBM] [Info] Number of data points in the train set: 7636, number of used features: 254\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score -1.572921\n",
      "[LightGBM] [Info] Start training from score -1.007549\n",
      "[LightGBM] [Info] Start training from score -1.558505\n",
      "[LightGBM] [Info] Start training from score -1.527865\n",
      "[LightGBM] [Warning] Unknown parameter: estimators\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Fold 6\n",
      "train: 7636\n",
      "test: 848\n",
      "[LightGBM] [Warning] Unknown parameter: estimators\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] Unknown parameter: estimators\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008177 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56238\n",
      "[LightGBM] [Info] Number of data points in the train set: 7636, number of used features: 254\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score -1.551065\n",
      "[LightGBM] [Info] Start training from score -1.014388\n",
      "[LightGBM] [Info] Start training from score -1.546136\n",
      "[LightGBM] [Info] Start training from score -1.549831\n",
      "[LightGBM] [Warning] Unknown parameter: estimators\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Fold 7\n",
      "train: 7636\n",
      "test: 848\n",
      "[LightGBM] [Warning] Unknown parameter: estimators\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] Unknown parameter: estimators\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009101 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56235\n",
      "[LightGBM] [Info] Number of data points in the train set: 7636, number of used features: 254\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score -1.562245\n",
      "[LightGBM] [Info] Start training from score -1.005758\n",
      "[LightGBM] [Info] Start training from score -1.560373\n",
      "[LightGBM] [Info] Start training from score -1.539398\n",
      "[LightGBM] [Warning] Unknown parameter: estimators\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Fold 8\n",
      "train: 7636\n",
      "test: 848\n",
      "[LightGBM] [Warning] Unknown parameter: estimators\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] Unknown parameter: estimators\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010087 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56241\n",
      "[LightGBM] [Info] Number of data points in the train set: 7636, number of used features: 254\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score -1.561621\n",
      "[LightGBM] [Info] Start training from score -0.995782\n",
      "[LightGBM] [Info] Start training from score -1.566000\n",
      "[LightGBM] [Info] Start training from score -1.551683\n",
      "[LightGBM] [Warning] Unknown parameter: estimators\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Fold 9\n",
      "train: 7636\n",
      "test: 848\n",
      "[LightGBM] [Warning] Unknown parameter: estimators\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] Unknown parameter: estimators\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010198 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 56238\n",
      "[LightGBM] [Info] Number of data points in the train set: 7636, number of used features: 254\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score -1.555398\n",
      "[LightGBM] [Info] Start training from score -1.003612\n",
      "[LightGBM] [Info] Start training from score -1.567255\n",
      "[LightGBM] [Info] Start training from score -1.543068\n",
      "[LightGBM] [Warning] Unknown parameter: estimators\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] Unknown parameter: estimators\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] Unknown parameter: estimators\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008112 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 56241\n",
      "[LightGBM] [Info] Number of data points in the train set: 8484, number of used features: 254\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score -1.561569\n",
      "[LightGBM] [Info] Start training from score -1.003238\n",
      "[LightGBM] [Info] Start training from score -1.561569\n",
      "[LightGBM] [Info] Start training from score -1.543199\n",
      "[LightGBM] [Warning] Unknown parameter: estimators\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "-------Done-------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold,StratifiedKFold,StratifiedShuffleSplit,StratifiedGroupKFold\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "kf = KFold(n_splits=10,random_state=None)\n",
    "\n",
    "train_news = np.empty((x_train.shape[0], 0))\n",
    "test_news = np.empty((x_test.shape[0], 0))\n",
    "\n",
    "for name, model in level0:\n",
    "    print(f\"Model: {name}\")\n",
    "    print(\"----------------\")\n",
    "    train_model = []\n",
    "    for i,(train_index, test_index) in enumerate(kf.split(x_train, y_train)):\n",
    "        print(f\"Fold {i}\")\n",
    "        print(f\"train: {len(train_index)}\")\n",
    "        print(f\"test: {len(test_index)}\")\n",
    "        # Train model in 4/5\n",
    "        model.fit(x_train[train_index], y_train[train_index])\n",
    "        # predict model in 1/5\n",
    "        y_pred = model.predict(x_train[test_index])\n",
    "        train_model.extend(y_pred)\n",
    "    # Predict on test set\n",
    "    model.fit(x_train, y_train)\n",
    "    test_model = model.predict(x_test)\n",
    "    # Add column to news data\n",
    "    train_news = np.column_stack((train_news, np.array(train_model).reshape(-1, 1)))\n",
    "    test_news = np.column_stack((test_news, np.array(test_model).reshape(-1, 1)))\n",
    "    print(\"-------Done-------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_news_final = pd.read_csv(\"../train_news_final.csv\")\n",
    "# train_news_final.drop(columns=['Unnamed: 0'],inplace=True)\n",
    "# test_news_final = pd.read_csv(\"../test_news_final.csv\")\n",
    "# test_news_final.drop(columns=['Unnamed: 0'],inplace=True)\n",
    "# train_news_final = train_news_final.to_numpy()\n",
    "# test_news_final = test_news_final.to_numpy()\n",
    "# pd.DataFrame(train_news_final).to_csv(\"../train_news_final_split10.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# train_news_final = np.empty((train_news.shape[0], 0))\n",
    "# test_news_final = np.empty((test_news.shape[0], 0))\n",
    "train_news_final = train_news\n",
    "test_news_final = test_news\n",
    "\n",
    "# for name, model in level1:\n",
    "#     print(f\"Model: {name}\")\n",
    "#     print(\"----------------\")\n",
    "#     train_model = []\n",
    "#     for i,(train_index, test_index) in enumerate(kf.split(train_news, y_train)):\n",
    "#         print(f\"Fold {i}\")\n",
    "#         # Train model in 4/5\n",
    "#         model.fit(train_news[train_index], y_train[train_index])\n",
    "#         # predict model in 1/5\n",
    "#         y_pred = model.predict(train_news[test_index])\n",
    "#         train_model.extend(y_pred)\n",
    "#     # Predict on test set\n",
    "#     model.fit(train_news, y_train)\n",
    "#     test_model = model.predict(test_news)\n",
    "#     # Add column to news data\n",
    "#     train_news_final = np.column_stack((train_news_final, np.array(train_model).reshape(-1, 1)))\n",
    "#     test_news_final = np.column_stack((test_news_final, np.array(test_model).reshape(-1, 1)))\n",
    "#     print(\"-------Done-------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(train_news_final).to_csv(\"../train_news_final_split15.csv\")\n",
    "# pd.DataFrame(test_news_final).to_csv(\"../test_news_final_split15.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_news_final = pd.read_csv(\"../train_news_final_split15.csv\")\n",
    "# test_news_final = pd.read_csv(\"../test_news_final_split15.csv\")\n",
    "# train_news_final.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "# test_news_final.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "# train_news_final = train_news_final.values\n",
    "# test_news_final = test_news_final.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # KNeighborsClassifier\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# meta_model = KNeighborsClassifier()\n",
    "# params = {\n",
    "#     'n_neighbors': [4,5,6],\n",
    "#     'weights': ['uniform', 'distance'],\n",
    "#     'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "#     'p': [1,2,3]\n",
    "# }\n",
    "# GS = GridSearchCV(estimator=meta_model, param_grid=params, cv=10, verbose=5)\n",
    "# GS.fit(train_news_final, y_train)\n",
    "# GS.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # LogisticRegression\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# meta_model = LogisticRegression()\n",
    "# params = {\n",
    "#     'penalty': ['l1', 'l2'],\n",
    "#     'C': [ 0.01,0.1,1, 10],\n",
    "#     'solver': ['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'],\n",
    "#     'max_iter': [100,200]\n",
    "# }\n",
    "# GS = GridSearchCV(estimator=meta_model, param_grid=params, cv=10, verbose=5)\n",
    "# GS.fit(train_news_final, y_train)\n",
    "# GS.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # XGBClassifier\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# meta_model = XGBClassifier()\n",
    "# params = {\n",
    "#     'n_estimators': [10,50,100,1000],\n",
    "#     'learning_rate': [0.01,0.1,1,10],\n",
    "#     'max_depth': [3,4,5],\n",
    "#     'min_child_weight':[1],\n",
    "#     'gamma':[0,0.1,0.2],\n",
    "# }\n",
    "# GS = GridSearchCV(estimator=meta_model, param_grid=params, cv=10, verbose=5)\n",
    "# GS.fit(train_news_final, y_train)\n",
    "# GS.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # XGBClassifier\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# meta_model = SVC()\n",
    "# params = {\n",
    "#     'kernel': ['rbf', 'sigmoid', 'poly', 'linear'],\n",
    "#     'C': [0.01, 0.1, 1, 10, 100],\n",
    "#     'gamma': ['scale', 'auto'],\n",
    "#     'probability': [True]\n",
    "# }\n",
    "# GS = GridSearchCV(estimator=meta_model, param_grid=params, cv=10, verbose=5)\n",
    "# GS.fit(train_news_final, y_train)\n",
    "# GS.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 32 candidates, totalling 320 fits\n",
      "[CV 1/10] END algorithm=SAMME, learning_rate=0.01, n_estimators=10;, score=0.558 total time=   0.0s\n",
      "[CV 2/10] END algorithm=SAMME, learning_rate=0.01, n_estimators=10;, score=0.557 total time=   0.0s\n",
      "[CV 3/10] END algorithm=SAMME, learning_rate=0.01, n_estimators=10;, score=0.569 total time=   0.0s\n",
      "[CV 4/10] END algorithm=SAMME, learning_rate=0.01, n_estimators=10;, score=0.564 total time=   0.0s\n",
      "[CV 5/10] END algorithm=SAMME, learning_rate=0.01, n_estimators=10;, score=0.566 total time=   0.0s\n",
      "[CV 6/10] END algorithm=SAMME, learning_rate=0.01, n_estimators=10;, score=0.572 total time=   0.0s\n",
      "[CV 7/10] END algorithm=SAMME, learning_rate=0.01, n_estimators=10;, score=0.566 total time=   0.0s\n",
      "[CV 8/10] END algorithm=SAMME, learning_rate=0.01, n_estimators=10;, score=0.570 total time=   0.0s\n",
      "[CV 9/10] END algorithm=SAMME, learning_rate=0.01, n_estimators=10;, score=0.566 total time=   0.0s\n",
      "[CV 10/10] END algorithm=SAMME, learning_rate=0.01, n_estimators=10;, score=0.567 total time=   0.0s\n",
      "[CV 1/10] END algorithm=SAMME, learning_rate=0.01, n_estimators=50;, score=0.558 total time=   0.0s\n",
      "[CV 2/10] END algorithm=SAMME, learning_rate=0.01, n_estimators=50;, score=0.557 total time=   0.0s\n",
      "[CV 3/10] END algorithm=SAMME, learning_rate=0.01, n_estimators=50;, score=0.569 total time=   0.0s\n",
      "[CV 4/10] END algorithm=SAMME, learning_rate=0.01, n_estimators=50;, score=0.564 total time=   0.0s\n",
      "[CV 5/10] END algorithm=SAMME, learning_rate=0.01, n_estimators=50;, score=0.566 total time=   0.0s\n",
      "[CV 6/10] END algorithm=SAMME, learning_rate=0.01, n_estimators=50;, score=0.572 total time=   0.1s\n",
      "[CV 7/10] END algorithm=SAMME, learning_rate=0.01, n_estimators=50;, score=0.566 total time=   0.1s\n",
      "[CV 8/10] END algorithm=SAMME, learning_rate=0.01, n_estimators=50;, score=0.570 total time=   0.1s\n",
      "[CV 9/10] END algorithm=SAMME, learning_rate=0.01, n_estimators=50;, score=0.566 total time=   0.0s\n",
      "[CV 10/10] END algorithm=SAMME, learning_rate=0.01, n_estimators=50;, score=0.567 total time=   0.0s\n",
      "[CV 1/10] END algorithm=SAMME, learning_rate=0.01, n_estimators=100;, score=0.558 total time=   0.2s\n",
      "[CV 2/10] END algorithm=SAMME, learning_rate=0.01, n_estimators=100;, score=0.557 total time=   0.2s\n",
      "[CV 3/10] END algorithm=SAMME, learning_rate=0.01, n_estimators=100;, score=0.570 total time=   0.2s\n",
      "[CV 4/10] END algorithm=SAMME, learning_rate=0.01, n_estimators=100;, score=0.564 total time=   0.2s\n",
      "[CV 5/10] END algorithm=SAMME, learning_rate=0.01, n_estimators=100;, score=0.566 total time=   0.1s\n",
      "[CV 6/10] END algorithm=SAMME, learning_rate=0.01, n_estimators=100;, score=0.571 total time=   0.2s\n",
      "[CV 7/10] END algorithm=SAMME, learning_rate=0.01, n_estimators=100;, score=0.566 total time=   0.2s\n",
      "[CV 8/10] END algorithm=SAMME, learning_rate=0.01, n_estimators=100;, score=0.570 total time=   0.2s\n",
      "[CV 9/10] END algorithm=SAMME, learning_rate=0.01, n_estimators=100;, score=0.566 total time=   0.2s\n",
      "[CV 10/10] END algorithm=SAMME, learning_rate=0.01, n_estimators=100;, score=0.572 total time=   0.2s\n",
      "[CV 1/10] END algorithm=SAMME, learning_rate=0.01, n_estimators=1000;, score=0.925 total time=   2.3s\n",
      "[CV 2/10] END algorithm=SAMME, learning_rate=0.01, n_estimators=1000;, score=0.927 total time=   2.4s\n",
      "[CV 3/10] END algorithm=SAMME, learning_rate=0.01, n_estimators=1000;, score=0.935 total time=   2.2s\n",
      "[CV 4/10] END algorithm=SAMME, learning_rate=0.01, n_estimators=1000;, score=0.921 total time=   2.4s\n",
      "[CV 5/10] END algorithm=SAMME, learning_rate=0.01, n_estimators=1000;, score=0.919 total time=   2.2s\n",
      "[CV 6/10] END algorithm=SAMME, learning_rate=0.01, n_estimators=1000;, score=0.940 total time=   2.9s\n",
      "[CV 7/10] END algorithm=SAMME, learning_rate=0.01, n_estimators=1000;, score=0.934 total time=   2.7s\n",
      "[CV 8/10] END algorithm=SAMME, learning_rate=0.01, n_estimators=1000;, score=0.933 total time=   3.1s\n",
      "[CV 9/10] END algorithm=SAMME, learning_rate=0.01, n_estimators=1000;, score=0.932 total time=   2.5s\n",
      "[CV 10/10] END algorithm=SAMME, learning_rate=0.01, n_estimators=1000;, score=0.915 total time=   3.0s\n",
      "[CV 1/10] END algorithm=SAMME, learning_rate=0.1, n_estimators=10;, score=0.558 total time=   0.0s\n",
      "[CV 2/10] END algorithm=SAMME, learning_rate=0.1, n_estimators=10;, score=0.557 total time=   0.0s\n",
      "[CV 3/10] END algorithm=SAMME, learning_rate=0.1, n_estimators=10;, score=0.570 total time=   0.0s\n",
      "[CV 4/10] END algorithm=SAMME, learning_rate=0.1, n_estimators=10;, score=0.564 total time=   0.0s\n",
      "[CV 5/10] END algorithm=SAMME, learning_rate=0.1, n_estimators=10;, score=0.566 total time=   0.0s\n",
      "[CV 6/10] END algorithm=SAMME, learning_rate=0.1, n_estimators=10;, score=0.572 total time=   0.0s\n",
      "[CV 7/10] END algorithm=SAMME, learning_rate=0.1, n_estimators=10;, score=0.566 total time=   0.0s\n",
      "[CV 8/10] END algorithm=SAMME, learning_rate=0.1, n_estimators=10;, score=0.570 total time=   0.0s\n",
      "[CV 9/10] END algorithm=SAMME, learning_rate=0.1, n_estimators=10;, score=0.566 total time=   0.0s\n",
      "[CV 10/10] END algorithm=SAMME, learning_rate=0.1, n_estimators=10;, score=0.567 total time=   0.0s\n",
      "[CV 1/10] END algorithm=SAMME, learning_rate=0.1, n_estimators=50;, score=0.733 total time=   0.1s\n",
      "[CV 2/10] END algorithm=SAMME, learning_rate=0.1, n_estimators=50;, score=0.742 total time=   0.1s\n",
      "[CV 3/10] END algorithm=SAMME, learning_rate=0.1, n_estimators=50;, score=0.762 total time=   0.0s\n",
      "[CV 4/10] END algorithm=SAMME, learning_rate=0.1, n_estimators=50;, score=0.748 total time=   0.0s\n",
      "[CV 5/10] END algorithm=SAMME, learning_rate=0.1, n_estimators=50;, score=0.751 total time=   0.0s\n",
      "[CV 6/10] END algorithm=SAMME, learning_rate=0.1, n_estimators=50;, score=0.764 total time=   0.0s\n",
      "[CV 7/10] END algorithm=SAMME, learning_rate=0.1, n_estimators=50;, score=0.752 total time=   0.0s\n",
      "[CV 8/10] END algorithm=SAMME, learning_rate=0.1, n_estimators=50;, score=0.752 total time=   0.0s\n",
      "[CV 9/10] END algorithm=SAMME, learning_rate=0.1, n_estimators=50;, score=0.755 total time=   0.0s\n",
      "[CV 10/10] END algorithm=SAMME, learning_rate=0.1, n_estimators=50;, score=0.750 total time=   0.0s\n",
      "[CV 1/10] END algorithm=SAMME, learning_rate=0.1, n_estimators=100;, score=0.925 total time=   0.1s\n",
      "[CV 2/10] END algorithm=SAMME, learning_rate=0.1, n_estimators=100;, score=0.927 total time=   0.2s\n",
      "[CV 3/10] END algorithm=SAMME, learning_rate=0.1, n_estimators=100;, score=0.935 total time=   0.2s\n",
      "[CV 4/10] END algorithm=SAMME, learning_rate=0.1, n_estimators=100;, score=0.925 total time=   0.2s\n",
      "[CV 5/10] END algorithm=SAMME, learning_rate=0.1, n_estimators=100;, score=0.921 total time=   0.2s\n",
      "[CV 6/10] END algorithm=SAMME, learning_rate=0.1, n_estimators=100;, score=0.942 total time=   0.2s\n",
      "[CV 7/10] END algorithm=SAMME, learning_rate=0.1, n_estimators=100;, score=0.936 total time=   0.1s\n",
      "[CV 8/10] END algorithm=SAMME, learning_rate=0.1, n_estimators=100;, score=0.930 total time=   0.2s\n",
      "[CV 9/10] END algorithm=SAMME, learning_rate=0.1, n_estimators=100;, score=0.932 total time=   0.1s\n",
      "[CV 10/10] END algorithm=SAMME, learning_rate=0.1, n_estimators=100;, score=0.916 total time=   0.1s\n",
      "[CV 1/10] END algorithm=SAMME, learning_rate=0.1, n_estimators=1000;, score=0.936 total time=   2.3s\n",
      "[CV 2/10] END algorithm=SAMME, learning_rate=0.1, n_estimators=1000;, score=0.936 total time=   2.5s\n",
      "[CV 3/10] END algorithm=SAMME, learning_rate=0.1, n_estimators=1000;, score=0.933 total time=   2.4s\n",
      "[CV 4/10] END algorithm=SAMME, learning_rate=0.1, n_estimators=1000;, score=0.932 total time=   2.4s\n",
      "[CV 5/10] END algorithm=SAMME, learning_rate=0.1, n_estimators=1000;, score=0.934 total time=   2.3s\n",
      "[CV 6/10] END algorithm=SAMME, learning_rate=0.1, n_estimators=1000;, score=0.949 total time=   2.2s\n",
      "[CV 7/10] END algorithm=SAMME, learning_rate=0.1, n_estimators=1000;, score=0.943 total time=   2.5s\n",
      "[CV 8/10] END algorithm=SAMME, learning_rate=0.1, n_estimators=1000;, score=0.942 total time=   2.4s\n",
      "[CV 9/10] END algorithm=SAMME, learning_rate=0.1, n_estimators=1000;, score=0.941 total time=   2.3s\n",
      "[CV 10/10] END algorithm=SAMME, learning_rate=0.1, n_estimators=1000;, score=0.925 total time=   2.4s\n",
      "[CV 1/10] END algorithm=SAMME, learning_rate=1, n_estimators=10;, score=0.927 total time=   0.0s\n",
      "[CV 2/10] END algorithm=SAMME, learning_rate=1, n_estimators=10;, score=0.926 total time=   0.0s\n",
      "[CV 3/10] END algorithm=SAMME, learning_rate=1, n_estimators=10;, score=0.932 total time=   0.0s\n",
      "[CV 4/10] END algorithm=SAMME, learning_rate=1, n_estimators=10;, score=0.926 total time=   0.0s\n",
      "[CV 5/10] END algorithm=SAMME, learning_rate=1, n_estimators=10;, score=0.929 total time=   0.0s\n",
      "[CV 6/10] END algorithm=SAMME, learning_rate=1, n_estimators=10;, score=0.940 total time=   0.0s\n",
      "[CV 7/10] END algorithm=SAMME, learning_rate=1, n_estimators=10;, score=0.933 total time=   0.0s\n",
      "[CV 8/10] END algorithm=SAMME, learning_rate=1, n_estimators=10;, score=0.932 total time=   0.0s\n",
      "[CV 9/10] END algorithm=SAMME, learning_rate=1, n_estimators=10;, score=0.927 total time=   0.0s\n",
      "[CV 10/10] END algorithm=SAMME, learning_rate=1, n_estimators=10;, score=0.917 total time=   0.0s\n",
      "[CV 1/10] END algorithm=SAMME, learning_rate=1, n_estimators=50;, score=0.936 total time=   0.0s\n",
      "[CV 2/10] END algorithm=SAMME, learning_rate=1, n_estimators=50;, score=0.938 total time=   0.0s\n",
      "[CV 3/10] END algorithm=SAMME, learning_rate=1, n_estimators=50;, score=0.934 total time=   0.0s\n",
      "[CV 4/10] END algorithm=SAMME, learning_rate=1, n_estimators=50;, score=0.929 total time=   0.0s\n",
      "[CV 5/10] END algorithm=SAMME, learning_rate=1, n_estimators=50;, score=0.935 total time=   0.0s\n",
      "[CV 6/10] END algorithm=SAMME, learning_rate=1, n_estimators=50;, score=0.952 total time=   0.0s\n",
      "[CV 7/10] END algorithm=SAMME, learning_rate=1, n_estimators=50;, score=0.942 total time=   0.0s\n",
      "[CV 8/10] END algorithm=SAMME, learning_rate=1, n_estimators=50;, score=0.941 total time=   0.0s\n",
      "[CV 9/10] END algorithm=SAMME, learning_rate=1, n_estimators=50;, score=0.941 total time=   0.0s\n",
      "[CV 10/10] END algorithm=SAMME, learning_rate=1, n_estimators=50;, score=0.923 total time=   0.0s\n",
      "[CV 1/10] END algorithm=SAMME, learning_rate=1, n_estimators=100;, score=0.938 total time=   0.1s\n",
      "[CV 2/10] END algorithm=SAMME, learning_rate=1, n_estimators=100;, score=0.936 total time=   0.3s\n",
      "[CV 3/10] END algorithm=SAMME, learning_rate=1, n_estimators=100;, score=0.936 total time=   0.2s\n",
      "[CV 4/10] END algorithm=SAMME, learning_rate=1, n_estimators=100;, score=0.929 total time=   0.2s\n",
      "[CV 5/10] END algorithm=SAMME, learning_rate=1, n_estimators=100;, score=0.934 total time=   0.3s\n",
      "[CV 6/10] END algorithm=SAMME, learning_rate=1, n_estimators=100;, score=0.950 total time=   0.2s\n",
      "[CV 7/10] END algorithm=SAMME, learning_rate=1, n_estimators=100;, score=0.943 total time=   0.2s\n",
      "[CV 8/10] END algorithm=SAMME, learning_rate=1, n_estimators=100;, score=0.941 total time=   0.2s\n",
      "[CV 9/10] END algorithm=SAMME, learning_rate=1, n_estimators=100;, score=0.941 total time=   0.2s\n",
      "[CV 10/10] END algorithm=SAMME, learning_rate=1, n_estimators=100;, score=0.922 total time=   0.1s\n",
      "[CV 1/10] END algorithm=SAMME, learning_rate=1, n_estimators=1000;, score=0.938 total time=   2.3s\n",
      "[CV 2/10] END algorithm=SAMME, learning_rate=1, n_estimators=1000;, score=0.935 total time=   2.2s\n",
      "[CV 3/10] END algorithm=SAMME, learning_rate=1, n_estimators=1000;, score=0.935 total time=   2.9s\n",
      "[CV 4/10] END algorithm=SAMME, learning_rate=1, n_estimators=1000;, score=0.932 total time=   2.8s\n",
      "[CV 5/10] END algorithm=SAMME, learning_rate=1, n_estimators=1000;, score=0.933 total time=   2.5s\n",
      "[CV 6/10] END algorithm=SAMME, learning_rate=1, n_estimators=1000;, score=0.949 total time=   2.5s\n",
      "[CV 7/10] END algorithm=SAMME, learning_rate=1, n_estimators=1000;, score=0.942 total time=   2.5s\n",
      "[CV 8/10] END algorithm=SAMME, learning_rate=1, n_estimators=1000;, score=0.941 total time=   2.5s\n",
      "[CV 9/10] END algorithm=SAMME, learning_rate=1, n_estimators=1000;, score=0.942 total time=   2.4s\n",
      "[CV 10/10] END algorithm=SAMME, learning_rate=1, n_estimators=1000;, score=0.926 total time=   3.1s\n",
      "[CV 1/10] END algorithm=SAMME, learning_rate=10, n_estimators=10;, score=0.558 total time=   0.0s\n",
      "[CV 2/10] END algorithm=SAMME, learning_rate=10, n_estimators=10;, score=0.557 total time=   0.0s\n",
      "[CV 3/10] END algorithm=SAMME, learning_rate=10, n_estimators=10;, score=0.569 total time=   0.0s\n",
      "[CV 4/10] END algorithm=SAMME, learning_rate=10, n_estimators=10;, score=0.564 total time=   0.0s\n",
      "[CV 5/10] END algorithm=SAMME, learning_rate=10, n_estimators=10;, score=0.566 total time=   0.0s\n",
      "[CV 6/10] END algorithm=SAMME, learning_rate=10, n_estimators=10;, score=0.572 total time=   0.0s\n",
      "[CV 7/10] END algorithm=SAMME, learning_rate=10, n_estimators=10;, score=0.566 total time=   0.0s\n",
      "[CV 8/10] END algorithm=SAMME, learning_rate=10, n_estimators=10;, score=0.570 total time=   0.0s\n",
      "[CV 9/10] END algorithm=SAMME, learning_rate=10, n_estimators=10;, score=0.566 total time=   0.0s\n",
      "[CV 10/10] END algorithm=SAMME, learning_rate=10, n_estimators=10;, score=0.567 total time=   0.0s\n",
      "[CV 1/10] END algorithm=SAMME, learning_rate=10, n_estimators=50;, score=0.558 total time=   0.1s\n",
      "[CV 2/10] END algorithm=SAMME, learning_rate=10, n_estimators=50;, score=0.557 total time=   0.1s\n",
      "[CV 3/10] END algorithm=SAMME, learning_rate=10, n_estimators=50;, score=0.569 total time=   0.1s\n",
      "[CV 4/10] END algorithm=SAMME, learning_rate=10, n_estimators=50;, score=0.564 total time=   0.1s\n",
      "[CV 5/10] END algorithm=SAMME, learning_rate=10, n_estimators=50;, score=0.566 total time=   0.1s\n",
      "[CV 6/10] END algorithm=SAMME, learning_rate=10, n_estimators=50;, score=0.572 total time=   0.1s\n",
      "[CV 7/10] END algorithm=SAMME, learning_rate=10, n_estimators=50;, score=0.566 total time=   0.1s\n",
      "[CV 8/10] END algorithm=SAMME, learning_rate=10, n_estimators=50;, score=0.570 total time=   0.1s\n",
      "[CV 9/10] END algorithm=SAMME, learning_rate=10, n_estimators=50;, score=0.566 total time=   0.1s\n",
      "[CV 10/10] END algorithm=SAMME, learning_rate=10, n_estimators=50;, score=0.567 total time=   0.1s\n",
      "[CV 1/10] END algorithm=SAMME, learning_rate=10, n_estimators=100;, score=0.558 total time=   0.3s\n",
      "[CV 2/10] END algorithm=SAMME, learning_rate=10, n_estimators=100;, score=0.557 total time=   0.4s\n",
      "[CV 3/10] END algorithm=SAMME, learning_rate=10, n_estimators=100;, score=0.569 total time=   0.3s\n",
      "[CV 4/10] END algorithm=SAMME, learning_rate=10, n_estimators=100;, score=0.564 total time=   0.4s\n",
      "[CV 5/10] END algorithm=SAMME, learning_rate=10, n_estimators=100;, score=0.566 total time=   0.3s\n",
      "[CV 6/10] END algorithm=SAMME, learning_rate=10, n_estimators=100;, score=0.572 total time=   0.4s\n",
      "[CV 7/10] END algorithm=SAMME, learning_rate=10, n_estimators=100;, score=0.566 total time=   0.4s\n",
      "[CV 8/10] END algorithm=SAMME, learning_rate=10, n_estimators=100;, score=0.570 total time=   0.3s\n",
      "[CV 9/10] END algorithm=SAMME, learning_rate=10, n_estimators=100;, score=0.566 total time=   0.3s\n",
      "[CV 10/10] END algorithm=SAMME, learning_rate=10, n_estimators=100;, score=0.567 total time=   0.3s\n",
      "[CV 1/10] END algorithm=SAMME, learning_rate=10, n_estimators=1000;, score=0.558 total time=   4.1s\n",
      "[CV 2/10] END algorithm=SAMME, learning_rate=10, n_estimators=1000;, score=0.557 total time=   4.2s\n",
      "[CV 3/10] END algorithm=SAMME, learning_rate=10, n_estimators=1000;, score=0.569 total time=   4.3s\n",
      "[CV 4/10] END algorithm=SAMME, learning_rate=10, n_estimators=1000;, score=0.564 total time=   4.4s\n",
      "[CV 5/10] END algorithm=SAMME, learning_rate=10, n_estimators=1000;, score=0.566 total time=   4.3s\n",
      "[CV 6/10] END algorithm=SAMME, learning_rate=10, n_estimators=1000;, score=0.572 total time=   4.3s\n",
      "[CV 7/10] END algorithm=SAMME, learning_rate=10, n_estimators=1000;, score=0.566 total time=   4.3s\n",
      "[CV 8/10] END algorithm=SAMME, learning_rate=10, n_estimators=1000;, score=0.570 total time=   4.3s\n",
      "[CV 9/10] END algorithm=SAMME, learning_rate=10, n_estimators=1000;, score=0.566 total time=   4.5s\n",
      "[CV 10/10] END algorithm=SAMME, learning_rate=10, n_estimators=1000;, score=0.567 total time=   4.5s\n",
      "[CV 1/10] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=10;, score=0.562 total time=   0.0s\n",
      "[CV 2/10] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=10;, score=0.557 total time=   0.0s\n",
      "[CV 3/10] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=10;, score=0.569 total time=   0.0s\n",
      "[CV 4/10] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=10;, score=0.564 total time=   0.0s\n",
      "[CV 5/10] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=10;, score=0.566 total time=   0.0s\n",
      "[CV 6/10] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=10;, score=0.574 total time=   0.0s\n",
      "[CV 7/10] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=10;, score=0.566 total time=   0.0s\n",
      "[CV 8/10] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=10;, score=0.570 total time=   0.0s\n",
      "[CV 9/10] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=10;, score=0.566 total time=   0.0s\n",
      "[CV 10/10] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=10;, score=0.567 total time=   0.0s\n",
      "[CV 1/10] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=50;, score=0.561 total time=   0.2s\n",
      "[CV 2/10] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=50;, score=0.558 total time=   0.2s\n",
      "[CV 3/10] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=50;, score=0.576 total time=   0.2s\n",
      "[CV 4/10] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=50;, score=0.565 total time=   0.2s\n",
      "[CV 5/10] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=50;, score=0.567 total time=   0.2s\n",
      "[CV 6/10] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=50;, score=0.580 total time=   0.2s\n",
      "[CV 7/10] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=50;, score=0.572 total time=   0.2s\n",
      "[CV 8/10] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=50;, score=0.574 total time=   0.2s\n",
      "[CV 9/10] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=50;, score=0.571 total time=   0.2s\n",
      "[CV 10/10] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=50;, score=0.579 total time=   0.2s\n",
      "[CV 1/10] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=100;, score=0.929 total time=   0.4s\n",
      "[CV 2/10] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=100;, score=0.932 total time=   0.5s\n",
      "[CV 3/10] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=100;, score=0.934 total time=   0.4s\n",
      "[CV 4/10] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=100;, score=0.932 total time=   0.4s\n",
      "[CV 5/10] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=100;, score=0.932 total time=   0.5s\n",
      "[CV 6/10] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=100;, score=0.953 total time=   0.4s\n",
      "[CV 7/10] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=100;, score=0.942 total time=   0.5s\n",
      "[CV 8/10] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=100;, score=0.938 total time=   0.4s\n",
      "[CV 9/10] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=100;, score=0.941 total time=   0.5s\n",
      "[CV 10/10] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=100;, score=0.920 total time=   0.5s\n",
      "[CV 1/10] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=1000;, score=0.935 total time=   4.1s\n",
      "[CV 2/10] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=1000;, score=0.936 total time=   3.3s\n",
      "[CV 3/10] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=1000;, score=0.934 total time=   3.2s\n",
      "[CV 4/10] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=1000;, score=0.933 total time=   3.2s\n",
      "[CV 5/10] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=1000;, score=0.934 total time=   3.0s\n",
      "[CV 6/10] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=1000;, score=0.952 total time=   3.6s\n",
      "[CV 7/10] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=1000;, score=0.947 total time=   3.5s\n",
      "[CV 8/10] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=1000;, score=0.941 total time=   3.0s\n",
      "[CV 9/10] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=1000;, score=0.945 total time=   3.3s\n",
      "[CV 10/10] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=1000;, score=0.923 total time=   3.0s\n",
      "[CV 1/10] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=10;, score=0.933 total time=   0.0s\n",
      "[CV 2/10] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=10;, score=0.932 total time=   0.0s\n",
      "[CV 3/10] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=10;, score=0.933 total time=   0.0s\n",
      "[CV 4/10] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=10;, score=0.931 total time=   0.0s\n",
      "[CV 5/10] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=10;, score=0.930 total time=   0.0s\n",
      "[CV 6/10] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=10;, score=0.952 total time=   0.0s\n",
      "[CV 7/10] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=10;, score=0.943 total time=   0.0s\n",
      "[CV 8/10] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=10;, score=0.939 total time=   0.0s\n",
      "[CV 9/10] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=10;, score=0.941 total time=   0.0s\n",
      "[CV 10/10] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=10;, score=0.920 total time=   0.0s\n",
      "[CV 1/10] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=50;, score=0.935 total time=   0.1s\n",
      "[CV 2/10] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=50;, score=0.936 total time=   0.1s\n",
      "[CV 3/10] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=50;, score=0.933 total time=   0.1s\n",
      "[CV 4/10] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=50;, score=0.743 total time=   0.1s\n",
      "[CV 5/10] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=50;, score=0.934 total time=   0.1s\n",
      "[CV 6/10] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=50;, score=0.952 total time=   0.1s\n",
      "[CV 7/10] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=50;, score=0.946 total time=   0.1s\n",
      "[CV 8/10] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=50;, score=0.942 total time=   0.1s\n",
      "[CV 9/10] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=50;, score=0.943 total time=   0.1s\n",
      "[CV 10/10] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=50;, score=0.925 total time=   0.1s\n",
      "[CV 1/10] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=100;, score=0.934 total time=   0.2s\n",
      "[CV 2/10] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=100;, score=0.936 total time=   0.2s\n",
      "[CV 3/10] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=100;, score=0.934 total time=   0.2s\n",
      "[CV 4/10] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=100;, score=0.931 total time=   0.2s\n",
      "[CV 5/10] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=100;, score=0.934 total time=   0.2s\n",
      "[CV 6/10] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=100;, score=0.952 total time=   0.2s\n",
      "[CV 7/10] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=100;, score=0.947 total time=   0.3s\n",
      "[CV 8/10] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=100;, score=0.941 total time=   0.2s\n",
      "[CV 9/10] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=100;, score=0.945 total time=   0.2s\n",
      "[CV 10/10] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=100;, score=0.923 total time=   0.2s\n",
      "[CV 1/10] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=1000;, score=0.935 total time=   3.1s\n",
      "[CV 2/10] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=1000;, score=0.936 total time=   3.1s\n",
      "[CV 3/10] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=1000;, score=0.935 total time=   3.1s\n",
      "[CV 4/10] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=1000;, score=0.929 total time=   3.1s\n",
      "[CV 5/10] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=1000;, score=0.935 total time=   2.9s\n",
      "[CV 6/10] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=1000;, score=0.952 total time=   2.9s\n",
      "[CV 7/10] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=1000;, score=0.945 total time=   3.1s\n",
      "[CV 8/10] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=1000;, score=0.942 total time=   2.8s\n",
      "[CV 9/10] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=1000;, score=0.942 total time=   2.8s\n",
      "[CV 10/10] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=1000;, score=0.925 total time=   3.2s\n",
      "[CV 1/10] END algorithm=SAMME.R, learning_rate=1, n_estimators=10;, score=0.935 total time=   0.0s\n",
      "[CV 2/10] END algorithm=SAMME.R, learning_rate=1, n_estimators=10;, score=0.935 total time=   0.0s\n",
      "[CV 3/10] END algorithm=SAMME.R, learning_rate=1, n_estimators=10;, score=0.935 total time=   0.0s\n",
      "[CV 4/10] END algorithm=SAMME.R, learning_rate=1, n_estimators=10;, score=0.932 total time=   0.0s\n",
      "[CV 5/10] END algorithm=SAMME.R, learning_rate=1, n_estimators=10;, score=0.934 total time=   0.0s\n",
      "[CV 6/10] END algorithm=SAMME.R, learning_rate=1, n_estimators=10;, score=0.954 total time=   0.0s\n",
      "[CV 7/10] END algorithm=SAMME.R, learning_rate=1, n_estimators=10;, score=0.942 total time=   0.0s\n",
      "[CV 8/10] END algorithm=SAMME.R, learning_rate=1, n_estimators=10;, score=0.941 total time=   0.0s\n",
      "[CV 9/10] END algorithm=SAMME.R, learning_rate=1, n_estimators=10;, score=0.940 total time=   0.0s\n",
      "[CV 10/10] END algorithm=SAMME.R, learning_rate=1, n_estimators=10;, score=0.922 total time=   0.0s\n",
      "[CV 1/10] END algorithm=SAMME.R, learning_rate=1, n_estimators=50;, score=0.933 total time=   0.1s\n",
      "[CV 2/10] END algorithm=SAMME.R, learning_rate=1, n_estimators=50;, score=0.938 total time=   0.1s\n",
      "[CV 3/10] END algorithm=SAMME.R, learning_rate=1, n_estimators=50;, score=0.934 total time=   0.1s\n",
      "[CV 4/10] END algorithm=SAMME.R, learning_rate=1, n_estimators=50;, score=0.932 total time=   0.1s\n",
      "[CV 5/10] END algorithm=SAMME.R, learning_rate=1, n_estimators=50;, score=0.934 total time=   0.1s\n",
      "[CV 6/10] END algorithm=SAMME.R, learning_rate=1, n_estimators=50;, score=0.952 total time=   0.0s\n",
      "[CV 7/10] END algorithm=SAMME.R, learning_rate=1, n_estimators=50;, score=0.943 total time=   0.1s\n",
      "[CV 8/10] END algorithm=SAMME.R, learning_rate=1, n_estimators=50;, score=0.942 total time=   0.1s\n",
      "[CV 9/10] END algorithm=SAMME.R, learning_rate=1, n_estimators=50;, score=0.941 total time=   0.1s\n",
      "[CV 10/10] END algorithm=SAMME.R, learning_rate=1, n_estimators=50;, score=0.922 total time=   0.1s\n",
      "[CV 1/10] END algorithm=SAMME.R, learning_rate=1, n_estimators=100;, score=0.933 total time=   0.3s\n",
      "[CV 2/10] END algorithm=SAMME.R, learning_rate=1, n_estimators=100;, score=0.938 total time=   0.3s\n",
      "[CV 3/10] END algorithm=SAMME.R, learning_rate=1, n_estimators=100;, score=0.933 total time=   0.4s\n",
      "[CV 4/10] END algorithm=SAMME.R, learning_rate=1, n_estimators=100;, score=0.929 total time=   0.4s\n",
      "[CV 5/10] END algorithm=SAMME.R, learning_rate=1, n_estimators=100;, score=0.933 total time=   0.5s\n",
      "[CV 6/10] END algorithm=SAMME.R, learning_rate=1, n_estimators=100;, score=0.949 total time=   0.5s\n",
      "[CV 7/10] END algorithm=SAMME.R, learning_rate=1, n_estimators=100;, score=0.942 total time=   0.5s\n",
      "[CV 8/10] END algorithm=SAMME.R, learning_rate=1, n_estimators=100;, score=0.941 total time=   0.5s\n",
      "[CV 9/10] END algorithm=SAMME.R, learning_rate=1, n_estimators=100;, score=0.941 total time=   0.5s\n",
      "[CV 10/10] END algorithm=SAMME.R, learning_rate=1, n_estimators=100;, score=0.923 total time=   0.5s\n",
      "[CV 1/10] END algorithm=SAMME.R, learning_rate=1, n_estimators=1000;, score=0.935 total time=   5.5s\n",
      "[CV 2/10] END algorithm=SAMME.R, learning_rate=1, n_estimators=1000;, score=0.935 total time=   5.5s\n",
      "[CV 3/10] END algorithm=SAMME.R, learning_rate=1, n_estimators=1000;, score=0.934 total time=   5.4s\n",
      "[CV 4/10] END algorithm=SAMME.R, learning_rate=1, n_estimators=1000;, score=0.927 total time=   5.1s\n",
      "[CV 5/10] END algorithm=SAMME.R, learning_rate=1, n_estimators=1000;, score=0.932 total time=   5.5s\n",
      "[CV 6/10] END algorithm=SAMME.R, learning_rate=1, n_estimators=1000;, score=0.945 total time=   3.5s\n",
      "[CV 7/10] END algorithm=SAMME.R, learning_rate=1, n_estimators=1000;, score=0.942 total time=   2.9s\n",
      "[CV 8/10] END algorithm=SAMME.R, learning_rate=1, n_estimators=1000;, score=0.941 total time=   2.9s\n",
      "[CV 9/10] END algorithm=SAMME.R, learning_rate=1, n_estimators=1000;, score=0.940 total time=   3.0s\n",
      "[CV 10/10] END algorithm=SAMME.R, learning_rate=1, n_estimators=1000;, score=0.921 total time=   3.1s\n",
      "[CV 1/10] END algorithm=SAMME.R, learning_rate=10, n_estimators=10;, score=0.558 total time=   0.0s\n",
      "[CV 2/10] END algorithm=SAMME.R, learning_rate=10, n_estimators=10;, score=0.551 total time=   0.0s\n",
      "[CV 3/10] END algorithm=SAMME.R, learning_rate=10, n_estimators=10;, score=0.544 total time=   0.0s\n",
      "[CV 4/10] END algorithm=SAMME.R, learning_rate=10, n_estimators=10;, score=0.557 total time=   0.0s\n",
      "[CV 5/10] END algorithm=SAMME.R, learning_rate=10, n_estimators=10;, score=0.537 total time=   0.0s\n",
      "[CV 6/10] END algorithm=SAMME.R, learning_rate=10, n_estimators=10;, score=0.548 total time=   0.0s\n",
      "[CV 7/10] END algorithm=SAMME.R, learning_rate=10, n_estimators=10;, score=0.554 total time=   0.0s\n",
      "[CV 8/10] END algorithm=SAMME.R, learning_rate=10, n_estimators=10;, score=0.550 total time=   0.0s\n",
      "[CV 9/10] END algorithm=SAMME.R, learning_rate=10, n_estimators=10;, score=0.546 total time=   0.0s\n",
      "[CV 10/10] END algorithm=SAMME.R, learning_rate=10, n_estimators=10;, score=0.534 total time=   0.0s\n",
      "[CV 1/10] END algorithm=SAMME.R, learning_rate=10, n_estimators=50;, score=0.558 total time=   0.0s\n",
      "[CV 2/10] END algorithm=SAMME.R, learning_rate=10, n_estimators=50;, score=0.551 total time=   0.0s\n",
      "[CV 3/10] END algorithm=SAMME.R, learning_rate=10, n_estimators=50;, score=0.544 total time=   0.1s\n",
      "[CV 4/10] END algorithm=SAMME.R, learning_rate=10, n_estimators=50;, score=0.557 total time=   0.0s\n",
      "[CV 5/10] END algorithm=SAMME.R, learning_rate=10, n_estimators=50;, score=0.537 total time=   0.1s\n",
      "[CV 6/10] END algorithm=SAMME.R, learning_rate=10, n_estimators=50;, score=0.548 total time=   0.0s\n",
      "[CV 7/10] END algorithm=SAMME.R, learning_rate=10, n_estimators=50;, score=0.554 total time=   0.1s\n",
      "[CV 8/10] END algorithm=SAMME.R, learning_rate=10, n_estimators=50;, score=0.550 total time=   0.0s\n",
      "[CV 9/10] END algorithm=SAMME.R, learning_rate=10, n_estimators=50;, score=0.546 total time=   0.0s\n",
      "[CV 10/10] END algorithm=SAMME.R, learning_rate=10, n_estimators=50;, score=0.534 total time=   0.1s\n",
      "[CV 1/10] END algorithm=SAMME.R, learning_rate=10, n_estimators=100;, score=0.558 total time=   0.2s\n",
      "[CV 2/10] END algorithm=SAMME.R, learning_rate=10, n_estimators=100;, score=0.551 total time=   0.2s\n",
      "[CV 3/10] END algorithm=SAMME.R, learning_rate=10, n_estimators=100;, score=0.544 total time=   0.2s\n",
      "[CV 4/10] END algorithm=SAMME.R, learning_rate=10, n_estimators=100;, score=0.557 total time=   0.2s\n",
      "[CV 5/10] END algorithm=SAMME.R, learning_rate=10, n_estimators=100;, score=0.537 total time=   0.2s\n",
      "[CV 6/10] END algorithm=SAMME.R, learning_rate=10, n_estimators=100;, score=0.548 total time=   0.2s\n",
      "[CV 7/10] END algorithm=SAMME.R, learning_rate=10, n_estimators=100;, score=0.554 total time=   0.2s\n",
      "[CV 8/10] END algorithm=SAMME.R, learning_rate=10, n_estimators=100;, score=0.550 total time=   0.2s\n",
      "[CV 9/10] END algorithm=SAMME.R, learning_rate=10, n_estimators=100;, score=0.546 total time=   0.2s\n",
      "[CV 10/10] END algorithm=SAMME.R, learning_rate=10, n_estimators=100;, score=0.534 total time=   0.2s\n",
      "[CV 1/10] END algorithm=SAMME.R, learning_rate=10, n_estimators=1000;, score=0.558 total time=   2.8s\n",
      "[CV 2/10] END algorithm=SAMME.R, learning_rate=10, n_estimators=1000;, score=0.551 total time=   2.8s\n",
      "[CV 3/10] END algorithm=SAMME.R, learning_rate=10, n_estimators=1000;, score=0.544 total time=   2.7s\n",
      "[CV 4/10] END algorithm=SAMME.R, learning_rate=10, n_estimators=1000;, score=0.557 total time=   2.6s\n",
      "[CV 5/10] END algorithm=SAMME.R, learning_rate=10, n_estimators=1000;, score=0.537 total time=   2.6s\n",
      "[CV 6/10] END algorithm=SAMME.R, learning_rate=10, n_estimators=1000;, score=0.548 total time=   2.7s\n",
      "[CV 7/10] END algorithm=SAMME.R, learning_rate=10, n_estimators=1000;, score=0.554 total time=   2.9s\n",
      "[CV 8/10] END algorithm=SAMME.R, learning_rate=10, n_estimators=1000;, score=0.550 total time=   3.1s\n",
      "[CV 9/10] END algorithm=SAMME.R, learning_rate=10, n_estimators=1000;, score=0.546 total time=   2.9s\n",
      "[CV 10/10] END algorithm=SAMME.R, learning_rate=10, n_estimators=1000;, score=0.534 total time=   3.0s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>AdaBoostClassifier(learning_rate=0.01, n_estimators=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;AdaBoostClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.ensemble.AdaBoostClassifier.html\">?<span>Documentation for AdaBoostClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>AdaBoostClassifier(learning_rate=0.01, n_estimators=1000)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "AdaBoostClassifier(learning_rate=0.01, n_estimators=1000)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RandomForest\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "meta_model = AdaBoostClassifier()\n",
    "params = {\n",
    "    'n_estimators': [10,50,100,1000],\n",
    "    'learning_rate': [0.01,0.1,1,10],\n",
    "    'algorithm': ['SAMME', 'SAMME.R'],\n",
    "}\n",
    "GS = GridSearchCV(estimator=meta_model, param_grid=params, cv=10, verbose=5)\n",
    "GS.fit(train_news_final, y_train)\n",
    "GS.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = GS.best_estimator_\n",
    "y_pred = best_model.predict(test_news_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay,multilabel_confusion_matrix,f1_score,precision_score,accuracy_score,recall_score,precision_recall_fscore_support\n",
    "def evaluation_test(y,y_pred):\n",
    "    cm = confusion_matrix(y,y_pred)\n",
    "    disp = ConfusionMatrixDisplay(cm,display_labels=['AFIB','SB','SR','GSVT'])\n",
    "    disp.plot()\n",
    "    plt.show()\n",
    "    n_classes = len(cm)\n",
    "    result = []\n",
    "    for c in range(n_classes):\n",
    "        tp = cm[c,c]\n",
    "        fp = sum(cm[:,c]) - cm[c,c]\n",
    "        fn = sum(cm[c,:]) - cm[c,c]\n",
    "        tn = sum(np.delete(sum(cm)-cm[c,:],c))\n",
    "        acc = (tp+tn) / (tp+fn+tn+fp)\n",
    "        recall = tp/(tp+fn)\n",
    "        precision = tp/(tp+fp)\n",
    "        specificity = tn/(tn+fp)\n",
    "        f1_score = 2*((precision*recall)/(precision+recall))\n",
    "        if c+1 == 1:\n",
    "            Rhythm = 'AFIB'\n",
    "        elif c+1 == 2:\n",
    "            Rhythm = 'SB'\n",
    "        elif c+1 == 3:\n",
    "            Rhythm = 'SR'\n",
    "        else:\n",
    "            Rhythm = 'GSVT'\n",
    "        result.append([Rhythm,acc,recall,precision,f1_score,specificity])\n",
    "    p_macro,r_macro,f_macro,support_macro = precision_recall_fscore_support(y,y_pred,average='macro')\n",
    "    p_micro,r_micro,f_micro,support_micro = precision_recall_fscore_support(y,y_pred,average='micro')\n",
    "    p_weighted,r_weighted,f_weighted,support_weighted = precision_recall_fscore_support(y,y_pred,average='weighted')\n",
    "    result.append(['macro avg',None,f_macro,p_macro,r_macro,None])\n",
    "    result.append(['micro avg',None,f_micro,p_micro,r_micro,None])\n",
    "    result.append(['weighted avg',None,f_weighted,p_weighted,r_weighted,None])\n",
    "    return result\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rhythm Group</th>\n",
       "      <th>ACC</th>\n",
       "      <th>F1-score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>specificity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AFIB</td>\n",
       "      <td>0.948633</td>\n",
       "      <td>0.885393</td>\n",
       "      <td>0.871681</td>\n",
       "      <td>0.878484</td>\n",
       "      <td>0.965414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SB</td>\n",
       "      <td>0.988690</td>\n",
       "      <td>0.984576</td>\n",
       "      <td>0.984576</td>\n",
       "      <td>0.984576</td>\n",
       "      <td>0.991071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SR</td>\n",
       "      <td>0.975495</td>\n",
       "      <td>0.952809</td>\n",
       "      <td>0.931868</td>\n",
       "      <td>0.942222</td>\n",
       "      <td>0.981515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GSVT</td>\n",
       "      <td>0.964656</td>\n",
       "      <td>0.898678</td>\n",
       "      <td>0.933638</td>\n",
       "      <td>0.915825</td>\n",
       "      <td>0.982614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>macro avg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.930277</td>\n",
       "      <td>0.930441</td>\n",
       "      <td>0.930364</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>micro avg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.938737</td>\n",
       "      <td>0.938737</td>\n",
       "      <td>0.938737</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>weighted avg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.938736</td>\n",
       "      <td>0.938950</td>\n",
       "      <td>0.938737</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rhythm Group       ACC  F1-score  Precision    Recall  specificity\n",
       "0          AFIB  0.948633  0.885393   0.871681  0.878484     0.965414\n",
       "1            SB  0.988690  0.984576   0.984576  0.984576     0.991071\n",
       "2            SR  0.975495  0.952809   0.931868  0.942222     0.981515\n",
       "3          GSVT  0.964656  0.898678   0.933638  0.915825     0.982614\n",
       "4     macro avg       NaN  0.930277   0.930441  0.930364          NaN\n",
       "5     micro avg       NaN  0.938737   0.938737  0.938737          NaN\n",
       "6  weighted avg       NaN  0.938736   0.938950  0.938737          NaN"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_test = evaluation_test(y_test,y_pred)\n",
    "df_evaluation_test = pd.DataFrame(data=evaluation_test,columns=[\"Rhythm Group\",\"ACC\",\"F1-score\",\"Precision\",\"Recall\",\"specificity\"])\n",
    "df_evaluation_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_evaluation_test.to_csv(\"../Result/Stacking_multipleClass_frequency_split15_cv10_AB.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testdatasets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
