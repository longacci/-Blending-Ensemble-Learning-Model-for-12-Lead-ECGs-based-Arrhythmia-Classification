{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>204</th>\n",
       "      <th>205</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>950.000000</td>\n",
       "      <td>1074.0</td>\n",
       "      <td>274.986868</td>\n",
       "      <td>782.0</td>\n",
       "      <td>-0.319753</td>\n",
       "      <td>-1.432466</td>\n",
       "      <td>325.821586</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>172.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>252.222222</td>\n",
       "      <td>10656.395062</td>\n",
       "      <td>87.777778</td>\n",
       "      <td>10339.061728</td>\n",
       "      <td>135.800000</td>\n",
       "      <td>4315.560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>574.500000</td>\n",
       "      <td>582.0</td>\n",
       "      <td>104.913059</td>\n",
       "      <td>378.0</td>\n",
       "      <td>0.158313</td>\n",
       "      <td>-0.696295</td>\n",
       "      <td>336.569414</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>-15.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>158.000000</td>\n",
       "      <td>3944.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>6555.000000</td>\n",
       "      <td>-1.066667</td>\n",
       "      <td>697.528889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>593.600000</td>\n",
       "      <td>594.0</td>\n",
       "      <td>4.687572</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.396421</td>\n",
       "      <td>-0.312612</td>\n",
       "      <td>94.909877</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>122.400000</td>\n",
       "      <td>2058.773333</td>\n",
       "      <td>12.533333</td>\n",
       "      <td>1360.782222</td>\n",
       "      <td>95.500000</td>\n",
       "      <td>68.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>420.090909</td>\n",
       "      <td>420.0</td>\n",
       "      <td>3.591772</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-0.021014</td>\n",
       "      <td>-0.856142</td>\n",
       "      <td>254.059787</td>\n",
       "      <td>0.826087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.739130</td>\n",
       "      <td>-9.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>40.666667</td>\n",
       "      <td>1120.888889</td>\n",
       "      <td>5.333333</td>\n",
       "      <td>1504.888889</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>1464.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1068.750000</td>\n",
       "      <td>1075.0</td>\n",
       "      <td>25.118469</td>\n",
       "      <td>76.0</td>\n",
       "      <td>-0.276816</td>\n",
       "      <td>-1.271399</td>\n",
       "      <td>461.130814</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>671.000000</td>\n",
       "      <td>19.750000</td>\n",
       "      <td>569.437500</td>\n",
       "      <td>136.444444</td>\n",
       "      <td>43.358025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8511</th>\n",
       "      <td>3.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>615.733333</td>\n",
       "      <td>596.0</td>\n",
       "      <td>51.114860</td>\n",
       "      <td>152.0</td>\n",
       "      <td>2.153820</td>\n",
       "      <td>2.645687</td>\n",
       "      <td>365.256750</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003757</td>\n",
       "      <td>0.022262</td>\n",
       "      <td>0.003757</td>\n",
       "      <td>0.003757</td>\n",
       "      <td>0.044242</td>\n",
       "      <td>0.044242</td>\n",
       "      <td>0.043021</td>\n",
       "      <td>0.043021</td>\n",
       "      <td>0.037385</td>\n",
       "      <td>0.037385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8512</th>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1091.500000</td>\n",
       "      <td>1093.0</td>\n",
       "      <td>5.894913</td>\n",
       "      <td>18.0</td>\n",
       "      <td>-0.311206</td>\n",
       "      <td>-1.184514</td>\n",
       "      <td>358.414529</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>81.428571</td>\n",
       "      <td>1294.530612</td>\n",
       "      <td>-40.000000</td>\n",
       "      <td>1746.285714</td>\n",
       "      <td>155.333333</td>\n",
       "      <td>4722.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8513</th>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>654.428571</td>\n",
       "      <td>648.0</td>\n",
       "      <td>107.653355</td>\n",
       "      <td>458.0</td>\n",
       "      <td>0.475616</td>\n",
       "      <td>0.784000</td>\n",
       "      <td>180.045117</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>77.142857</td>\n",
       "      <td>2213.551020</td>\n",
       "      <td>-1.714286</td>\n",
       "      <td>2686.204082</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>3602.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8514</th>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1075.000000</td>\n",
       "      <td>1083.0</td>\n",
       "      <td>24.535688</td>\n",
       "      <td>66.0</td>\n",
       "      <td>-0.263431</td>\n",
       "      <td>-1.567800</td>\n",
       "      <td>251.455499</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>101.142857</td>\n",
       "      <td>4933.551020</td>\n",
       "      <td>-10.750000</td>\n",
       "      <td>7259.937500</td>\n",
       "      <td>88.222222</td>\n",
       "      <td>202.172840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8515</th>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1041.250000</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>8.242421</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.214800</td>\n",
       "      <td>-1.575835</td>\n",
       "      <td>505.203302</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>350.000000</td>\n",
       "      <td>-20.000000</td>\n",
       "      <td>588.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8516 rows × 213 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        1     2            3       4           5      6         7         8  \\\n",
       "0     0.0  10.0   950.000000  1074.0  274.986868  782.0 -0.319753 -1.432466   \n",
       "1     0.0  17.0   574.500000   582.0  104.913059  378.0  0.158313 -0.696295   \n",
       "2     3.0  16.0   593.600000   594.0    4.687572   18.0  0.396421 -0.312612   \n",
       "3     3.0  23.0   420.090909   420.0    3.591772   12.0 -0.021014 -0.856142   \n",
       "4     1.0   9.0  1068.750000  1075.0   25.118469   76.0 -0.276816 -1.271399   \n",
       "...   ...   ...          ...     ...         ...    ...       ...       ...   \n",
       "8511  3.0  16.0   615.733333   596.0   51.114860  152.0  2.153820  2.645687   \n",
       "8512  1.0   9.0  1091.500000  1093.0    5.894913   18.0 -0.311206 -1.184514   \n",
       "8513  2.0  15.0   654.428571   648.0  107.653355  458.0  0.475616  0.784000   \n",
       "8514  1.0   9.0  1075.000000  1083.0   24.535688   66.0 -0.263431 -1.567800   \n",
       "8515  1.0   9.0  1041.250000  1040.0    8.242421   22.0  0.214800 -1.575835   \n",
       "\n",
       "               9        10  ...       204         205        206        207  \\\n",
       "0     325.821586  1.000000  ...  1.000000  172.000000  10.000000   9.000000   \n",
       "1     336.569414  1.000000  ...  0.882353  -15.000000  15.000000   4.000000   \n",
       "2      94.909877  1.000000  ...  1.000000   -4.000000  16.000000  15.000000   \n",
       "3     254.059787  0.826087  ...  0.739130   -9.000000   6.000000   4.000000   \n",
       "4     461.130814  1.000000  ...  1.000000    2.000000   9.000000   8.000000   \n",
       "...          ...       ...  ...       ...         ...        ...        ...   \n",
       "8511  365.256750  1.000000  ...  0.003757    0.022262   0.003757   0.003757   \n",
       "8512  358.414529  1.000000  ...  0.888889   -3.000000   9.000000   8.000000   \n",
       "8513  180.045117  1.000000  ...  1.000000   -4.000000  15.000000  14.000000   \n",
       "8514  251.455499  1.000000  ...  1.000000   14.000000   9.000000   8.000000   \n",
       "8515  505.203302  1.000000  ...  1.000000    0.000000   9.000000   8.000000   \n",
       "\n",
       "             208           209        210           211         212  \\\n",
       "0     252.222222  10656.395062  87.777778  10339.061728  135.800000   \n",
       "1     158.000000   3944.000000  73.000000   6555.000000   -1.066667   \n",
       "2     122.400000   2058.773333  12.533333   1360.782222   95.500000   \n",
       "3      40.666667   1120.888889   5.333333   1504.888889   12.000000   \n",
       "4     122.000000    671.000000  19.750000    569.437500  136.444444   \n",
       "...          ...           ...        ...           ...         ...   \n",
       "8511    0.044242      0.044242   0.043021      0.043021    0.037385   \n",
       "8512   81.428571   1294.530612 -40.000000   1746.285714  155.333333   \n",
       "8513   77.142857   2213.551020  -1.714286   2686.204082  104.000000   \n",
       "8514  101.142857   4933.551020 -10.750000   7259.937500   88.222222   \n",
       "8515  102.000000    350.000000 -20.000000    588.000000  150.000000   \n",
       "\n",
       "              213  \n",
       "0     4315.560000  \n",
       "1      697.528889  \n",
       "2       68.750000  \n",
       "3     1464.000000  \n",
       "4       43.358025  \n",
       "...           ...  \n",
       "8511     0.037385  \n",
       "8512  4722.666667  \n",
       "8513  3602.666667  \n",
       "8514   202.172840  \n",
       "8515     0.000000  \n",
       "\n",
       "[8516 rows x 213 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"../data_train_frequency.csv\")\n",
    "df_train.drop(columns=[\"Unnamed: 0\"],inplace=True)\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>203</th>\n",
       "      <th>204</th>\n",
       "      <th>205</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>710.769231</td>\n",
       "      <td>628.0</td>\n",
       "      <td>153.204817</td>\n",
       "      <td>556.0</td>\n",
       "      <td>0.996355</td>\n",
       "      <td>0.207174</td>\n",
       "      <td>459.037295</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>-10.000000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>729.000000</td>\n",
       "      <td>78.250000</td>\n",
       "      <td>3140.437500</td>\n",
       "      <td>127.600000</td>\n",
       "      <td>1041.440000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>968.666667</td>\n",
       "      <td>894.0</td>\n",
       "      <td>266.399867</td>\n",
       "      <td>932.0</td>\n",
       "      <td>0.979352</td>\n",
       "      <td>0.388359</td>\n",
       "      <td>398.464564</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>140.500000</td>\n",
       "      <td>15314.750000</td>\n",
       "      <td>-27.000000</td>\n",
       "      <td>5249.000000</td>\n",
       "      <td>112.285714</td>\n",
       "      <td>8081.632653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>797.000000</td>\n",
       "      <td>780.0</td>\n",
       "      <td>251.329664</td>\n",
       "      <td>794.0</td>\n",
       "      <td>0.260470</td>\n",
       "      <td>-1.002325</td>\n",
       "      <td>340.802438</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>154.285714</td>\n",
       "      <td>1944.489796</td>\n",
       "      <td>18.571429</td>\n",
       "      <td>8070.530612</td>\n",
       "      <td>131.111111</td>\n",
       "      <td>1078.320988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>757.500000</td>\n",
       "      <td>755.0</td>\n",
       "      <td>8.986100</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.048579</td>\n",
       "      <td>-1.449012</td>\n",
       "      <td>412.324324</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>108.500000</td>\n",
       "      <td>6122.750000</td>\n",
       "      <td>46.500000</td>\n",
       "      <td>7081.416667</td>\n",
       "      <td>121.833333</td>\n",
       "      <td>264.305556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>413.909091</td>\n",
       "      <td>409.0</td>\n",
       "      <td>82.344017</td>\n",
       "      <td>426.0</td>\n",
       "      <td>3.023659</td>\n",
       "      <td>10.404884</td>\n",
       "      <td>168.041577</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.022262</td>\n",
       "      <td>11.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.044242</td>\n",
       "      <td>0.044242</td>\n",
       "      <td>-50.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>45.818182</td>\n",
       "      <td>832.330579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2125</th>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1071.250000</td>\n",
       "      <td>1062.0</td>\n",
       "      <td>36.509417</td>\n",
       "      <td>118.0</td>\n",
       "      <td>1.263183</td>\n",
       "      <td>0.543003</td>\n",
       "      <td>364.303573</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>342.857143</td>\n",
       "      <td>2843.265306</td>\n",
       "      <td>205.142857</td>\n",
       "      <td>11207.836735</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>2281.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2126</th>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1196.000000</td>\n",
       "      <td>1202.0</td>\n",
       "      <td>33.839959</td>\n",
       "      <td>102.0</td>\n",
       "      <td>-0.454057</td>\n",
       "      <td>-1.036905</td>\n",
       "      <td>181.876516</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-26.000000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>137.666667</td>\n",
       "      <td>228.555556</td>\n",
       "      <td>87.714286</td>\n",
       "      <td>14282.775510</td>\n",
       "      <td>169.142857</td>\n",
       "      <td>46.693878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2127</th>\n",
       "      <td>3.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>595.600000</td>\n",
       "      <td>590.0</td>\n",
       "      <td>23.734082</td>\n",
       "      <td>82.0</td>\n",
       "      <td>0.371174</td>\n",
       "      <td>-0.657132</td>\n",
       "      <td>137.696567</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-8.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>102.714286</td>\n",
       "      <td>1270.061224</td>\n",
       "      <td>7.285714</td>\n",
       "      <td>361.489796</td>\n",
       "      <td>90.400000</td>\n",
       "      <td>2186.240000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2128</th>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1080.285714</td>\n",
       "      <td>996.0</td>\n",
       "      <td>180.470587</td>\n",
       "      <td>448.0</td>\n",
       "      <td>0.587475</td>\n",
       "      <td>-1.363827</td>\n",
       "      <td>561.988537</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>62.400000</td>\n",
       "      <td>51.840000</td>\n",
       "      <td>-45.200000</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>5002.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2129</th>\n",
       "      <td>3.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>391.250000</td>\n",
       "      <td>390.0</td>\n",
       "      <td>2.569857</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.605786</td>\n",
       "      <td>-0.869886</td>\n",
       "      <td>654.123072</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.044242</td>\n",
       "      <td>0.044242</td>\n",
       "      <td>0.043021</td>\n",
       "      <td>0.043021</td>\n",
       "      <td>0.037385</td>\n",
       "      <td>0.037385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2130 rows × 213 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0     1            2       3           4      5         6          7  \\\n",
       "0     0.0  14.0   710.769231   628.0  153.204817  556.0  0.996355   0.207174   \n",
       "1     0.0  10.0   968.666667   894.0  266.399867  932.0  0.979352   0.388359   \n",
       "2     0.0  11.0   797.000000   780.0  251.329664  794.0  0.260470  -1.002325   \n",
       "3     2.0  13.0   757.500000   755.0    8.986100   26.0  0.048579  -1.449012   \n",
       "4     0.0  23.0   413.909091   409.0   82.344017  426.0  3.023659  10.404884   \n",
       "...   ...   ...          ...     ...         ...    ...       ...        ...   \n",
       "2125  1.0   9.0  1071.250000  1062.0   36.509417  118.0  1.263183   0.543003   \n",
       "2126  1.0   8.0  1196.000000  1202.0   33.839959  102.0 -0.454057  -1.036905   \n",
       "2127  3.0  16.0   595.600000   590.0   23.734082   82.0  0.371174  -0.657132   \n",
       "2128  1.0   8.0  1080.285714   996.0  180.470587  448.0  0.587475  -1.363827   \n",
       "2129  3.0  25.0   391.250000   390.0    2.569857    8.0  0.605786  -0.869886   \n",
       "\n",
       "               8         9  ...       203        204   205   206         207  \\\n",
       "0     459.037295  1.000000  ...  0.928571 -10.000000  10.0   9.0  146.000000   \n",
       "1     398.464564  1.000000  ...  0.600000  64.000000   7.0   7.0  140.500000   \n",
       "2     340.802438  1.000000  ...  1.000000  26.000000   9.0   7.0  154.285714   \n",
       "3     412.324324  1.000000  ...  1.000000  -4.000000  12.0  12.0  108.500000   \n",
       "4     168.041577  0.956522  ...  0.083333   0.022262  11.0  12.0    0.044242   \n",
       "...          ...       ...  ...       ...        ...   ...   ...         ...   \n",
       "2125  364.303573  0.888889  ...  0.777778   0.000000   9.0   8.0  342.857143   \n",
       "2126  181.876516  1.000000  ...  1.000000 -26.000000   8.0   7.0  137.666667   \n",
       "2127  137.696567  1.000000  ...  1.000000  -8.000000  16.0  14.0  102.714286   \n",
       "2128  561.988537  1.000000  ...  1.000000  18.000000   8.0   5.0   62.400000   \n",
       "2129  654.123072  0.400000  ...  0.240000   4.000000   0.0   0.0    0.044242   \n",
       "\n",
       "               208         209           210         211          212  \n",
       "0       729.000000   78.250000   3140.437500  127.600000  1041.440000  \n",
       "1     15314.750000  -27.000000   5249.000000  112.285714  8081.632653  \n",
       "2      1944.489796   18.571429   8070.530612  131.111111  1078.320988  \n",
       "3      6122.750000   46.500000   7081.416667  121.833333   264.305556  \n",
       "4         0.044242  -50.000000      0.000000   45.818182   832.330579  \n",
       "...            ...         ...           ...         ...          ...  \n",
       "2125   2843.265306  205.142857  11207.836735   96.000000  2281.142857  \n",
       "2126    228.555556   87.714286  14282.775510  169.142857    46.693878  \n",
       "2127   1270.061224    7.285714    361.489796   90.400000  2186.240000  \n",
       "2128     51.840000  -45.200000      0.960000  101.000000  5002.000000  \n",
       "2129      0.044242    0.043021      0.043021    0.037385     0.037385  \n",
       "\n",
       "[2130 rows x 213 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv(\"../data_test_frequency.csv\")\n",
    "df_test.drop(columns=[\"Unnamed: 0\"],inplace=True)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = df_train.iloc[:,1:].values\n",
    "y_train = df_train.iloc[:,0].values\n",
    "x_test = df_test.iloc[:,1:].values\n",
    "y_test = df_test.iloc[:,0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = MinMaxScaler()\n",
    "x_train = scale.fit_transform(x_train)\n",
    "x_test = scale.fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components= 0.8)\n",
    "x_train = pca.fit_transform(x_train)\n",
    "x_test = pca.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (4258, 24)\n",
      "Vallidation: (4258, 24)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train , test_size=0.5, shuffle=True, stratify=y_train, random_state=119)\n",
    "print(f\"Train: {x_train.shape}\")\n",
    "print(f\"Vallidation: {x_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier(criterion= 'log_loss', max_depth= 5, max_features= 'sqrt', n_estimators= 1000)\n",
    "ab_clf = AdaBoostClassifier(algorithm= 'SAMME.R', learning_rate= 0.1, n_estimators= 50)\n",
    "knn_clf = KNeighborsClassifier(algorithm= 'auto', n_neighbors= 5, p= 1, weights= 'uniform')\n",
    "svc_clf = SVC(C= 100, gamma= 'scale', kernel= 'rbf', probability= True)\n",
    "xgb_clf = XGBClassifier(gamma= 0,learning_rate= 0.1,max_depth= 5,min_child_weight= 1,n_estimators= 1000)\n",
    "dt_clf = DecisionTreeClassifier(criterion= 'entropy',max_depth= 5,max_features= 'sqrt',splitter= 'best')\n",
    "lgb_clf = LGBMClassifier(boosting = 'gbdt', data_sample_strategy= 'goss', estimators=50, learning_rate = 0.1, objective= 'multiclass')\n",
    "cb_clf = CatBoostClassifier(iterations = 10, learning_rate= 0.1)\n",
    "gb_clf = GradientBoostingClassifier(criterion='squared_error', learning_rate = 0.1, loss= 'log_loss', n_estimators=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Huấn luyện các mô hình con\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mrf_clf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m ab_clf\u001b[38;5;241m.\u001b[39mfit(x_train, y_train)\n\u001b[0;32m      4\u001b[0m knn_clf\u001b[38;5;241m.\u001b[39mfit(x_train, y_train)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\base.py:1351\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1344\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1346\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1347\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1348\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1349\u001b[0m     )\n\u001b[0;32m   1350\u001b[0m ):\n\u001b[1;32m-> 1351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\ensemble\\_forest.py:489\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    478\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    480\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    481\u001b[0m ]\n\u001b[0;32m    483\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    486\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 489\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    505\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    506\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\utils\\parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     66\u001b[0m )\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\testdatasets\\lib\\site-packages\\joblib\\parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1085\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1088\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1089\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1092\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\testdatasets\\lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\testdatasets\\lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\testdatasets\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\testdatasets\\lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\testdatasets\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\testdatasets\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\utils\\parallel.py:129\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    127\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\ensemble\\_forest.py:192\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    190\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[1;32m--> 192\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    200\u001b[0m     tree\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[0;32m    201\u001b[0m         X,\n\u001b[0;32m    202\u001b[0m         y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    205\u001b[0m         missing_values_in_feature_mask\u001b[38;5;241m=\u001b[39mmissing_values_in_feature_mask,\n\u001b[0;32m    206\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\tree\\_classes.py:472\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    462\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    463\u001b[0m         splitter,\n\u001b[0;32m    464\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    470\u001b[0m     )\n\u001b[1;32m--> 472\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Huấn luyện các mô hình con\n",
    "rf_clf.fit(x_train,y_train)\n",
    "ab_clf.fit(x_train, y_train)\n",
    "knn_clf.fit(x_train, y_train)\n",
    "svc_clf.fit(x_train, y_train)\n",
    "xgb_clf.fit(x_train, y_train)\n",
    "dt_clf.fit(x_train,y_train)\n",
    "# lgb_clf.fit(x_train, y_train)\n",
    "# cb_clf.fit(x_train, y_train)\n",
    "# gb_clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dự đoán trên tập huấn luyện để tạo đặc trưng mới cho mô hình blending\n",
    "X_train_meta = np.column_stack((\n",
    "    rf_clf.predict_proba(x_val),\n",
    "    ab_clf.predict_proba(x_val),\n",
    "    knn_clf.predict_proba(x_val),\n",
    "    svc_clf.predict_proba(x_val),\n",
    "    xgb_clf.predict_proba(x_val),\n",
    "    dt_clf.predict_proba(x_val),\n",
    "))\n",
    "# Dự đoán trên tập kiểm tra để tạo đặc trưng mới cho mô hình blending\n",
    "X_test_meta = np.column_stack((\n",
    "    rf_clf.predict_proba(x_test),\n",
    "    ab_clf.predict_proba(x_test),\n",
    "    knn_clf.predict_proba(x_test),\n",
    "    svc_clf.predict_proba(x_test),\n",
    "    xgb_clf.predict_proba(x_test),\n",
    "    dt_clf.predict_proba(x_test),\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_meta:(4258, 24)\n",
      "X_test_meta:(2130, 24)\n"
     ]
    }
   ],
   "source": [
    "print(f\"X_train_meta:{X_train_meta.shape}\")\n",
    "print(f\"X_test_meta:{X_test_meta.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 48 candidates, totalling 144 fits\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.001, loss=log_loss, n_estimators=100;, score=(train=0.365, test=0.365) total time=   4.3s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.001, loss=log_loss, n_estimators=100;, score=(train=0.366, test=0.365) total time=   4.3s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.001, loss=log_loss, n_estimators=100;, score=(train=0.365, test=0.366) total time=   4.3s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.001, loss=log_loss, n_estimators=200;, score=(train=0.932, test=0.918) total time=   8.7s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.001, loss=log_loss, n_estimators=200;, score=(train=0.915, test=0.918) total time=   8.6s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.001, loss=log_loss, n_estimators=200;, score=(train=0.932, test=0.899) total time=   8.9s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.001, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.001, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.001, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.001, loss=deviance, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.001, loss=deviance, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.001, loss=deviance, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.001, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.001, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.001, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.001, loss=exponential, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.001, loss=exponential, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.001, loss=exponential, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.01, loss=log_loss, n_estimators=100;, score=(train=0.955, test=0.935) total time=   4.3s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.01, loss=log_loss, n_estimators=100;, score=(train=0.953, test=0.946) total time=   4.4s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.01, loss=log_loss, n_estimators=100;, score=(train=0.960, test=0.925) total time=   4.2s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.01, loss=log_loss, n_estimators=200;, score=(train=0.962, test=0.935) total time=   8.7s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.01, loss=log_loss, n_estimators=200;, score=(train=0.957, test=0.949) total time=   8.7s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.01, loss=log_loss, n_estimators=200;, score=(train=0.964, test=0.930) total time=   8.7s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.01, loss=deviance, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.01, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.01, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.01, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.01, loss=exponential, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.01, loss=exponential, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.01, loss=exponential, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.1, loss=log_loss, n_estimators=100;, score=(train=0.995, test=0.929) total time=   4.1s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.1, loss=log_loss, n_estimators=100;, score=(train=0.993, test=0.942) total time=   4.2s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.1, loss=log_loss, n_estimators=100;, score=(train=0.998, test=0.930) total time=   4.1s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.1, loss=log_loss, n_estimators=200;, score=(train=1.000, test=0.926) total time=   8.5s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.1, loss=log_loss, n_estimators=200;, score=(train=1.000, test=0.942) total time=   8.8s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.1, loss=log_loss, n_estimators=200;, score=(train=1.000, test=0.925) total time=   8.8s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.1, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.1, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.1, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.1, loss=exponential, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.1, loss=exponential, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.1, loss=exponential, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=1, loss=log_loss, n_estimators=100;, score=(train=1.000, test=0.925) total time=   4.3s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=1, loss=log_loss, n_estimators=100;, score=(train=1.000, test=0.929) total time=   4.5s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=1, loss=log_loss, n_estimators=100;, score=(train=1.000, test=0.918) total time=   4.4s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=1, loss=log_loss, n_estimators=200;, score=(train=1.000, test=0.924) total time=   8.9s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=1, loss=log_loss, n_estimators=200;, score=(train=0.999, test=0.930) total time=   9.2s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=1, loss=log_loss, n_estimators=200;, score=(train=1.000, test=0.920) total time=  10.2s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=1, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=1, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=1, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=1, loss=deviance, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=1, loss=deviance, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=1, loss=deviance, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=1, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=1, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=1, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=1, loss=exponential, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=1, loss=exponential, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=1, loss=exponential, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.001, loss=log_loss, n_estimators=100;, score=(train=0.365, test=0.365) total time=   5.6s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.001, loss=log_loss, n_estimators=100;, score=(train=0.366, test=0.365) total time=   5.3s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.001, loss=log_loss, n_estimators=100;, score=(train=0.365, test=0.366) total time=   5.3s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.001, loss=log_loss, n_estimators=200;, score=(train=0.932, test=0.918) total time=   8.9s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.001, loss=log_loss, n_estimators=200;, score=(train=0.915, test=0.918) total time=   8.7s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.001, loss=log_loss, n_estimators=200;, score=(train=0.932, test=0.899) total time=   8.7s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.001, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.001, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.001, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.001, loss=deviance, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.001, loss=deviance, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.001, loss=deviance, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.001, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.001, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.001, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.001, loss=exponential, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.001, loss=exponential, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.001, loss=exponential, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.01, loss=log_loss, n_estimators=100;, score=(train=0.955, test=0.935) total time=   4.4s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.01, loss=log_loss, n_estimators=100;, score=(train=0.953, test=0.946) total time=   4.3s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.01, loss=log_loss, n_estimators=100;, score=(train=0.960, test=0.925) total time=   4.6s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.01, loss=log_loss, n_estimators=200;, score=(train=0.962, test=0.936) total time=   9.6s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.01, loss=log_loss, n_estimators=200;, score=(train=0.957, test=0.949) total time=  10.2s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.01, loss=log_loss, n_estimators=200;, score=(train=0.964, test=0.930) total time=  10.4s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.01, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.01, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.01, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.01, loss=deviance, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.01, loss=deviance, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.01, loss=deviance, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.01, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.01, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.01, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.01, loss=exponential, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.01, loss=exponential, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.01, loss=exponential, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.1, loss=log_loss, n_estimators=100;, score=(train=0.995, test=0.928) total time=   5.3s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.1, loss=log_loss, n_estimators=100;, score=(train=0.991, test=0.941) total time=   4.5s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.1, loss=log_loss, n_estimators=100;, score=(train=0.998, test=0.930) total time=   4.2s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.1, loss=log_loss, n_estimators=200;, score=(train=1.000, test=0.923) total time=   8.9s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.1, loss=log_loss, n_estimators=200;, score=(train=1.000, test=0.942) total time=   9.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.1, loss=log_loss, n_estimators=200;, score=(train=1.000, test=0.927) total time=   9.1s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.1, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.1, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.1, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.1, loss=deviance, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.1, loss=deviance, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.1, loss=deviance, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.1, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.1, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.1, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.1, loss=exponential, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.1, loss=exponential, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.1, loss=exponential, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=1, loss=log_loss, n_estimators=100;, score=(train=1.000, test=0.911) total time=   4.3s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=1, loss=log_loss, n_estimators=100;, score=(train=1.000, test=0.930) total time=   4.5s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=1, loss=log_loss, n_estimators=100;, score=(train=1.000, test=0.916) total time=   4.7s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=1, loss=log_loss, n_estimators=200;, score=(train=1.000, test=0.925) total time=  10.2s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=1, loss=log_loss, n_estimators=200;, score=(train=0.999, test=0.928) total time=  10.8s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=1, loss=log_loss, n_estimators=200;, score=(train=1.000, test=0.913) total time=   8.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=1, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=1, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=1, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=1, loss=deviance, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=1, loss=deviance, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=1, loss=deviance, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=1, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=1, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=1, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=1, loss=exponential, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=1, loss=exponential, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=1, loss=exponential, n_estimators=200;, score=(train=nan, test=nan) total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_validation.py:542: FitFailedWarning: \n",
      "96 fits failed out of a total of 144.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "48 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_validation.py\", line 890, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\base.py\", line 1344, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of GradientBoostingClassifier must be a str among {'exponential', 'log_loss'}. Got 'deviance' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "48 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_validation.py\", line 890, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\base.py\", line 1351, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\ensemble\\_gb.py\", line 673, in fit\n",
      "    self._loss = self._get_loss(sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\ensemble\\_gb.py\", line 1537, in _get_loss\n",
      "    raise ValueError(\n",
      "ValueError: loss='exponential' is only suitable for a binary classification problem, you have n_classes=4. Please use loss='log_loss' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the test scores are non-finite: [0.36542976 0.91169408        nan        nan        nan        nan\n",
      " 0.93541574 0.93776481        nan        nan        nan        nan\n",
      " 0.93377287 0.93118956        nan        nan        nan        nan\n",
      " 0.92367286 0.92461265        nan        nan        nan        nan\n",
      " 0.36542976 0.91169408        nan        nan        nan        nan\n",
      " 0.93518099 0.93823446        nan        nan        nan        nan\n",
      " 0.93283341 0.93025076        nan        nan        nan        nan\n",
      " 0.91874311 0.92226325        nan        nan        nan        nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the train scores are non-finite: [0.36542978 0.926492          nan        nan        nan        nan\n",
      " 0.95596516 0.96077985        nan        nan        nan        nan\n",
      " 0.99518552 1.                nan        nan        nan        nan\n",
      " 1.         0.99976518        nan        nan        nan        nan\n",
      " 0.36542978 0.926492          nan        nan        nan        nan\n",
      " 0.95596516 0.96077985        nan        nan        nan        nan\n",
      " 0.99459842 1.                nan        nan        nan        nan\n",
      " 1.         0.99976518        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "model = GradientBoostingClassifier()\n",
    "params = {\n",
    "    'loss':['log_loss', 'deviance', 'exponential'],\n",
    "    'learning_rate':[0.001,0.01,0.1,1],\n",
    "    'n_estimators':[100,200],\n",
    "    'criterion':['friedman_mse', 'squared_error']\n",
    "}\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=params, cv=3, verbose=5, return_train_score=True,refit=True)\n",
    "grid_model = grid_search.fit(X_train_meta,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_test = grid_model.predict(X_test_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'squared_error',\n",
       " 'learning_rate': 0.01,\n",
       " 'loss': 'log_loss',\n",
       " 'n_estimators': 200}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9382344572485417"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_model.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay,multilabel_confusion_matrix,f1_score,precision_score,accuracy_score,recall_score,precision_recall_fscore_support\n",
    "def evaluation_test(y,y_pred):\n",
    "    cm = confusion_matrix(y,y_pred)\n",
    "    disp = ConfusionMatrixDisplay(cm,display_labels=['AFIB','SB','SR','GSVT'])\n",
    "    disp.plot()\n",
    "    plt.show()\n",
    "    n_classes = len(cm)\n",
    "    result = []\n",
    "    for c in range(n_classes):\n",
    "        tp = cm[c,c]\n",
    "        fp = sum(cm[:,c]) - cm[c,c]\n",
    "        fn = sum(cm[c,:]) - cm[c,c]\n",
    "        tn = sum(np.delete(sum(cm)-cm[c,:],c))\n",
    "        acc = (tp+tn) / (tp+fn+tn+fp)\n",
    "        recall = tp/(tp+fn)\n",
    "        precision = tp/(tp+fp)\n",
    "        specificity = tn/(tn+fp)\n",
    "        f1_score = 2*((precision*recall)/(precision+recall))\n",
    "        if c+1 == 1:\n",
    "            Rhythm = 'AFIB'\n",
    "        elif c+1 == 2:\n",
    "            Rhythm = 'SB'\n",
    "        elif c+1 == 3:\n",
    "            Rhythm = 'SR'\n",
    "        else:\n",
    "            Rhythm = 'GSVT'\n",
    "        result.append([Rhythm,acc,recall,precision,f1_score,specificity])\n",
    "    p_macro,r_macro,f_macro,support_macro = precision_recall_fscore_support(y,y_pred,average='macro')\n",
    "    p_micro,r_micro,f_micro,support_micro = precision_recall_fscore_support(y,y_pred,average='micro')\n",
    "    p_weighted,r_weighted,f_weighted,support_weighted = precision_recall_fscore_support(y,y_pred,average='weighted')\n",
    "    result.append(['macro avg',None,f_macro,p_macro,r_macro,None])\n",
    "    result.append(['micro avg',None,f_micro,p_micro,r_micro,None])\n",
    "    result.append(['weighted avg',None,f_weighted,p_weighted,r_weighted,None])\n",
    "    return result\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rhythm Group</th>\n",
       "      <th>ACC</th>\n",
       "      <th>F1-score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>specificity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AFIB</td>\n",
       "      <td>0.963850</td>\n",
       "      <td>0.889888</td>\n",
       "      <td>0.933962</td>\n",
       "      <td>0.911392</td>\n",
       "      <td>0.983383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SB</td>\n",
       "      <td>0.913146</td>\n",
       "      <td>0.993573</td>\n",
       "      <td>0.811123</td>\n",
       "      <td>0.893125</td>\n",
       "      <td>0.866864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SR</td>\n",
       "      <td>0.903756</td>\n",
       "      <td>0.617978</td>\n",
       "      <td>0.887097</td>\n",
       "      <td>0.728477</td>\n",
       "      <td>0.979228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GSVT</td>\n",
       "      <td>0.958216</td>\n",
       "      <td>0.883117</td>\n",
       "      <td>0.920993</td>\n",
       "      <td>0.901657</td>\n",
       "      <td>0.979017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>macro avg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.858663</td>\n",
       "      <td>0.888294</td>\n",
       "      <td>0.846139</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>micro avg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.869484</td>\n",
       "      <td>0.869484</td>\n",
       "      <td>0.869484</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>weighted avg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.864394</td>\n",
       "      <td>0.876490</td>\n",
       "      <td>0.869484</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rhythm Group       ACC  F1-score  Precision    Recall  specificity\n",
       "0          AFIB  0.963850  0.889888   0.933962  0.911392     0.983383\n",
       "1            SB  0.913146  0.993573   0.811123  0.893125     0.866864\n",
       "2            SR  0.903756  0.617978   0.887097  0.728477     0.979228\n",
       "3          GSVT  0.958216  0.883117   0.920993  0.901657     0.979017\n",
       "4     macro avg       NaN  0.858663   0.888294  0.846139          NaN\n",
       "5     micro avg       NaN  0.869484   0.869484  0.869484          NaN\n",
       "6  weighted avg       NaN  0.864394   0.876490  0.869484          NaN"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_test = evaluation_test(y_test,result_test)\n",
    "df_evaluation_test = pd.DataFrame(data=evaluation_test,columns=[\"Rhythm Group\",\"ACC\",\"F1-score\",\"Precision\",\"Recall\",\"specificity\"])\n",
    "df_evaluation_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_evaluation_test.to_csv(\"./Result/Blending_GB.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
