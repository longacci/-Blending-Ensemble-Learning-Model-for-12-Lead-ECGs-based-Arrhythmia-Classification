{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>204</th>\n",
       "      <th>205</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>950.000000</td>\n",
       "      <td>1074.0</td>\n",
       "      <td>274.986868</td>\n",
       "      <td>782.0</td>\n",
       "      <td>-0.319753</td>\n",
       "      <td>-1.432466</td>\n",
       "      <td>325.821586</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>172.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>252.222222</td>\n",
       "      <td>10656.395062</td>\n",
       "      <td>87.777778</td>\n",
       "      <td>10339.061728</td>\n",
       "      <td>135.800000</td>\n",
       "      <td>4315.560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>574.500000</td>\n",
       "      <td>582.0</td>\n",
       "      <td>104.913059</td>\n",
       "      <td>378.0</td>\n",
       "      <td>0.158313</td>\n",
       "      <td>-0.696295</td>\n",
       "      <td>336.569414</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>-15.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>158.000000</td>\n",
       "      <td>3944.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>6555.000000</td>\n",
       "      <td>-1.066667</td>\n",
       "      <td>697.528889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>593.600000</td>\n",
       "      <td>594.0</td>\n",
       "      <td>4.687572</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.396421</td>\n",
       "      <td>-0.312612</td>\n",
       "      <td>94.909877</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>122.400000</td>\n",
       "      <td>2058.773333</td>\n",
       "      <td>12.533333</td>\n",
       "      <td>1360.782222</td>\n",
       "      <td>95.500000</td>\n",
       "      <td>68.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>420.090909</td>\n",
       "      <td>420.0</td>\n",
       "      <td>3.591772</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-0.021014</td>\n",
       "      <td>-0.856142</td>\n",
       "      <td>254.059787</td>\n",
       "      <td>0.826087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.739130</td>\n",
       "      <td>-9.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>40.666667</td>\n",
       "      <td>1120.888889</td>\n",
       "      <td>5.333333</td>\n",
       "      <td>1504.888889</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>1464.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1068.750000</td>\n",
       "      <td>1075.0</td>\n",
       "      <td>25.118469</td>\n",
       "      <td>76.0</td>\n",
       "      <td>-0.276816</td>\n",
       "      <td>-1.271399</td>\n",
       "      <td>461.130814</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>671.000000</td>\n",
       "      <td>19.750000</td>\n",
       "      <td>569.437500</td>\n",
       "      <td>136.444444</td>\n",
       "      <td>43.358025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8511</th>\n",
       "      <td>3.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>615.733333</td>\n",
       "      <td>596.0</td>\n",
       "      <td>51.114860</td>\n",
       "      <td>152.0</td>\n",
       "      <td>2.153820</td>\n",
       "      <td>2.645687</td>\n",
       "      <td>365.256750</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003757</td>\n",
       "      <td>0.022262</td>\n",
       "      <td>0.003757</td>\n",
       "      <td>0.003757</td>\n",
       "      <td>0.044242</td>\n",
       "      <td>0.044242</td>\n",
       "      <td>0.043021</td>\n",
       "      <td>0.043021</td>\n",
       "      <td>0.037385</td>\n",
       "      <td>0.037385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8512</th>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1091.500000</td>\n",
       "      <td>1093.0</td>\n",
       "      <td>5.894913</td>\n",
       "      <td>18.0</td>\n",
       "      <td>-0.311206</td>\n",
       "      <td>-1.184514</td>\n",
       "      <td>358.414529</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>81.428571</td>\n",
       "      <td>1294.530612</td>\n",
       "      <td>-40.000000</td>\n",
       "      <td>1746.285714</td>\n",
       "      <td>155.333333</td>\n",
       "      <td>4722.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8513</th>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>654.428571</td>\n",
       "      <td>648.0</td>\n",
       "      <td>107.653355</td>\n",
       "      <td>458.0</td>\n",
       "      <td>0.475616</td>\n",
       "      <td>0.784000</td>\n",
       "      <td>180.045117</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>77.142857</td>\n",
       "      <td>2213.551020</td>\n",
       "      <td>-1.714286</td>\n",
       "      <td>2686.204082</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>3602.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8514</th>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1075.000000</td>\n",
       "      <td>1083.0</td>\n",
       "      <td>24.535688</td>\n",
       "      <td>66.0</td>\n",
       "      <td>-0.263431</td>\n",
       "      <td>-1.567800</td>\n",
       "      <td>251.455499</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>101.142857</td>\n",
       "      <td>4933.551020</td>\n",
       "      <td>-10.750000</td>\n",
       "      <td>7259.937500</td>\n",
       "      <td>88.222222</td>\n",
       "      <td>202.172840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8515</th>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1041.250000</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>8.242421</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.214800</td>\n",
       "      <td>-1.575835</td>\n",
       "      <td>505.203302</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>350.000000</td>\n",
       "      <td>-20.000000</td>\n",
       "      <td>588.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8516 rows × 213 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        1     2            3       4           5      6         7         8  \\\n",
       "0     0.0  10.0   950.000000  1074.0  274.986868  782.0 -0.319753 -1.432466   \n",
       "1     0.0  17.0   574.500000   582.0  104.913059  378.0  0.158313 -0.696295   \n",
       "2     3.0  16.0   593.600000   594.0    4.687572   18.0  0.396421 -0.312612   \n",
       "3     3.0  23.0   420.090909   420.0    3.591772   12.0 -0.021014 -0.856142   \n",
       "4     1.0   9.0  1068.750000  1075.0   25.118469   76.0 -0.276816 -1.271399   \n",
       "...   ...   ...          ...     ...         ...    ...       ...       ...   \n",
       "8511  3.0  16.0   615.733333   596.0   51.114860  152.0  2.153820  2.645687   \n",
       "8512  1.0   9.0  1091.500000  1093.0    5.894913   18.0 -0.311206 -1.184514   \n",
       "8513  2.0  15.0   654.428571   648.0  107.653355  458.0  0.475616  0.784000   \n",
       "8514  1.0   9.0  1075.000000  1083.0   24.535688   66.0 -0.263431 -1.567800   \n",
       "8515  1.0   9.0  1041.250000  1040.0    8.242421   22.0  0.214800 -1.575835   \n",
       "\n",
       "               9        10  ...       204         205        206        207  \\\n",
       "0     325.821586  1.000000  ...  1.000000  172.000000  10.000000   9.000000   \n",
       "1     336.569414  1.000000  ...  0.882353  -15.000000  15.000000   4.000000   \n",
       "2      94.909877  1.000000  ...  1.000000   -4.000000  16.000000  15.000000   \n",
       "3     254.059787  0.826087  ...  0.739130   -9.000000   6.000000   4.000000   \n",
       "4     461.130814  1.000000  ...  1.000000    2.000000   9.000000   8.000000   \n",
       "...          ...       ...  ...       ...         ...        ...        ...   \n",
       "8511  365.256750  1.000000  ...  0.003757    0.022262   0.003757   0.003757   \n",
       "8512  358.414529  1.000000  ...  0.888889   -3.000000   9.000000   8.000000   \n",
       "8513  180.045117  1.000000  ...  1.000000   -4.000000  15.000000  14.000000   \n",
       "8514  251.455499  1.000000  ...  1.000000   14.000000   9.000000   8.000000   \n",
       "8515  505.203302  1.000000  ...  1.000000    0.000000   9.000000   8.000000   \n",
       "\n",
       "             208           209        210           211         212  \\\n",
       "0     252.222222  10656.395062  87.777778  10339.061728  135.800000   \n",
       "1     158.000000   3944.000000  73.000000   6555.000000   -1.066667   \n",
       "2     122.400000   2058.773333  12.533333   1360.782222   95.500000   \n",
       "3      40.666667   1120.888889   5.333333   1504.888889   12.000000   \n",
       "4     122.000000    671.000000  19.750000    569.437500  136.444444   \n",
       "...          ...           ...        ...           ...         ...   \n",
       "8511    0.044242      0.044242   0.043021      0.043021    0.037385   \n",
       "8512   81.428571   1294.530612 -40.000000   1746.285714  155.333333   \n",
       "8513   77.142857   2213.551020  -1.714286   2686.204082  104.000000   \n",
       "8514  101.142857   4933.551020 -10.750000   7259.937500   88.222222   \n",
       "8515  102.000000    350.000000 -20.000000    588.000000  150.000000   \n",
       "\n",
       "              213  \n",
       "0     4315.560000  \n",
       "1      697.528889  \n",
       "2       68.750000  \n",
       "3     1464.000000  \n",
       "4       43.358025  \n",
       "...           ...  \n",
       "8511     0.037385  \n",
       "8512  4722.666667  \n",
       "8513  3602.666667  \n",
       "8514   202.172840  \n",
       "8515     0.000000  \n",
       "\n",
       "[8516 rows x 213 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"../data_train_frequency.csv\")\n",
    "df_train.drop(columns=[\"Unnamed: 0\"],inplace=True)\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = df_train.iloc[:,1:].values\n",
    "y_train = df_train.iloc[:,0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = MinMaxScaler()\n",
    "x_train = scale.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>203</th>\n",
       "      <th>204</th>\n",
       "      <th>205</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>710.769231</td>\n",
       "      <td>628.0</td>\n",
       "      <td>153.204817</td>\n",
       "      <td>556.0</td>\n",
       "      <td>0.996355</td>\n",
       "      <td>0.207174</td>\n",
       "      <td>459.037295</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>-10.000000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>729.000000</td>\n",
       "      <td>78.250000</td>\n",
       "      <td>3140.437500</td>\n",
       "      <td>127.600000</td>\n",
       "      <td>1041.440000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>968.666667</td>\n",
       "      <td>894.0</td>\n",
       "      <td>266.399867</td>\n",
       "      <td>932.0</td>\n",
       "      <td>0.979352</td>\n",
       "      <td>0.388359</td>\n",
       "      <td>398.464564</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>140.500000</td>\n",
       "      <td>15314.750000</td>\n",
       "      <td>-27.000000</td>\n",
       "      <td>5249.000000</td>\n",
       "      <td>112.285714</td>\n",
       "      <td>8081.632653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>797.000000</td>\n",
       "      <td>780.0</td>\n",
       "      <td>251.329664</td>\n",
       "      <td>794.0</td>\n",
       "      <td>0.260470</td>\n",
       "      <td>-1.002325</td>\n",
       "      <td>340.802438</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>154.285714</td>\n",
       "      <td>1944.489796</td>\n",
       "      <td>18.571429</td>\n",
       "      <td>8070.530612</td>\n",
       "      <td>131.111111</td>\n",
       "      <td>1078.320988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>757.500000</td>\n",
       "      <td>755.0</td>\n",
       "      <td>8.986100</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.048579</td>\n",
       "      <td>-1.449012</td>\n",
       "      <td>412.324324</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>108.500000</td>\n",
       "      <td>6122.750000</td>\n",
       "      <td>46.500000</td>\n",
       "      <td>7081.416667</td>\n",
       "      <td>121.833333</td>\n",
       "      <td>264.305556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>413.909091</td>\n",
       "      <td>409.0</td>\n",
       "      <td>82.344017</td>\n",
       "      <td>426.0</td>\n",
       "      <td>3.023659</td>\n",
       "      <td>10.404884</td>\n",
       "      <td>168.041577</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.022262</td>\n",
       "      <td>11.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.044242</td>\n",
       "      <td>0.044242</td>\n",
       "      <td>-50.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>45.818182</td>\n",
       "      <td>832.330579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2125</th>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1071.250000</td>\n",
       "      <td>1062.0</td>\n",
       "      <td>36.509417</td>\n",
       "      <td>118.0</td>\n",
       "      <td>1.263183</td>\n",
       "      <td>0.543003</td>\n",
       "      <td>364.303573</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>342.857143</td>\n",
       "      <td>2843.265306</td>\n",
       "      <td>205.142857</td>\n",
       "      <td>11207.836735</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>2281.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2126</th>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1196.000000</td>\n",
       "      <td>1202.0</td>\n",
       "      <td>33.839959</td>\n",
       "      <td>102.0</td>\n",
       "      <td>-0.454057</td>\n",
       "      <td>-1.036905</td>\n",
       "      <td>181.876516</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-26.000000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>137.666667</td>\n",
       "      <td>228.555556</td>\n",
       "      <td>87.714286</td>\n",
       "      <td>14282.775510</td>\n",
       "      <td>169.142857</td>\n",
       "      <td>46.693878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2127</th>\n",
       "      <td>3.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>595.600000</td>\n",
       "      <td>590.0</td>\n",
       "      <td>23.734082</td>\n",
       "      <td>82.0</td>\n",
       "      <td>0.371174</td>\n",
       "      <td>-0.657132</td>\n",
       "      <td>137.696567</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-8.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>102.714286</td>\n",
       "      <td>1270.061224</td>\n",
       "      <td>7.285714</td>\n",
       "      <td>361.489796</td>\n",
       "      <td>90.400000</td>\n",
       "      <td>2186.240000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2128</th>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1080.285714</td>\n",
       "      <td>996.0</td>\n",
       "      <td>180.470587</td>\n",
       "      <td>448.0</td>\n",
       "      <td>0.587475</td>\n",
       "      <td>-1.363827</td>\n",
       "      <td>561.988537</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>62.400000</td>\n",
       "      <td>51.840000</td>\n",
       "      <td>-45.200000</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>5002.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2129</th>\n",
       "      <td>3.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>391.250000</td>\n",
       "      <td>390.0</td>\n",
       "      <td>2.569857</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.605786</td>\n",
       "      <td>-0.869886</td>\n",
       "      <td>654.123072</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.044242</td>\n",
       "      <td>0.044242</td>\n",
       "      <td>0.043021</td>\n",
       "      <td>0.043021</td>\n",
       "      <td>0.037385</td>\n",
       "      <td>0.037385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2130 rows × 213 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0     1            2       3           4      5         6          7  \\\n",
       "0     0.0  14.0   710.769231   628.0  153.204817  556.0  0.996355   0.207174   \n",
       "1     0.0  10.0   968.666667   894.0  266.399867  932.0  0.979352   0.388359   \n",
       "2     0.0  11.0   797.000000   780.0  251.329664  794.0  0.260470  -1.002325   \n",
       "3     2.0  13.0   757.500000   755.0    8.986100   26.0  0.048579  -1.449012   \n",
       "4     0.0  23.0   413.909091   409.0   82.344017  426.0  3.023659  10.404884   \n",
       "...   ...   ...          ...     ...         ...    ...       ...        ...   \n",
       "2125  1.0   9.0  1071.250000  1062.0   36.509417  118.0  1.263183   0.543003   \n",
       "2126  1.0   8.0  1196.000000  1202.0   33.839959  102.0 -0.454057  -1.036905   \n",
       "2127  3.0  16.0   595.600000   590.0   23.734082   82.0  0.371174  -0.657132   \n",
       "2128  1.0   8.0  1080.285714   996.0  180.470587  448.0  0.587475  -1.363827   \n",
       "2129  3.0  25.0   391.250000   390.0    2.569857    8.0  0.605786  -0.869886   \n",
       "\n",
       "               8         9  ...       203        204   205   206         207  \\\n",
       "0     459.037295  1.000000  ...  0.928571 -10.000000  10.0   9.0  146.000000   \n",
       "1     398.464564  1.000000  ...  0.600000  64.000000   7.0   7.0  140.500000   \n",
       "2     340.802438  1.000000  ...  1.000000  26.000000   9.0   7.0  154.285714   \n",
       "3     412.324324  1.000000  ...  1.000000  -4.000000  12.0  12.0  108.500000   \n",
       "4     168.041577  0.956522  ...  0.083333   0.022262  11.0  12.0    0.044242   \n",
       "...          ...       ...  ...       ...        ...   ...   ...         ...   \n",
       "2125  364.303573  0.888889  ...  0.777778   0.000000   9.0   8.0  342.857143   \n",
       "2126  181.876516  1.000000  ...  1.000000 -26.000000   8.0   7.0  137.666667   \n",
       "2127  137.696567  1.000000  ...  1.000000  -8.000000  16.0  14.0  102.714286   \n",
       "2128  561.988537  1.000000  ...  1.000000  18.000000   8.0   5.0   62.400000   \n",
       "2129  654.123072  0.400000  ...  0.240000   4.000000   0.0   0.0    0.044242   \n",
       "\n",
       "               208         209           210         211          212  \n",
       "0       729.000000   78.250000   3140.437500  127.600000  1041.440000  \n",
       "1     15314.750000  -27.000000   5249.000000  112.285714  8081.632653  \n",
       "2      1944.489796   18.571429   8070.530612  131.111111  1078.320988  \n",
       "3      6122.750000   46.500000   7081.416667  121.833333   264.305556  \n",
       "4         0.044242  -50.000000      0.000000   45.818182   832.330579  \n",
       "...            ...         ...           ...         ...          ...  \n",
       "2125   2843.265306  205.142857  11207.836735   96.000000  2281.142857  \n",
       "2126    228.555556   87.714286  14282.775510  169.142857    46.693878  \n",
       "2127   1270.061224    7.285714    361.489796   90.400000  2186.240000  \n",
       "2128     51.840000  -45.200000      0.960000  101.000000  5002.000000  \n",
       "2129      0.044242    0.043021      0.043021    0.037385     0.037385  \n",
       "\n",
       "[2130 rows x 213 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv(\"../data_test_frequency.csv\")\n",
    "df_test.drop(columns=[\"Unnamed: 0\"],inplace=True)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = df_test.iloc[:,1:].values\n",
    "y_test = df_test.iloc[:,0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = scale.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components= 0.9)\n",
    "x_train = pca.fit_transform(x_train)\n",
    "x_test = pca.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (4258, 51)\n",
      "Vallidation: (4258, 51)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train , test_size=0.5, shuffle=True, stratify=y_train, random_state=119)\n",
    "print(f\"Train: {x_train.shape}\")\n",
    "print(f\"Vallidation: {x_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.92751237,  0.24428387,  0.21880476, ...,  0.11821269,\n",
       "         0.13078917,  0.16361813],\n",
       "       [-0.09196007, -0.27562514, -0.07993932, ..., -0.00241733,\n",
       "         0.01987519, -0.04117443],\n",
       "       [-0.90080863,  0.17058481,  1.13973268, ...,  0.03666463,\n",
       "         0.31670008,  0.15303079],\n",
       "       ...,\n",
       "       [ 1.46079875,  0.33639184,  0.28623897, ..., -0.05690723,\n",
       "        -0.09643881,  0.09462577],\n",
       "       [-0.83333127,  0.10273478, -0.1787326 , ...,  0.10302272,\n",
       "         0.08335295,  0.01152653],\n",
       "       [-1.03894045,  0.27679287, -0.15405236, ..., -0.00556075,\n",
       "        -0.08800187, -0.13263636]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf_clf = RandomForestClassifier(criterion= 'log_loss', max_depth= 5, max_features= 'sqrt', n_estimators= 1000)\n",
    "ab_clf = AdaBoostClassifier(algorithm= 'SAMME.R', learning_rate= 0.1, n_estimators= 50)\n",
    "knn_clf = KNeighborsClassifier(algorithm= 'auto', n_neighbors= 5, p= 1, weights= 'uniform')\n",
    "svc_clf = SVC(C= 100, gamma= 'scale', kernel= 'rbf', probability= True)\n",
    "xgb_clf = XGBClassifier(gamma= 0,learning_rate= 0.1,max_depth= 5,min_child_weight= 1,n_estimators= 1000)\n",
    "dt_clf = DecisionTreeClassifier(criterion= 'entropy',max_depth= 5,max_features= 'sqrt',splitter= 'best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(criterion=&#x27;entropy&#x27;, max_depth=5, max_features=&#x27;sqrt&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;DecisionTreeClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.tree.DecisionTreeClassifier.html\">?<span>Documentation for DecisionTreeClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>DecisionTreeClassifier(criterion=&#x27;entropy&#x27;, max_depth=5, max_features=&#x27;sqrt&#x27;)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier(criterion='entropy', max_depth=5, max_features='sqrt')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Huấn luyện các mô hình con\n",
    "# rf_clf.fit(x_train,y_train)\n",
    "ab_clf.fit(x_train, y_train)\n",
    "knn_clf.fit(x_train, y_train)\n",
    "svc_clf.fit(x_train, y_train)\n",
    "xgb_clf.fit(x_train, y_train)\n",
    "dt_clf.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dự đoán trên tập huấn luyện để tạo đặc trưng mới cho mô hình blending\n",
    "X_train_meta = np.column_stack((\n",
    "    # rf_clf.predict_proba(x_val),\n",
    "    ab_clf.predict_proba(x_val),\n",
    "    knn_clf.predict_proba(x_val),\n",
    "    svc_clf.predict_proba(x_val),\n",
    "    xgb_clf.predict_proba(x_val),\n",
    "    dt_clf.predict_proba(x_val)\n",
    "))\n",
    "# Dự đoán trên tập kiểm tra để tạo đặc trưng mới cho mô hình blending\n",
    "X_test_meta = np.column_stack((\n",
    "    # rf_clf.predict_proba(x_test),\n",
    "    ab_clf.predict_proba(x_test),\n",
    "    knn_clf.predict_proba(x_test),\n",
    "    svc_clf.predict_proba(x_test),\n",
    "    xgb_clf.predict_proba(x_test),\n",
    "    dt_clf.predict_proba(x_test)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_meta:(4258, 20)\n",
      "X_test_meta:(2130, 20)\n"
     ]
    }
   ],
   "source": [
    "print(f\"X_train_meta:{X_train_meta.shape}\")\n",
    "print(f\"X_test_meta:{X_test_meta.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 72 candidates, totalling 216 fits\n",
      "[CV 1/3] END criterion=gini, max_depth=3, max_features=sqrt, n_estimators=10;, score=(train=0.941, test=0.938) total time=   0.0s\n",
      "[CV 2/3] END criterion=gini, max_depth=3, max_features=sqrt, n_estimators=10;, score=(train=0.940, test=0.946) total time=   0.0s\n",
      "[CV 3/3] END criterion=gini, max_depth=3, max_features=sqrt, n_estimators=10;, score=(train=0.943, test=0.927) total time=   0.0s\n",
      "[CV 1/3] END criterion=gini, max_depth=3, max_features=sqrt, n_estimators=50;, score=(train=0.942, test=0.944) total time=   0.1s\n",
      "[CV 2/3] END criterion=gini, max_depth=3, max_features=sqrt, n_estimators=50;, score=(train=0.939, test=0.946) total time=   0.0s\n",
      "[CV 3/3] END criterion=gini, max_depth=3, max_features=sqrt, n_estimators=50;, score=(train=0.946, test=0.925) total time=   0.0s\n",
      "[CV 1/3] END criterion=gini, max_depth=3, max_features=sqrt, n_estimators=100;, score=(train=0.944, test=0.942) total time=   0.2s\n",
      "[CV 2/3] END criterion=gini, max_depth=3, max_features=sqrt, n_estimators=100;, score=(train=0.941, test=0.946) total time=   0.2s\n",
      "[CV 3/3] END criterion=gini, max_depth=3, max_features=sqrt, n_estimators=100;, score=(train=0.949, test=0.930) total time=   0.2s\n",
      "[CV 1/3] END criterion=gini, max_depth=3, max_features=sqrt, n_estimators=1000;, score=(train=0.943, test=0.943) total time=   2.7s\n",
      "[CV 2/3] END criterion=gini, max_depth=3, max_features=sqrt, n_estimators=1000;, score=(train=0.941, test=0.949) total time=   2.7s\n",
      "[CV 3/3] END criterion=gini, max_depth=3, max_features=sqrt, n_estimators=1000;, score=(train=0.948, test=0.931) total time=   2.9s\n",
      "[CV 1/3] END criterion=gini, max_depth=3, max_features=log2, n_estimators=10;, score=(train=0.937, test=0.935) total time=   0.0s\n",
      "[CV 2/3] END criterion=gini, max_depth=3, max_features=log2, n_estimators=10;, score=(train=0.940, test=0.946) total time=   0.0s\n",
      "[CV 3/3] END criterion=gini, max_depth=3, max_features=log2, n_estimators=10;, score=(train=0.944, test=0.931) total time=   0.0s\n",
      "[CV 1/3] END criterion=gini, max_depth=3, max_features=log2, n_estimators=50;, score=(train=0.942, test=0.943) total time=   0.1s\n",
      "[CV 2/3] END criterion=gini, max_depth=3, max_features=log2, n_estimators=50;, score=(train=0.941, test=0.949) total time=   0.1s\n",
      "[CV 3/3] END criterion=gini, max_depth=3, max_features=log2, n_estimators=50;, score=(train=0.948, test=0.928) total time=   0.1s\n",
      "[CV 1/3] END criterion=gini, max_depth=3, max_features=log2, n_estimators=100;, score=(train=0.943, test=0.944) total time=   0.2s\n",
      "[CV 2/3] END criterion=gini, max_depth=3, max_features=log2, n_estimators=100;, score=(train=0.941, test=0.949) total time=   0.2s\n",
      "[CV 3/3] END criterion=gini, max_depth=3, max_features=log2, n_estimators=100;, score=(train=0.947, test=0.927) total time=   0.2s\n",
      "[CV 1/3] END criterion=gini, max_depth=3, max_features=log2, n_estimators=1000;, score=(train=0.943, test=0.943) total time=   2.8s\n",
      "[CV 2/3] END criterion=gini, max_depth=3, max_features=log2, n_estimators=1000;, score=(train=0.941, test=0.950) total time=   3.0s\n",
      "[CV 3/3] END criterion=gini, max_depth=3, max_features=log2, n_estimators=1000;, score=(train=0.948, test=0.928) total time=   2.9s\n",
      "[CV 1/3] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=10;, score=(train=0.946, test=0.939) total time=   0.0s\n",
      "[CV 2/3] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=10;, score=(train=0.941, test=0.946) total time=   0.0s\n",
      "[CV 3/3] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=10;, score=(train=0.951, test=0.924) total time=   0.0s\n",
      "[CV 1/3] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=50;, score=(train=0.946, test=0.939) total time=   0.1s\n",
      "[CV 2/3] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=50;, score=(train=0.942, test=0.947) total time=   0.1s\n",
      "[CV 3/3] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=50;, score=(train=0.950, test=0.932) total time=   0.1s\n",
      "[CV 1/3] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=100;, score=(train=0.945, test=0.942) total time=   0.3s\n",
      "[CV 2/3] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=100;, score=(train=0.944, test=0.949) total time=   0.3s\n",
      "[CV 3/3] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=100;, score=(train=0.950, test=0.929) total time=   0.3s\n",
      "[CV 1/3] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=1000;, score=(train=0.947, test=0.942) total time=   3.4s\n",
      "[CV 2/3] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=1000;, score=(train=0.944, test=0.949) total time=   3.1s\n",
      "[CV 3/3] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=1000;, score=(train=0.951, test=0.932) total time=   3.2s\n",
      "[CV 1/3] END criterion=gini, max_depth=4, max_features=log2, n_estimators=10;, score=(train=0.947, test=0.938) total time=   0.0s\n",
      "[CV 2/3] END criterion=gini, max_depth=4, max_features=log2, n_estimators=10;, score=(train=0.940, test=0.949) total time=   0.0s\n",
      "[CV 3/3] END criterion=gini, max_depth=4, max_features=log2, n_estimators=10;, score=(train=0.948, test=0.930) total time=   0.0s\n",
      "[CV 1/3] END criterion=gini, max_depth=4, max_features=log2, n_estimators=50;, score=(train=0.946, test=0.942) total time=   0.1s\n",
      "[CV 2/3] END criterion=gini, max_depth=4, max_features=log2, n_estimators=50;, score=(train=0.945, test=0.947) total time=   0.1s\n",
      "[CV 3/3] END criterion=gini, max_depth=4, max_features=log2, n_estimators=50;, score=(train=0.951, test=0.933) total time=   0.1s\n",
      "[CV 1/3] END criterion=gini, max_depth=4, max_features=log2, n_estimators=100;, score=(train=0.948, test=0.943) total time=   0.3s\n",
      "[CV 2/3] END criterion=gini, max_depth=4, max_features=log2, n_estimators=100;, score=(train=0.944, test=0.950) total time=   0.3s\n",
      "[CV 3/3] END criterion=gini, max_depth=4, max_features=log2, n_estimators=100;, score=(train=0.951, test=0.931) total time=   0.2s\n",
      "[CV 1/3] END criterion=gini, max_depth=4, max_features=log2, n_estimators=1000;, score=(train=0.946, test=0.942) total time=   3.2s\n",
      "[CV 2/3] END criterion=gini, max_depth=4, max_features=log2, n_estimators=1000;, score=(train=0.944, test=0.949) total time=   3.4s\n",
      "[CV 3/3] END criterion=gini, max_depth=4, max_features=log2, n_estimators=1000;, score=(train=0.951, test=0.930) total time=   3.6s\n",
      "[CV 1/3] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=10;, score=(train=0.951, test=0.936) total time=   0.0s\n",
      "[CV 2/3] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=10;, score=(train=0.948, test=0.943) total time=   0.0s\n",
      "[CV 3/3] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=10;, score=(train=0.956, test=0.929) total time=   0.0s\n",
      "[CV 1/3] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=50;, score=(train=0.952, test=0.942) total time=   0.1s\n",
      "[CV 2/3] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=50;, score=(train=0.949, test=0.945) total time=   0.1s\n",
      "[CV 3/3] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=50;, score=(train=0.956, test=0.930) total time=   0.1s\n",
      "[CV 1/3] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=100;, score=(train=0.953, test=0.940) total time=   0.3s\n",
      "[CV 2/3] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=100;, score=(train=0.946, test=0.947) total time=   0.2s\n",
      "[CV 3/3] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=100;, score=(train=0.954, test=0.929) total time=   0.3s\n",
      "[CV 1/3] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=1000;, score=(train=0.951, test=0.942) total time=   3.9s\n",
      "[CV 2/3] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=1000;, score=(train=0.948, test=0.948) total time=   3.5s\n",
      "[CV 3/3] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=1000;, score=(train=0.954, test=0.930) total time=   3.7s\n",
      "[CV 1/3] END criterion=gini, max_depth=5, max_features=log2, n_estimators=10;, score=(train=0.951, test=0.938) total time=   0.0s\n",
      "[CV 2/3] END criterion=gini, max_depth=5, max_features=log2, n_estimators=10;, score=(train=0.946, test=0.946) total time=   0.0s\n",
      "[CV 3/3] END criterion=gini, max_depth=5, max_features=log2, n_estimators=10;, score=(train=0.954, test=0.929) total time=   0.0s\n",
      "[CV 1/3] END criterion=gini, max_depth=5, max_features=log2, n_estimators=50;, score=(train=0.951, test=0.940) total time=   0.1s\n",
      "[CV 2/3] END criterion=gini, max_depth=5, max_features=log2, n_estimators=50;, score=(train=0.947, test=0.946) total time=   0.1s\n",
      "[CV 3/3] END criterion=gini, max_depth=5, max_features=log2, n_estimators=50;, score=(train=0.954, test=0.932) total time=   0.1s\n",
      "[CV 1/3] END criterion=gini, max_depth=5, max_features=log2, n_estimators=100;, score=(train=0.951, test=0.940) total time=   0.3s\n",
      "[CV 2/3] END criterion=gini, max_depth=5, max_features=log2, n_estimators=100;, score=(train=0.947, test=0.949) total time=   0.3s\n",
      "[CV 3/3] END criterion=gini, max_depth=5, max_features=log2, n_estimators=100;, score=(train=0.955, test=0.932) total time=   0.3s\n",
      "[CV 1/3] END criterion=gini, max_depth=5, max_features=log2, n_estimators=1000;, score=(train=0.952, test=0.940) total time=   3.6s\n",
      "[CV 2/3] END criterion=gini, max_depth=5, max_features=log2, n_estimators=1000;, score=(train=0.948, test=0.948) total time=   3.4s\n",
      "[CV 3/3] END criterion=gini, max_depth=5, max_features=log2, n_estimators=1000;, score=(train=0.955, test=0.932) total time=   3.7s\n",
      "[CV 1/3] END criterion=entropy, max_depth=3, max_features=sqrt, n_estimators=10;, score=(train=0.941, test=0.941) total time=   0.0s\n",
      "[CV 2/3] END criterion=entropy, max_depth=3, max_features=sqrt, n_estimators=10;, score=(train=0.935, test=0.944) total time=   0.0s\n",
      "[CV 3/3] END criterion=entropy, max_depth=3, max_features=sqrt, n_estimators=10;, score=(train=0.943, test=0.927) total time=   0.0s\n",
      "[CV 1/3] END criterion=entropy, max_depth=3, max_features=sqrt, n_estimators=50;, score=(train=0.942, test=0.942) total time=   0.1s\n",
      "[CV 2/3] END criterion=entropy, max_depth=3, max_features=sqrt, n_estimators=50;, score=(train=0.939, test=0.949) total time=   0.1s\n",
      "[CV 3/3] END criterion=entropy, max_depth=3, max_features=sqrt, n_estimators=50;, score=(train=0.945, test=0.925) total time=   0.1s\n",
      "[CV 1/3] END criterion=entropy, max_depth=3, max_features=sqrt, n_estimators=100;, score=(train=0.944, test=0.944) total time=   0.3s\n",
      "[CV 2/3] END criterion=entropy, max_depth=3, max_features=sqrt, n_estimators=100;, score=(train=0.939, test=0.949) total time=   0.3s\n",
      "[CV 3/3] END criterion=entropy, max_depth=3, max_features=sqrt, n_estimators=100;, score=(train=0.945, test=0.932) total time=   0.3s\n",
      "[CV 1/3] END criterion=entropy, max_depth=3, max_features=sqrt, n_estimators=1000;, score=(train=0.944, test=0.943) total time=   3.6s\n",
      "[CV 2/3] END criterion=entropy, max_depth=3, max_features=sqrt, n_estimators=1000;, score=(train=0.941, test=0.949) total time=   3.2s\n",
      "[CV 3/3] END criterion=entropy, max_depth=3, max_features=sqrt, n_estimators=1000;, score=(train=0.946, test=0.928) total time=   2.6s\n",
      "[CV 1/3] END criterion=entropy, max_depth=3, max_features=log2, n_estimators=10;, score=(train=0.939, test=0.939) total time=   0.0s\n",
      "[CV 2/3] END criterion=entropy, max_depth=3, max_features=log2, n_estimators=10;, score=(train=0.930, test=0.940) total time=   0.0s\n",
      "[CV 3/3] END criterion=entropy, max_depth=3, max_features=log2, n_estimators=10;, score=(train=0.947, test=0.931) total time=   0.0s\n",
      "[CV 1/3] END criterion=entropy, max_depth=3, max_features=log2, n_estimators=50;, score=(train=0.942, test=0.942) total time=   0.0s\n",
      "[CV 2/3] END criterion=entropy, max_depth=3, max_features=log2, n_estimators=50;, score=(train=0.941, test=0.947) total time=   0.0s\n",
      "[CV 3/3] END criterion=entropy, max_depth=3, max_features=log2, n_estimators=50;, score=(train=0.947, test=0.928) total time=   0.0s\n",
      "[CV 1/3] END criterion=entropy, max_depth=3, max_features=log2, n_estimators=100;, score=(train=0.943, test=0.942) total time=   0.2s\n",
      "[CV 2/3] END criterion=entropy, max_depth=3, max_features=log2, n_estimators=100;, score=(train=0.939, test=0.948) total time=   0.2s\n",
      "[CV 3/3] END criterion=entropy, max_depth=3, max_features=log2, n_estimators=100;, score=(train=0.945, test=0.931) total time=   0.2s\n",
      "[CV 1/3] END criterion=entropy, max_depth=3, max_features=log2, n_estimators=1000;, score=(train=0.944, test=0.943) total time=   2.8s\n",
      "[CV 2/3] END criterion=entropy, max_depth=3, max_features=log2, n_estimators=1000;, score=(train=0.941, test=0.949) total time=   2.7s\n",
      "[CV 3/3] END criterion=entropy, max_depth=3, max_features=log2, n_estimators=1000;, score=(train=0.946, test=0.929) total time=   2.7s\n",
      "[CV 1/3] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=10;, score=(train=0.949, test=0.939) total time=   0.0s\n",
      "[CV 2/3] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=10;, score=(train=0.942, test=0.948) total time=   0.0s\n",
      "[CV 3/3] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=10;, score=(train=0.948, test=0.932) total time=   0.0s\n",
      "[CV 1/3] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=50;, score=(train=0.946, test=0.942) total time=   0.1s\n",
      "[CV 2/3] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=50;, score=(train=0.944, test=0.948) total time=   0.1s\n",
      "[CV 3/3] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=50;, score=(train=0.948, test=0.930) total time=   0.1s\n",
      "[CV 1/3] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=100;, score=(train=0.946, test=0.944) total time=   0.3s\n",
      "[CV 2/3] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=100;, score=(train=0.943, test=0.947) total time=   0.3s\n",
      "[CV 3/3] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=100;, score=(train=0.949, test=0.934) total time=   0.3s\n",
      "[CV 1/3] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=1000;, score=(train=0.946, test=0.942) total time=   3.2s\n",
      "[CV 2/3] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=1000;, score=(train=0.943, test=0.949) total time=   3.4s\n",
      "[CV 3/3] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=1000;, score=(train=0.950, test=0.929) total time=   3.3s\n",
      "[CV 1/3] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=10;, score=(train=0.944, test=0.936) total time=   0.0s\n",
      "[CV 2/3] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=10;, score=(train=0.940, test=0.949) total time=   0.0s\n",
      "[CV 3/3] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=10;, score=(train=0.946, test=0.925) total time=   0.0s\n",
      "[CV 1/3] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=50;, score=(train=0.945, test=0.943) total time=   0.1s\n",
      "[CV 2/3] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=50;, score=(train=0.944, test=0.949) total time=   0.1s\n",
      "[CV 3/3] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=50;, score=(train=0.948, test=0.930) total time=   0.1s\n",
      "[CV 1/3] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=100;, score=(train=0.945, test=0.944) total time=   0.2s\n",
      "[CV 2/3] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=100;, score=(train=0.944, test=0.947) total time=   0.2s\n",
      "[CV 3/3] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=100;, score=(train=0.948, test=0.928) total time=   0.2s\n",
      "[CV 1/3] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=1000;, score=(train=0.945, test=0.943) total time=   3.1s\n",
      "[CV 2/3] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=1000;, score=(train=0.944, test=0.949) total time=   3.4s\n",
      "[CV 3/3] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=1000;, score=(train=0.950, test=0.930) total time=   3.7s\n",
      "[CV 1/3] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=10;, score=(train=0.947, test=0.937) total time=   0.0s\n",
      "[CV 2/3] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=10;, score=(train=0.948, test=0.950) total time=   0.0s\n",
      "[CV 3/3] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=10;, score=(train=0.954, test=0.927) total time=   0.0s\n",
      "[CV 1/3] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=50;, score=(train=0.950, test=0.942) total time=   0.1s\n",
      "[CV 2/3] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=50;, score=(train=0.946, test=0.948) total time=   0.1s\n",
      "[CV 3/3] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=50;, score=(train=0.953, test=0.929) total time=   0.1s\n",
      "[CV 1/3] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=100;, score=(train=0.950, test=0.938) total time=   0.3s\n",
      "[CV 2/3] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=100;, score=(train=0.947, test=0.948) total time=   0.3s\n",
      "[CV 3/3] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=100;, score=(train=0.952, test=0.932) total time=   0.3s\n",
      "[CV 1/3] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=1000;, score=(train=0.951, test=0.940) total time=   3.7s\n",
      "[CV 2/3] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=1000;, score=(train=0.947, test=0.947) total time=   3.9s\n",
      "[CV 3/3] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=1000;, score=(train=0.954, test=0.931) total time=   3.7s\n",
      "[CV 1/3] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=10;, score=(train=0.948, test=0.935) total time=   0.0s\n",
      "[CV 2/3] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=10;, score=(train=0.947, test=0.949) total time=   0.0s\n",
      "[CV 3/3] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=10;, score=(train=0.953, test=0.930) total time=   0.0s\n",
      "[CV 1/3] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=50;, score=(train=0.950, test=0.943) total time=   0.1s\n",
      "[CV 2/3] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=50;, score=(train=0.947, test=0.946) total time=   0.1s\n",
      "[CV 3/3] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=50;, score=(train=0.954, test=0.930) total time=   0.1s\n",
      "[CV 1/3] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=100;, score=(train=0.949, test=0.941) total time=   0.3s\n",
      "[CV 2/3] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=100;, score=(train=0.947, test=0.948) total time=   0.3s\n",
      "[CV 3/3] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=100;, score=(train=0.955, test=0.930) total time=   0.3s\n",
      "[CV 1/3] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=1000;, score=(train=0.950, test=0.941) total time=   4.1s\n",
      "[CV 2/3] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=1000;, score=(train=0.947, test=0.948) total time=   4.9s\n",
      "[CV 3/3] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=1000;, score=(train=0.955, test=0.932) total time=   3.5s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=3, max_features=sqrt, n_estimators=10;, score=(train=0.939, test=0.939) total time=   0.0s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=3, max_features=sqrt, n_estimators=10;, score=(train=0.936, test=0.943) total time=   0.0s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=3, max_features=sqrt, n_estimators=10;, score=(train=0.946, test=0.923) total time=   0.0s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=3, max_features=sqrt, n_estimators=50;, score=(train=0.942, test=0.939) total time=   0.0s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=3, max_features=sqrt, n_estimators=50;, score=(train=0.940, test=0.946) total time=   0.1s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=3, max_features=sqrt, n_estimators=50;, score=(train=0.945, test=0.930) total time=   0.1s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=3, max_features=sqrt, n_estimators=100;, score=(train=0.940, test=0.942) total time=   0.2s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=3, max_features=sqrt, n_estimators=100;, score=(train=0.942, test=0.947) total time=   0.2s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=3, max_features=sqrt, n_estimators=100;, score=(train=0.946, test=0.929) total time=   0.2s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=3, max_features=sqrt, n_estimators=1000;, score=(train=0.943, test=0.942) total time=   3.2s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=3, max_features=sqrt, n_estimators=1000;, score=(train=0.940, test=0.949) total time=   2.8s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=3, max_features=sqrt, n_estimators=1000;, score=(train=0.946, test=0.929) total time=   2.8s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=3, max_features=log2, n_estimators=10;, score=(train=0.941, test=0.938) total time=   0.0s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=3, max_features=log2, n_estimators=10;, score=(train=0.940, test=0.951) total time=   0.0s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=3, max_features=log2, n_estimators=10;, score=(train=0.945, test=0.922) total time=   0.0s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=3, max_features=log2, n_estimators=50;, score=(train=0.941, test=0.938) total time=   0.0s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=3, max_features=log2, n_estimators=50;, score=(train=0.939, test=0.950) total time=   0.0s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=3, max_features=log2, n_estimators=50;, score=(train=0.945, test=0.927) total time=   0.1s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=3, max_features=log2, n_estimators=100;, score=(train=0.943, test=0.944) total time=   0.2s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=3, max_features=log2, n_estimators=100;, score=(train=0.939, test=0.949) total time=   0.2s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=3, max_features=log2, n_estimators=100;, score=(train=0.945, test=0.930) total time=   0.2s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=3, max_features=log2, n_estimators=1000;, score=(train=0.943, test=0.942) total time=   2.8s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=3, max_features=log2, n_estimators=1000;, score=(train=0.940, test=0.949) total time=   2.9s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=3, max_features=log2, n_estimators=1000;, score=(train=0.946, test=0.930) total time=   2.7s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=4, max_features=sqrt, n_estimators=10;, score=(train=0.942, test=0.941) total time=   0.0s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=4, max_features=sqrt, n_estimators=10;, score=(train=0.940, test=0.947) total time=   0.0s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=4, max_features=sqrt, n_estimators=10;, score=(train=0.949, test=0.922) total time=   0.0s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=4, max_features=sqrt, n_estimators=50;, score=(train=0.946, test=0.940) total time=   0.1s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=4, max_features=sqrt, n_estimators=50;, score=(train=0.944, test=0.948) total time=   0.1s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=4, max_features=sqrt, n_estimators=50;, score=(train=0.949, test=0.933) total time=   0.1s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=4, max_features=sqrt, n_estimators=100;, score=(train=0.946, test=0.940) total time=   0.2s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=4, max_features=sqrt, n_estimators=100;, score=(train=0.943, test=0.948) total time=   0.2s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=4, max_features=sqrt, n_estimators=100;, score=(train=0.948, test=0.928) total time=   0.2s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=4, max_features=sqrt, n_estimators=1000;, score=(train=0.946, test=0.943) total time=   3.2s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=4, max_features=sqrt, n_estimators=1000;, score=(train=0.944, test=0.949) total time=   3.5s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=4, max_features=sqrt, n_estimators=1000;, score=(train=0.950, test=0.931) total time=   3.4s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=4, max_features=log2, n_estimators=10;, score=(train=0.942, test=0.938) total time=   0.0s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=4, max_features=log2, n_estimators=10;, score=(train=0.940, test=0.948) total time=   0.0s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=4, max_features=log2, n_estimators=10;, score=(train=0.949, test=0.925) total time=   0.0s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=4, max_features=log2, n_estimators=50;, score=(train=0.945, test=0.945) total time=   0.1s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=4, max_features=log2, n_estimators=50;, score=(train=0.942, test=0.949) total time=   0.1s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=4, max_features=log2, n_estimators=50;, score=(train=0.950, test=0.927) total time=   0.1s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=4, max_features=log2, n_estimators=100;, score=(train=0.946, test=0.942) total time=   0.3s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=4, max_features=log2, n_estimators=100;, score=(train=0.944, test=0.947) total time=   0.3s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=4, max_features=log2, n_estimators=100;, score=(train=0.949, test=0.928) total time=   0.3s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=4, max_features=log2, n_estimators=1000;, score=(train=0.945, test=0.943) total time=   3.3s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=4, max_features=log2, n_estimators=1000;, score=(train=0.942, test=0.949) total time=   3.1s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=4, max_features=log2, n_estimators=1000;, score=(train=0.950, test=0.930) total time=   3.2s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=5, max_features=sqrt, n_estimators=10;, score=(train=0.948, test=0.939) total time=   0.0s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=5, max_features=sqrt, n_estimators=10;, score=(train=0.944, test=0.944) total time=   0.0s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=5, max_features=sqrt, n_estimators=10;, score=(train=0.951, test=0.927) total time=   0.0s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=5, max_features=sqrt, n_estimators=50;, score=(train=0.950, test=0.941) total time=   0.1s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=5, max_features=sqrt, n_estimators=50;, score=(train=0.947, test=0.946) total time=   0.1s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=5, max_features=sqrt, n_estimators=50;, score=(train=0.953, test=0.930) total time=   0.1s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=5, max_features=sqrt, n_estimators=100;, score=(train=0.950, test=0.942) total time=   0.3s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=5, max_features=sqrt, n_estimators=100;, score=(train=0.946, test=0.949) total time=   0.3s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=5, max_features=sqrt, n_estimators=100;, score=(train=0.954, test=0.927) total time=   0.3s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=5, max_features=sqrt, n_estimators=1000;, score=(train=0.950, test=0.942) total time=   3.9s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=5, max_features=sqrt, n_estimators=1000;, score=(train=0.947, test=0.946) total time=   3.8s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=5, max_features=sqrt, n_estimators=1000;, score=(train=0.954, test=0.931) total time=   3.6s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=5, max_features=log2, n_estimators=10;, score=(train=0.948, test=0.937) total time=   0.0s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=5, max_features=log2, n_estimators=10;, score=(train=0.948, test=0.949) total time=   0.0s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=5, max_features=log2, n_estimators=10;, score=(train=0.952, test=0.926) total time=   0.0s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=5, max_features=log2, n_estimators=50;, score=(train=0.950, test=0.938) total time=   0.1s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=5, max_features=log2, n_estimators=50;, score=(train=0.947, test=0.949) total time=   0.1s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=5, max_features=log2, n_estimators=50;, score=(train=0.955, test=0.931) total time=   0.1s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=5, max_features=log2, n_estimators=100;, score=(train=0.951, test=0.942) total time=   0.4s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=5, max_features=log2, n_estimators=100;, score=(train=0.949, test=0.949) total time=   0.3s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=5, max_features=log2, n_estimators=100;, score=(train=0.953, test=0.932) total time=   0.3s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=5, max_features=log2, n_estimators=1000;, score=(train=0.950, test=0.942) total time=   4.1s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=5, max_features=log2, n_estimators=1000;, score=(train=0.947, test=0.948) total time=   3.9s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=5, max_features=log2, n_estimators=1000;, score=(train=0.954, test=0.932) total time=   3.6s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "model = RandomForestClassifier()\n",
    "params = {\n",
    "    'n_estimators': [10,50,100,1000],\n",
    "    'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "    'max_depth': [3,4,5],\n",
    "    'max_features':['sqrt', 'log2'],\n",
    "}\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=params, cv=3, verbose=5, return_train_score=True,refit=True)\n",
    "grid_model = grid_search.fit(X_train_meta,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_test = grid_model.predict(X_test_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'entropy',\n",
       " 'max_depth': 3,\n",
       " 'max_features': 'sqrt',\n",
       " 'n_estimators': 100}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9415213385079091"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_model.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay,multilabel_confusion_matrix,f1_score,precision_score,accuracy_score,recall_score,precision_recall_fscore_support\n",
    "def evaluation_test(y,y_pred):\n",
    "    cm = confusion_matrix(y,y_pred)\n",
    "    disp = ConfusionMatrixDisplay(cm,display_labels=['AFIB','SB','SR','GSVT'])\n",
    "    disp.plot()\n",
    "    plt.show()\n",
    "    n_classes = len(cm)\n",
    "    result = []\n",
    "    for c in range(n_classes):\n",
    "        tp = cm[c,c]\n",
    "        fp = sum(cm[:,c]) - cm[c,c]\n",
    "        fn = sum(cm[c,:]) - cm[c,c]\n",
    "        tn = sum(np.delete(sum(cm)-cm[c,:],c))\n",
    "        acc = (tp+tn) / (tp+fn+tn+fp)\n",
    "        recall = tp/(tp+fn)\n",
    "        precision = tp/(tp+fp)\n",
    "        specificity = tn/(tn+fp)\n",
    "        f1_score = 2*((precision*recall)/(precision+recall))\n",
    "        if c+1 == 1:\n",
    "            Rhythm = 'AFIB'\n",
    "        elif c+1 == 2:\n",
    "            Rhythm = 'SB'\n",
    "        elif c+1 == 3:\n",
    "            Rhythm = 'SR'\n",
    "        else:\n",
    "            Rhythm = 'GSVT'\n",
    "        result.append([Rhythm,acc,recall,precision,f1_score,specificity])\n",
    "    p_macro,r_macro,f_macro,support_macro = precision_recall_fscore_support(y,y_pred,average='macro')\n",
    "    p_micro,r_micro,f_micro,support_micro = precision_recall_fscore_support(y,y_pred,average='micro')\n",
    "    p_weighted,r_weighted,f_weighted,support_weighted = precision_recall_fscore_support(y,y_pred,average='weighted')\n",
    "    result.append(['macro avg',None,f_macro,p_macro,r_macro,None])\n",
    "    result.append(['micro avg',None,f_micro,p_micro,r_micro,None])\n",
    "    result.append(['weighted avg',None,f_weighted,p_weighted,r_weighted,None])\n",
    "    return result\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhgAAAGwCAYAAADrIxwOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAABZC0lEQVR4nO3deVxU5f4H8M8wAzPsO4wIIgqiCJpbbt00NzRN+9l1uVppoVmaRkmWWkrehLTrUlmapkKaaZtm3jKX0jLzqijuYikiKCMuyM6s5/cHOTqCxjhnGIb5vF+v86p5znOe+c4Iw3e+z3POkQiCIICIiIhIRE62DoCIiIgaHiYYREREJDomGERERCQ6JhhEREQkOiYYREREJDomGERERCQ6JhhEREQkOpmtA7A3BoMBly5dgqenJyQSia3DISIiMwmCgJKSEoSEhMDJyTrfsysrK6HRaEQZy8XFBQqFQpSx6hITDDNdunQJYWFhtg6DiIgslJubi9DQUNHHraysRES4B1QFelHGUyqVyM7OtrskgwmGmTw9PQEAISkz4GRn/9j2KvKVDFuH4HicpLaOwKFIPdxsHYJD0Qka7C75wvh5LjaNRgNVgR45GU3h5WlZhaS4xIDwDueh0WiYYDR0N6dFnBQKOLna1z+2vZJJnG0dguORMMGoS1KJi61DcEjWnub28JTAw9Oy5zDAfqfimWAQERFZgV4wQG/h3b70gkGcYGyACQYREZEVGCDAAMsyDEuPtyWepkpERESiYwWDiIjICgwwwNIJDstHsB0mGERERFagFwToBcumOCw93pY4RUJERESiYwWDiIjIChx9kScTDCIiIiswQIDegRMMTpEQERGR6FjBICIisgJOkRAREZHoeBYJERERkchYwSAiIrICw1+bpWPYKyYYREREVqAX4SwSS4+3JSYYREREVqAXIMLdVMWJxRa4BoOIiIhExwoGERGRFXANBhEREYnOAAn0kFg8hr3iFAkRERGJjhUMIiIiKzAIVZulY9grJhhERERWoBdhisTS422JUyREREQkOlYwiIiIrMDRKxhMMIiIiKzAIEhgECw8i8TC422JUyREREQkOlYwiIiIrMDRp0hYwSAiIrICPZxE2czRtGlTSCSSatukSZMAAIIgIDk5GSEhIXB1dUXPnj1x4sQJkzHUajUmT56MgIAAuLu7Y/DgwcjLyzP79TPBICIisgLhrzUYlmyCmWswDhw4gPz8fOO2fft2AMCwYcMAAPPnz8fChQuxZMkSHDhwAEqlEn379kVJSYlxjMTERGzcuBHr16/Hnj17UFpaikGDBkGv15sVC6dIiIiI6rni4mKTx3K5HHK5vFq/wMBAk8fvvPMOmjdvjh49ekAQBCxevBgzZ87E0KFDAQDp6ekIDg7GunXrMGHCBBQVFWHlypVYs2YN+vTpAwBYu3YtwsLCsGPHDsTHx9c6ZlYwiIiIrODmGgxLNwAICwuDt7e3cUtNTf3b59doNFi7di2effZZSCQSZGdnQ6VSoV+/fsY+crkcPXr0wN69ewEAGRkZ0Gq1Jn1CQkIQGxtr7FNbrGAQERFZgV5wgl6w7Hu8/q9Lhefm5sLLy8vYXlP14k6bNm3CjRs3MHbsWACASqUCAAQHB5v0Cw4ORk5OjrGPi4sLfH19q/W5eXxtMcEgIiKq57y8vEwSjNpYuXIlBgwYgJCQEJN2icR0XYcgCNXa7lSbPnfiFAkREZEVGCCBAU4Wbvd3mmpOTg527NiBcePGGduUSiUAVKtEFBQUGKsaSqUSGo0GhYWFd+1TW0wwiIiIrEDMNRjmWr16NYKCgjBw4EBjW0REBJRKpfHMEqBqncbu3bvRrVs3AECHDh3g7Oxs0ic/Px/Hjx839qktTpEQERE1IAaDAatXr8aYMWMgk936My+RSJCYmIiUlBRERUUhKioKKSkpcHNzw6hRowAA3t7eSEhIwNSpU+Hv7w8/Pz8kJSUhLi7OeFZJbTHBICIisgJxFnkKZh+zY8cOXLhwAc8++2y1fdOmTUNFRQUmTpyIwsJCdO7cGdu2bYOnp6exz6JFiyCTyTB8+HBUVFSgd+/eSEtLg1QqNSsOiSDcR/QOrLi4GN7e3ghdOAdOrgpbh+MQWkzcb+sQHI+TeR8kZBmph7utQ3AoOkGDncVrUVRUZPbCydq4+Xfi6yMt4O5p2e9SWYkeT7Q9Y7VYrYlrMIiIiEh0nCJpIHy3XkLg5jwUPhKMK8PCqxoFAf7/vQjv367AqVyHyqYeKBgRDk2Im8mxinMlCNicB8X5MghSCdShbrg4KRqCC/NPc4148TK6P1qEsEg1NJVOOHnQDSvnNkLeWVa7rMVfqUHCjIvo9EgxXBQGXDynwMKkcPx5zO3vD6Z7enTkJQz8Vz6CG6sBADl/uuHzD5vg4K9+AIBufa9iwIh8RLYuhbevDi8+3g7nTnvYMuR6xXAf9xKpPob9TjIwwWgA5OdL4fNbAdSNXU3afbfnw+cnFS4/1QyaYAX8friE0A+ykD27DQRFVdlOca4EjZecwfX4RigYHg5B5gR5Xjns+AZ+NtWmaxm+SwvAmUw3SGUCxr6Wj5TPz2F8j2ioKzjtIDYPbx0WbjyDo3s98MZTkbhxVYZG4WqUFfO9FsPVy3KsXhCB/AtVCXLvxwvw5ocnMXloO1z40x0KVz1OHvLCnq2BeOntP2wcbf1jqzUY9UW9/oq6d+9eSKVS9O/f36T9/PnzNd4t7sknnzTZn5mZWWN/FxcXREZG4u2334a9L0GRVOrRKO0sLo+OgN7ttnxREOD702Vc7x+C0nZ+0IS44fLTzSDRGOB14JqxW+BXF3DjkWAUxodAE+IGbZACpe39IDjX6x+Nemvm6GbY/oUfcs4ocO6kKxa83ATBoVpEtamwdWgN0vCJl3H1kjMWTG2KrEx3XM6TI/M3L+Tn/P1VDunv7f/ZHwd/8cPF8264eN4Nny5uispyKVq2rbox1k+bg/H5R+E4/LuPbQOtpyy/BkbVZq/qdQVj1apVmDx5Mj755BNcuHABTZo0Mdm/Y8cOtG7d2vjY1dX1ziFq7K9Wq7Fnzx6MGzcOjRo1QkJCglXirwtBG86jLNYH5S294ffDJWO78zU1ZMValLfyNrYJzk6oiPKE4lwJiv4RBGmJFq7ny1DSyR9h756E89VKaIJdcXVwKCojPWt6OjKTu1fV3QdLbvAbtTV06VuEjN1emLnsHNp0KcVVlTO2fBqIH9YF2Dq0BsfJScBD/a9A4abHqUx+PtDfq7cJRllZGb744gscOHAAKpUKaWlpmDVrlkkff39/45XJauP2/uHh4Vi1ahUOHTp0zwRDrVZDrVYbH995Rztb8jx4DYrcclx4rXW1fdIiLQBA5+ls0q7zdIbz9arX43y16r/+31/ElaFNoA51g9f/riL0/dPIeSMO2iCuG7CMgOeSL+H4/9yRk3Xv5JfuT6Mmagx66gq+WRGE9R8oEf1AGV6YkwutWoIdX/vbOrwGoWmLMiz4PBMucgMqyqX494sxyD3Ls15qQy9IoDfzdus1jWGv6m3tZcOGDYiOjkZ0dDSefPJJrF69WtTpjIMHD+LQoUPo3LnzPfulpqaa3MEuLCxMtBgsIbuuRuCXOcgf2/ze0xmSGh7evJ68oer9vPFQEIq7BkId5o4r/wyHNkgB771XrBK3I5mUchERrSqQOrHJ33em+yJxAv487obV8xrj7Ak3fP9ZVfVi4NNXbR1ag5GX7YoX/689Xhn5AL5f3whT38lCWPMyW4dlF/R/LfK0dLNX9TbylStXGtdU9O/fH6Wlpdi5c6dJn27dusHDw8O4HT58+J5j3uzv4uKCTp06Yfjw4Xj66afvecz06dNRVFRk3HJzcy17YSKRXyiHrESH8HeOI+rF/Yh6cT/c/iiBz67LiHpxP/ReVZULWbHW5DhpiRY6z6rClc7bBQCgUZp+u9YoXSEr1NTBq2i4Jr6dh679ijHtn81xNd/F1uE0WNcLnJHzh2mlLfcPBYIa8+dXLDqtE/IvuOKP455IWxiBc6c9MOTpS39/IDm8ejlFkpWVhf379+Obb74BAMhkMowYMQKrVq0yuVTphg0b0KpVK+Pjv6su3Oyv1Wpx7NgxTJkyBb6+vnjnnXfueoxcLq/VbXHrWnlLL5x/I9akTflpNjRKBa73awRtgBw6L2e4nSqGOuyvcqbOANc/SnD18ar3SefvAp23M1wKKk3GcS6oRFlrb9D9EDBp7kV061+EV/8Zicu59e9npyE5edAdYc1Mf34bN1OjII9JnbVIJAKcXQy2DsMuGAQnGCw8i8Rgxyci1MsEY+XKldDpdGjcuLGxTRAEODs7m9zhLSwsDJGRkbUe9/b+rVq1wrlz5/Dmm28iOTkZCoV9rTcQFNJq17MwyJ2gd5cZ2wt7BcPvx0vQBsmhCVLAb+slCC5OKO7019y0RILrfRvBf8tFqBu7GddguFyuQP742r+vdMuLKRfxyP8VIvmZCFSUOsE3sKqCVFYihaay3hYM7dY3K4KwaFMWRr6owi9bfBD9QDkeHX0Vi1/jtJQYxrx8Hgd/8cUVlRxu7no8/OgVxD1YhFnjq77ceHhrEdRIDb+gqopRaETV2VKFV11QeJVJnhhTHHpeB0M8Op0On376KRYsWIB+/fqZ7HviiSfw2WefYdCgQaI8l1QqhU6ng0ajsbsEozYK+zaCk8aAoPU5xgtt5U2ONl4DAwBu9FJCojUg8KsLkJbroG7shrzJLaENbHjvR114bGzVKcD/+easSft/EsOw/Qs/W4TUoJ054o4545rjmekXMToxH6pcFyxLDsXPG/lei8HHX4Ok+VnwC9SgrESG7Cx3zBofi8N7fQEAXXpdxyupZ4z9X190GgDw2ZIm+GxJuE1ipvqj3iUYW7ZsQWFhIRISEuDtbVqm/+c//4mVK1fed4Jx7do1qFQq6HQ6HDt2DO+99x4eeeQRu7u++93kvdzKtEEiwbVBobg2KPSexxXGh6AwPsSKkTmO+JC2tg7B4fxvpzf+t5NTetbw3hst7rl/x8Zg7NgYXEfR2B8DLD8LxJ4no+pdgrFy5Ur06dOnWnIBVFUwUlJScP369fsa++b6DalUikaNGuHRRx/F3LlzLYqXiIioJmJcKIsX2hLRd999d9d97du3N56qeq9TVps2bWqy/87HREREZF31LsEgIiJqCMS5FwkrGERERHQbAyQwWHjnSEuPtyUmGERERFbg6BUM+42ciIiI6i1WMIiIiKxAnAtt2W8dgAkGERGRFRgECQyWXgeDd1MlIiIiuoUVDCIiIiswiDBFwgttERERkQlx7qZqvwmG/UZORERE9RYrGERERFaghwR6Cy+UZenxtsQEg4iIyAo4RUJEREQkMlYwiIiIrEAPy6c49OKEYhNMMIiIiKzA0adImGAQERFZAW92RkRERCQyVjCIiIisQIAEBgvXYAg8TZWIiIhuxykSIiIiIpGxgkFERGQFjn67diYYREREVqAX4W6qlh5vS/YbOREREdVbTDCIiIis4OYUiaWbuS5evIgnn3wS/v7+cHNzwwMPPICMjAzjfkEQkJycjJCQELi6uqJnz544ceKEyRhqtRqTJ09GQEAA3N3dMXjwYOTl5ZkVBxMMIiIiKzDASZTNHIWFhejevTucnZ3xww8/4OTJk1iwYAF8fHyMfebPn4+FCxdiyZIlOHDgAJRKJfr27YuSkhJjn8TERGzcuBHr16/Hnj17UFpaikGDBkGvr/3Fy7kGg4iIqIGYN28ewsLCsHr1amNb06ZNjf8vCAIWL16MmTNnYujQoQCA9PR0BAcHY926dZgwYQKKioqwcuVKrFmzBn369AEArF27FmFhYdixYwfi4+NrFQsrGERERFagFySibABQXFxssqnV6hqfc/PmzejYsSOGDRuGoKAgtGvXDitWrDDuz87OhkqlQr9+/YxtcrkcPXr0wN69ewEAGRkZ0Gq1Jn1CQkIQGxtr7FMbTDCIiIisQMw1GGFhYfD29jZuqampNT7nuXPnsHTpUkRFReHHH3/E888/jylTpuDTTz8FAKhUKgBAcHCwyXHBwcHGfSqVCi4uLvD19b1rn9rgFAkREZEVCCLcTVX46/jc3Fx4eXkZ2+VyeY39DQYDOnbsiJSUFABAu3btcOLECSxduhRPP/20sZ9EYrp4VBCEam3VY/n7PrdjBYOIiKie8/LyMtnulmA0atQIMTExJm2tWrXChQsXAABKpRIAqlUiCgoKjFUNpVIJjUaDwsLCu/apDSYYREREVqCHRJTNHN27d0dWVpZJ25kzZxAeHg4AiIiIgFKpxPbt2437NRoNdu/ejW7dugEAOnToAGdnZ5M++fn5OH78uLFPbXCKhIiIyAoMguWX+jYI5vV/+eWX0a1bN6SkpGD48OHYv38/li9fjuXLlwOomhpJTExESkoKoqKiEBUVhZSUFLi5uWHUqFEAAG9vbyQkJGDq1Knw9/eHn58fkpKSEBcXZzyrpDaYYBARETUQnTp1wsaNGzF9+nTMmTMHERERWLx4MUaPHm3sM23aNFRUVGDixIkoLCxE586dsW3bNnh6ehr7LFq0CDKZDMOHD0dFRQV69+6NtLQ0SKXSWsciEQTBzPzIsRUXF8Pb2xuhC+fAyVVh63AcQouJ+20dguNxqv2HCFlO6uFu6xAcik7QYGfxWhQVFZksnBTLzb8TY34eCRcPF4vG0pRqkP7IeqvFak2sYBAREVmBARIYzFxDUdMY9oqLPImIiEh0rGAQERFZwe1X4rRkDHvFBIOIiMgKDCJcaMvS422JCcZ9ikzKhEzibOswHMKPlzJtHYLDiW/cztYhOBR9cbGtQ3AoekFr6xAcAhMMIiIiKzBAYvl1MOx4kScTDCIiIisQRDiLRGCCQURERLe7/W6oloxhr+x39QgRERHVW6xgEBERWQHPIiEiIiLRcYqEiIiISGSsYBAREVmBo9+LhAkGERGRFXCKhIiIiEhkrGAQERFZgaNXMJhgEBERWYGjJxicIiEiIiLRsYJBRERkBY5ewWCCQUREZAUCLD/NVBAnFJtggkFERGQFjl7B4BoMIiIiEh0rGERERFbg6BUMJhhERERW4OgJBqdIiIiISHSsYBAREVmBo1cwmGAQERFZgSBIIFiYIFh6vC1xioSIiIhExwoGERGRFRggsfhCW5Yeb0tMMIiIiKzA0ddgcIqEiIiIRMcKBhERkRU4+iJPJhhERERW4OhTJEwwiIiIrMDRKxhcg0FERESiYwWDiIjICgQRpkjsuYLBBIOIiMgKBACCYPkY9opTJERERA1EcnIyJBKJyaZUKo37BUFAcnIyQkJC4Orqip49e+LEiRMmY6jVakyePBkBAQFwd3fH4MGDkZeXZ3YsTDCIiIis4OaVPC3dzNW6dWvk5+cbt2PHjhn3zZ8/HwsXLsSSJUtw4MABKJVK9O3bFyUlJcY+iYmJ2LhxI9avX489e/agtLQUgwYNgl6vNysOTpEQERFZga3OIpHJZCZVi1tjCVi8eDFmzpyJoUOHAgDS09MRHByMdevWYcKECSgqKsLKlSuxZs0a9OnTBwCwdu1ahIWFYceOHYiPj691HKxgEBER1XPFxcUmm1qtvmvfP/74AyEhIYiIiMDIkSNx7tw5AEB2djZUKhX69etn7CuXy9GjRw/s3bsXAJCRkQGtVmvSJyQkBLGxscY+tcUEg4iIyApuXmjL0g0AwsLC4O3tbdxSU1NrfM7OnTvj008/xY8//ogVK1ZApVKhW7duuHbtGlQqFQAgODjY5Jjg4GDjPpVKBRcXF/j6+t61T21xioSIiMgKBEGEs0j+Oj43NxdeXl7GdrlcXmP/AQMGGP8/Li4OXbt2RfPmzZGeno4uXboAACQS02kXQRCqtVWP4+/73IkVDCIionrOy8vLZLtbgnEnd3d3xMXF4Y8//jCuy7izElFQUGCsaiiVSmg0GhQWFt61T20xwSAiIrKCm4s8Ld0soVarcerUKTRq1AgRERFQKpXYvn27cb9Go8Hu3bvRrVs3AECHDh3g7Oxs0ic/Px/Hjx839qktTpEQERFZgS3OIklKSsJjjz2GJk2aoKCgAG+//TaKi4sxZswYSCQSJCYmIiUlBVFRUYiKikJKSgrc3NwwatQoAIC3tzcSEhIwdepU+Pv7w8/PD0lJSYiLizOeVVJbTDAauPTfj0MZpqnWvjktAB++0cQGEdmvpx+MweU8l2rtj425ghdTLwIALvwhx8q3Q3B0nwcEAxAeXYmZy84jKFQLVa4LxnSOqXHsmR9n4+HHiqwavyMY8eJlPDs9Hxs/CcCy2aG2DqdBGzTmKoa9cAV+QVrknFFg2awQHN/vYeuw6hWDIIGkju+mmpeXh3/961+4evUqAgMD0aVLF+zbtw/h4eEAgGnTpqGiogITJ05EYWEhOnfujG3btsHT09M4xqJFiyCTyTB8+HBUVFSgd+/eSEtLg1QqNSuWBpVgFBQU4M0338QPP/yAy5cvw9fXF23btkVycjK6du2Kpk2bIicnBwDg5OSE4OBgDBgwAP/5z3+qrZhtKKYMjIbTbT8TTaMr8M76P/Hrfxvm67Wm93/IgkF/65f9/GkFpo+MxD/+SgwunXfBK49Hof/Ia3gqSQV3Lz0u/KGAi6JqlVZgiAafZx43GfP7tf748qMgdOpVArJMi7bleHT0NZw7qbB1KA1ej8GFeP6tS1gyozFO7HfHwKeu4e3PsjG+ZzSuXKyehFPdWb9+/T33SyQSJCcnIzk5+a59FAoFPvjgA3zwwQcWxdKgEownnngCWq0W6enpaNasGS5fvoydO3fi+vXrxj5z5szB+PHjodfrcebMGTz33HOYMmUK1qxZY8PIrafourPJ4xGTVLh0Xo6jv/Obhrl8/E2vYrdhiTcaNVWjTddSAEDaO43wYK9ijHsz39inUfit6pFUCvgF6UzG2PuDN3oMvgFXd4MVI2/4FG56vLYkB4unheFfU8w7lY7MN/S5q/jxcz9sXecPAFg2uzE69CzBoKevYXVqIxtHV3+IeRaJPWowCcaNGzewZ88e7Nq1Cz169AAAhIeH48EHHzTp5+npaVxJ27hxYzz99NN/m/E1FDJnA3oNvY5vlgcD93H5WbpFq5Hgp699MXRCASQSwGAA9u/0wrCJBZjxr2b487grlE00GPliAboNqHnq44+jrjh7wg2TUsy/xj+ZejElD/t3euHwr55MMKxM5mxAVJtybFgSZNKesdsTMR3LbBRV/VSVYFi6BkOkYGygwZxF4uHhAQ8PD2zatOmeVzi73cWLF7FlyxZ07tz5rn3UanW1K6jZq27xRfDw0mPbl362DsXu7d3qjdJiKfoNr6qO3bgqQ0WZFBuWBKHjIyVI/fwcuvcvwpxxTXH0d/cax9j6uT+aRFWidafyugy9wekxuBCRsRVYxW/OdcLLTw+prOpn/nY3rsjge0eFjhxbg0kwZDIZ0tLSkJ6eDh8fH3Tv3h0zZszA0aNHTfq99tpr8PDwgKurK0JDQyGRSLBw4cK7jpuammpy9bSwsDBrvxSriR95FQd+9sL1y5wjtdSPn/uh0yPF8FdWfaAKf81wdI0vxtDnrqB5bAVGTC5A5z7F+O+nAdWOV1dI8PNGX8T/61pdht3gBIZo8MKci5g/JRxadYP5OLMLd36zlkhg3/cWt4L6cJqqLTWo38gnnngCly5dwubNmxEfH49du3ahffv2SEtLM/Z59dVXkZmZiaNHj2Lnzp0AgIEDB971LnHTp09HUVGRccvNza2LlyK6oMZqtPtHCbZ+Xv2PHZnncp4zDv/qif6jbiUHVd/qBIS3qDTpGxZViYKLzncOgV//6wN1hQR9hl2vto9qLzKuHL6BOiz5IQvf52Ti+5xMtO1WhiHPXsX3OZlwcuJfPLEVX5dCrwN8A02rFd4BOhReaTCz7qIQRNrsVYP7aVAoFOjbty/69u2LWbNmYdy4cZg9ezbGjh0LAAgICEBkZCQAICoqCosXL0bXrl3x888/13iOr1wur/UV0+qzfiOu4cZVGf6309vWodi9bev94ROgQ+c+t6bLnF0EtGhbjryzpj8rF8/JERSqrTbGj5/7o0u/4moLR8k8mXs88VyvaJO2qQsvIPesAl98GASDwX6//dVXOq0T/jjqhvYPl2Dv1lufJ+0fLsHvP/LzhW5pUBWMmsTExKCs7O4Lj26e11tRUVFXIdU5iURAv+HXseMrf5PTLMl8BgOwbYMf+gy7Dukd6fmwiQXYvdkH33/mh4vZLvh2VQD2bffGY2OumvS7mO2CY/vcTSogdH8qyqTIyXI12SrLnVBSWNVO1vHN8gD0H3Ud/UZeQ1hkJSYkX0RQYy3++6m/rUOrVxx9iqTBVDCuXbuGYcOG4dlnn0WbNm3g6emJgwcPYv78+RgyZIixX0lJCVQqFQRBQG5uLqZNm4aAgACzL4FqT9r9owTBoRr8uJ6//JY6/IsnCi66IH5k9amN7gOKMOWdPKxfEoylb4YitJkab67IRmxn0wT3x/X+8Fdq0aEHr31B9mn3Zl94+uox+uXL8AvSISdLgTeejEABr4FhSow5DjueI5EIgj2fBHOLWq1GcnIytm3bhrNnz0Kr1SIsLAzDhg3DjBkz4OrqanKhLQAIDAxEp06dMHfuXDzwwAO1ep7i4mJ4e3ujp9NQyCTV59ZJfD/mZdg6BIcT37idrUNwLA3jY9hu6AQtduFbFBUVmdyhVCw3/040S5sJJzfLLvxmKK/EubFzrRarNTWYCoZcLkdqaipSU1Pv2uf8+fN1FxAREZEDazAJBhERUX3CK3kSERGR6GxxN9X6pMGfRUJERER1jxUMIiIiaxAkVZulY9gpJhhERERW4OhrMDhFQkRERKJjBYOIiMgaHPxCW0wwiIiIrMDRzyKpVYLx/vvv13rAKVOm3HcwRERE1DDUKsFYtGhRrQaTSCRMMIiIiG6y4ykOS9UqwcjOzrZ2HERERA2Ko0+R3PdZJBqNBllZWdDpdGLGQ0RE1DAIIm12yuwEo7y8HAkJCXBzc0Pr1q1x4cIFAFVrL9555x3RAyQiIiL7Y3aCMX36dBw5cgS7du2CQnHrNrR9+vTBhg0bRA2OiIjIfklE2uyT2aepbtq0CRs2bECXLl0gkdx64TExMTh79qyowREREdktB78OhtkVjCtXriAoKKhae1lZmUnCQURERI7L7ASjU6dO+O9//2t8fDOpWLFiBbp27SpeZERERPbMwRd5mj1Fkpqaiv79++PkyZPQ6XR47733cOLECfz+++/YvXu3NWIkIiKyPw5+N1WzKxjdunXDb7/9hvLycjRv3hzbtm1DcHAwfv/9d3To0MEaMRIREZGdua97kcTFxSE9PV3sWIiIiBoMR79d+30lGHq9Hhs3bsSpU6cgkUjQqlUrDBkyBDIZ751GREQEwOHPIjE7Izh+/DiGDBkClUqF6OhoAMCZM2cQGBiIzZs3Iy4uTvQgiYiIyL6YvQZj3LhxaN26NfLy8nDo0CEcOnQIubm5aNOmDZ577jlrxEhERGR/bi7ytHSzU2ZXMI4cOYKDBw/C19fX2Obr64u5c+eiU6dOogZHRERkryRC1WbpGPbK7ApGdHQ0Ll++XK29oKAAkZGRogRFRERk9xz8Ohi1SjCKi4uNW0pKCqZMmYKvvvoKeXl5yMvLw1dffYXExETMmzfP2vESERGRHajVFImPj4/JZcAFQcDw4cONbcJf59E89thj0Ov1VgiTiIjIzjj4hbZqlWD8/PPP1o6DiIioYbHxaaqpqamYMWMGXnrpJSxevLhqOEHAW2+9heXLl6OwsBCdO3fGhx9+iNatWxuPU6vVSEpKwueff46Kigr07t0bH330EUJDQ816/lolGD169DBrUCIiIrKdAwcOYPny5WjTpo1J+/z587Fw4UKkpaWhRYsWePvtt9G3b19kZWXB09MTAJCYmIjvvvsO69evh7+/P6ZOnYpBgwYhIyMDUqm01jGYvcjzpvLycpw+fRpHjx412YiIiAg2W+RZWlqK0aNHY8WKFSZnfAqCgMWLF2PmzJkYOnQoYmNjkZ6ejvLycqxbtw4AUFRUhJUrV2LBggXo06cP2rVrh7Vr1+LYsWPYsWOHWXHc1+3aBw0aBE9PT7Ru3Rrt2rUz2YiIiAiiJhi3n2xRXFwMtVp916edNGkSBg4ciD59+pi0Z2dnQ6VSoV+/fsY2uVyOHj16YO/evQCAjIwMaLVakz4hISGIjY019qktsxOMxMREFBYWYt++fXB1dcXWrVuRnp6OqKgobN682dzhiIiI6G+EhYXB29vbuKWmptbYb/369Th06FCN+1UqFQAgODjYpD04ONi4T6VSwcXFxaTycWef2jL7Qls//fQTvv32W3Tq1AlOTk4IDw9H37594eXlhdTUVAwcONDcIYmIiBoeEc8iyc3NhZeXl7FZLpdX65qbm4uXXnoJ27Ztg0KhuOuQt58VClRNndzZVi2MWvS5k9kVjLKyMgQFBQEA/Pz8cOXKFQBVd1g9dOiQucMRERE1SDev5GnpBgBeXl4mW00JRkZGBgoKCtChQwfIZDLIZDLs3r0b77//PmQymbFycWcloqCgwLhPqVRCo9GgsLDwrn1q676u5JmVlQUAeOCBB/Dxxx/j4sWLWLZsGRo1amTucERERCSC3r1749ixY8jMzDRuHTt2xOjRo5GZmYlmzZpBqVRi+/btxmM0Gg12796Nbt26AQA6dOgAZ2dnkz75+fk4fvy4sU9tmT1FkpiYiPz8fADA7NmzER8fj88++wwuLi5IS0szdzgiIqKGqY6vg+Hp6YnY2FiTNnd3d/j7+xvbExMTkZKSgqioKERFRSElJQVubm4YNWoUAMDb2xsJCQmYOnUq/P394efnh6SkJMTFxVVbNPp3zE4wRo8ebfz/du3a4fz58zh9+jSaNGmCgIAAc4cjIiKiOjJt2jRUVFRg4sSJxgttbdu2zXgNDABYtGgRZDIZhg8fbrzQVlpamlnXwAAAiXDzOt9UK8XFxfD29kZPp6GQSZxtHY5D+DEvw9YhOJz4xjzlvE7xY7hO6QQtduFbFBUVmSycFMvNvxPh896G0z0WW9aGobISOa+9YbVYralWFYxXXnml1gMuXLjwvoMhIiKihqFWCcbhw4drNZi5p7DYNcEAwGDrKBzCgEjzFhaR5c58FPv3nUg00ZN5Bl5dkggCoKuDJ+LNzv4eb3ZGRERkJhvf7MzW7vteJERERER3Y/ZZJERERFQLDl7BYIJBRERkBbdfidOSMewVp0iIiIhIdKxgEBERWYODT5HcVwVjzZo16N69O0JCQpCTkwMAWLx4Mb799ltRgyMiIrJbgkibnTI7wVi6dCleeeUVPProo7hx4wb0ej0AwMfHB4sXLxY7PiIiIrJDZicYH3zwAVasWIGZM2eaXJe8Y8eOOHbsmKjBERER2Ssxb9duj8xeg5GdnY127arfp0Aul6OsrEyUoIiIiOyeg1/J0+wKRkREBDIzM6u1//DDD4iJiREjJiIiIvvn4GswzK5gvPrqq5g0aRIqKyshCAL279+Pzz//HKmpqfjkk0+sESMRERHZGbMTjGeeeQY6nQ7Tpk1DeXk5Ro0ahcaNG+O9997DyJEjrREjERGR3XH0C23d13Uwxo8fj/Hjx+Pq1aswGAwICgoSOy4iIiL75uDXwbDoQlsBAQFixUFEREQNiNkJRkREBCSSu69qPXfunEUBERERNQhinGbqSBWMxMREk8darRaHDx/G1q1b8eqrr4oVFxERkX3jFIl5XnrppRrbP/zwQxw8eNDigIiIiMj+iXY31QEDBuDrr78WazgiIiL7xutgiOOrr76Cn5+fWMMRERHZNZ6maqZ27dqZLPIUBAEqlQpXrlzBRx99JGpwREREZJ/MTjAef/xxk8dOTk4IDAxEz5490bJlS7HiIiIiIjtmVoKh0+nQtGlTxMfHQ6lUWismIiIi++fgZ5GYtchTJpPhhRdegFqttlY8REREDYKj367d7LNIOnfujMOHD1sjFiIiImogzF6DMXHiREydOhV5eXno0KED3N3dTfa3adNGtOCIiIjsmh1XICxV6wTj2WefxeLFizFixAgAwJQpU4z7JBIJBEGARCKBXq8XP0oiIiJ74+BrMGqdYKSnp+Odd95Bdna2NeMhIiKiBqDWCYYgVKVR4eHhVguGiIiooeCFtsxwr7uoEhER0W04RVJ7LVq0+Nsk4/r16xYFRERERPbPrATjrbfegre3t7ViISIiajA4RWKGkSNHIigoyFqxEBERNRwOPkVS6wttcf0FERER1ZbZZ5EQERFRLTh4BaPWCYbBYLBmHERERA2Ko6/BMPteJERERFQLgkibGZYuXYo2bdrAy8sLXl5e6Nq1K3744YdbIQkCkpOTERISAldXV/Ts2RMnTpwwGUOtVmPy5MkICAiAu7s7Bg8ejLy8PLNfPhMMIiKiBiI0NBTvvPMODh48iIMHD6JXr14YMmSIMYmYP38+Fi5ciCVLluDAgQNQKpXo27cvSkpKjGMkJiZi48aNWL9+Pfbs2YPS0lIMGjTI7FuBMMEgIiKyBhErGMXFxSabWq2u8Skfe+wxPProo2jRogVatGiBuXPnwsPDA/v27YMgCFi8eDFmzpyJoUOHIjY2Funp6SgvL8e6desAAEVFRVi5ciUWLFiAPn36oF27dli7di2OHTuGHTt2mPXymWAQERFZwc01GJZuABAWFgZvb2/jlpqa+rfPr9frsX79epSVlaFr167Izs6GSqVCv379jH3kcjl69OiBvXv3AgAyMjKg1WpN+oSEhCA2NtbYp7bMvl072a8RL17Gs9PzsfGTACybHWrrcBqE4c9fRPd+1xDarAIatRNOHvLEqvnhuJjtauwzekouegy8isBGGmi1Evx53APpC8OQdcTThpHbH9+tlxD4bR4KHwnGleF/3RNJEOD/34vw3nMFTuU6VDb1QMHIcGhC3IzHOV+pRODXuVCcLYFEZ0B5jA8KRoRD7+Vso1di/1zd9Xg66RK6xd+AT4AWZ4+7YVlyGM4cdbd1aA1Wbm4uvLy8jI/lcvld+x47dgxdu3ZFZWUlPDw8sHHjRsTExBgThODgYJP+wcHByMnJAQCoVCq4uLjA19e3Wh+VSmVWzKxgOIgWbcvx6OhrOHdSYetQGpS4B4vw3VolXh4WhxljYiCVCpibdhJy11tzlRezFfjorQi8MLAtkkbG4vJFOeamnYK3n9aGkdsX+flS+OwpgLqxq0m777Z8+OxUoWBEOC681ho6L2eEvp8FSWXV+y9R69H4/SwIEiAvsSVyk2Ig0RvQ+KMzgMGOl+fbWOL8HLT/RzHeTWyK5/vG4NCvXkhddwb+wRpbh1a/iDhFcnPR5s3tXglGdHQ0MjMzsW/fPrzwwgsYM2YMTp48adx/53WtBEH422td1abPnRpUglFQUIAJEyagSZMmkMvlUCqViI+Px++//w4AaNq0KSQSCSQSCVxdXdGyZUu8++67Df4aHwo3PV5bkoPF08JQckNq63AalDefjcGOb4Jw4Q83ZJ92x6LXIxHcWIOo2DJjn13fBSJzrw9UuQpc+MMNK1LC4e6pR0R0uQ0jtx+SSj0arT6Ly6MjoHe7regqCPD96TKu9w9BaTs/aBq74fKYZpBoDPA6cA0A4Hq2FM7X1Lj8dDNoGrtB09gNqqeaQZFTBresYhu9IvvmIjfgoQGFWJkSiuP7PZGfo8DaRSFQ5cox6Kkrtg6vXhFzisQcLi4uiIyMRMeOHZGamoq2bdvivffeg1KpBIBqlYiCggJjVUOpVEKj0aCwsPCufWqrQSUYTzzxBI4cOYL09HScOXMGmzdvRs+ePU1uwDZnzhzk5+fj1KlTSEpKwowZM7B8+XIbRm19L6bkYf9OLxz+lSV5a3Pz1AEASm7UPPsoczZgwIgClBZLce60W419yFTQ+vMoi/VBeSvT+yA5X1VDVqxFecytdsHZCRVRnlCcrVoRL9EZAAkgyCQmfQQJ4Hq2BGQ+qUyAVAZo1KbfZjWVTmjdqdRGUdG9CIIAtVqNiIgIKJVKbN++3bhPo9Fg9+7d6NatGwCgQ4cOcHZ2NumTn5+P48ePG/vUVoNZg3Hjxg3s2bMHu3btQo8ePQAA4eHhePDBB036eXp6GrO4cePGYenSpdi2bRsmTJhQ47hqtdpktW5xsX196+kxuBCRsRWYPLCFrUNxAAKem5GD4wc8kfOHafLw4COFeH3xGchdDbhe4IyZY2JQXMg1AH/H88A1KHLLceH11tX2SYurpph0nqbvo87LGc7Xqn5nKyM8YHCRImBjLq4+HgoIQODGXEgEQFrEKar7UVEmxcmD7hg1JR8X/lTgxhVn9BxyHdHtynAp++5le4dkgyt5zpgxAwMGDEBYWBhKSkqwfv167Nq1C1u3boVEIkFiYiJSUlIQFRWFqKgopKSkwM3NDaNGjQIAeHt7IyEhAVOnToW/vz/8/PyQlJSEuLg49OnTx6xYGkyC4eHhAQ8PD2zatAldunS55/wUUJXR7d69G6dOnUJUVNRd+6WmpuKtt94SO9w6ERiiwQtzLmLGqObQqhtUsapempicjYjociSNrP7H8Mg+L0wa3Abevjr0H3EZ098/g8Qn4lB0nUnG3ciuqxH4ZQ7yprSE4HyPn987poUlAoC/5or1ns7IHx+JoM/Pw2fXZUAClHT0R2WYG+DE+yvdr3dfjsDL757HugPHoNcBfx53w65Nfmgex2k/EzZIMC5fvoynnnoK+fn58Pb2Rps2bbB161b07dsXADBt2jRUVFRg4sSJKCwsROfOnbFt2zZ4et6qcC9atAgymQzDhw9HRUUFevfujbS0NEil5k2xS4QGtADh66+/xvjx41FRUYH27dujR48eGDlyJNq0aQOgag1Gfn4+nJ2dodFooNVqoVAosHPnzruWfmqqYISFhaGn5HHIJPX7j0PX+BtIXnUeet2tNqkMMBgAwQAMimgLg6H+f8g6ubr+fScbe2FWNrr2vY5X/9Ual/P+fiHtJzsOY9tXQfhiWeM6iM58pxfE2joEuGcWovHHf0C4LbeQGABBAkACnE9ug4hZR5EzozXUYbfOXghZegZ6Vykuj21uMp5TqRZwksDgJkOz1w6jsLcShf0a1dGrubfoyYdsHcJ9kbvq4e5ZVZWb/uE5uLrpMeuZu39hqy90ghY/675GUVGRyZkZYikuLoa3tzdaTUyBVG7Zwnq9uhKnPpphtVitqcFUMICqNRgDBw7Er7/+it9//x1bt27F/Pnz8cknn2Ds2LEAgFdffRVjx47FlStXMHPmTPTq1eue80pyufxvqyH1VeYeTzzXK9qkberCC8g9q8AXHwbZRXJR/wl4YXY2uvW9jtdG1y65AACJRICzC+/vcy/lLb1w/g3TREe5JhuaYAWu92sEbYAcOi9nuJ0qvpVg6Axw/aMEV/8vrNp4Bo+qLwSup4shLdGitI2PtV9Cg6eukEJdIYWHtw4dHi7GytT6mTDbyl+5sMVj2KsGlWAAgEKhQN++fdG3b1/MmjUL48aNw+zZs40JRkBAACIjIxEZGYmvv/4akZGR6NKli9lzS/agokyKnCzTb/+V5U4oKazeTvdn0lvZ6PnYVcx5PhoVZVL4BlSdpldWIoVGLYXcVY+REy/ifzt9cb3ABZ6+WgwafRkBSg1+/cHfxtHXb4JCCk1j07UsBhcn6N1lxvbCXsHw23oJ2iA5NIEK+G29BMHFCcWdbr23XnuvQKN0hd5TBsW5UgR9mYPCXkpolfwduF8dHi4CJEDeOQVCmqoxbkYe8s7Jse2LAFuHVr/wbqoNW0xMDDZt2lTjPl9fX0yePBlJSUk4fPiw2ef4Eg0afRkAMH/dSZP2BdOaY8c3QTDoJQhrVoE+/1cAbz8digtlOHPMA6+OjMWFP3gWiaUK+zWCk9aAoM9zqi60FeGBvMnREBS35opdLlci4Ns8SMt00Pq74Fr/ENzorbRh1PbPzUuPZ167iAClFqVFUuz53hdp7zaGXsfP0Ns5+t1UG0yCce3aNQwbNgzPPvss2rRpA09PTxw8eBDz58/HkCFD7nrcpEmTMG/ePHz99df45z//WYcR28a0YfV/ftSeDIjses/9Wo0T3p4Ufc8+VHt5r7QybZBIcG1QKK4NuvuVaa/+X1iNUyZ0/37d4odft/jZOgyq5xpMguHh4YHOnTtj0aJFOHv2LLRaLcLCwjB+/HjMmDHjrscFBgbiqaeeQnJyMoYOHQonJ55tQUREIuAUScMgl8uRmpp6zxvAnD9/vsb2hn6hLSIishE7ThAsxa/rREREJLoGU8EgIiKqT7jIk4iIiMTn4GswOEVCREREomMFg4iIyAo4RUJERETi4xQJERERkbhYwSAiIrICTpEQERGR+Bx8ioQJBhERkTU4eILBNRhEREQkOlYwiIiIrIBrMIiIiEh8nCIhIiIiEhcrGERERFYgEQRIBMtKEJYeb0tMMIiIiKyBUyRERERE4mIFg4iIyAp4FgkRERGJj1MkREREROJiBYOIiMgKOEVCRERE4nPwKRImGERERFbg6BUMrsEgIiIi0bGCQUREZA2cIiEiIiJrsOcpDktxioSIiIhExwoGERGRNQhC1WbpGHaKCQYREZEV8CwSIiIiIpGxgkFERGQNDn4WCSsYREREViAxiLOZIzU1FZ06dYKnpyeCgoLw+OOPIysry6SPIAhITk5GSEgIXF1d0bNnT5w4ccKkj1qtxuTJkxEQEAB3d3cMHjwYeXl5ZsXCBIOIiKiB2L17NyZNmoR9+/Zh+/bt0Ol06NevH8rKyox95s+fj4ULF2LJkiU4cOAAlEol+vbti5KSEmOfxMREbNy4EevXr8eePXtQWlqKQYMGQa/X1zoWTpEQERFZg4hTJMXFxSbNcrkccrm8WvetW7eaPF69ejWCgoKQkZGBhx9+GIIgYPHixZg5cyaGDh0KAEhPT0dwcDDWrVuHCRMmoKioCCtXrsSaNWvQp08fAMDatWsRFhaGHTt2ID4+vlahs4JBRERkBTfPIrF0A4CwsDB4e3sbt9TU1FrFUFRUBADw8/MDAGRnZ0OlUqFfv37GPnK5HD169MDevXsBABkZGdBqtSZ9QkJCEBsba+xTG6xgEBERWYOI18HIzc2Fl5eXsbmm6kX1QwW88soreOihhxAbGwsAUKlUAIDg4GCTvsHBwcjJyTH2cXFxga+vb7U+N4+vDSYYRERE9ZyXl5dJglEbL774Io4ePYo9e/ZU2yeRSEweC4JQre1OtelzO06REBERWYGYUyTmmjx5MjZv3oyff/4ZoaGhxnalUgkA1SoRBQUFxqqGUqmERqNBYWHhXfvUBisY90nq6QGpxMXWYTgE/R2Lm8j6Wkw8YOsQHErW8o62DsGhGCoqgclfW/+JbHAdDEEQMHnyZGzcuBG7du1CRESEyf6IiAgolUps374d7dq1AwBoNBrs3r0b8+bNAwB06NABzs7O2L59O4YPHw4AyM/Px/HjxzF//vxax8IEg4iIqIGYNGkS1q1bh2+//Raenp7GSoW3tzdcXV0hkUiQmJiIlJQUREVFISoqCikpKXBzc8OoUaOMfRMSEjB16lT4+/vDz88PSUlJiIuLM55VUhtMMIiIiKzAFvciWbp0KQCgZ8+eJu2rV6/G2LFjAQDTpk1DRUUFJk6ciMLCQnTu3Bnbtm2Dp6ensf+iRYsgk8kwfPhwVFRUoHfv3khLS4NUKq11LEwwiIiIrMEGd1MVatFfIpEgOTkZycnJd+2jUCjwwQcf4IMPPjDr+W/HRZ5EREQkOlYwiIiIrMDRb9fOBIOIiMgaeDdVIiIiInGxgkFERGQFnCIhIiIi8RmEqs3SMewUEwwiIiJr4BoMIiIiInGxgkFERGQFEoiwBkOUSGyDCQYREZE12OBKnvUJp0iIiIhIdKxgEBERWQFPUyUiIiLx8SwSIiIiInGxgkFERGQFEkGAxMJFmpYeb0tMMIiIiKzB8Ndm6Rh2ilMkREREJDpWMIiIiKyAUyREREQkPgc/i4QJBhERkTXwSp5ERERE4mIFg4iIyAp4JU8iIiISH6dIiIiIiMTFCgYREZEVSAxVm6Vj2CsmGERERNbAKRIiIiIicbGCQUREZA280BYRERGJzdEvFc4pEiIiIhIdKxhERETW4OCLPJlgEBERWYMAwNLTTO03v2CCQUREZA1cg0FEREQkMlYwiIiIrEGACGswRInEJphgEBERWYODL/LkFAkRERGJjglGA/LoyEv48NsMfHVwL746uBcL1mei4z+uAwCkMgOemZqNjzZn4JtDv2HNL//D1Hey4BektnHUDU9s51K8lZ6NdYdO4MdLR9C1f5GtQ2rQnnwlHz9ezDTZPj983NZhNQi+319Ci/EHELj+QlWDzoCAr3IRnnwckZMy0CwpE8qV5yC9oal5AEFA4/fOoMX4A3A/XFh3gdcXBpE2M/zyyy947LHHEBISAolEgk2bNpnsFwQBycnJCAkJgaurK3r27IkTJ06Y9FGr1Zg8eTICAgLg7u6OwYMHIy8vz7xAwASjQbl6WY7VCyLw0j8fwEv/fABH9vngzQ9PoklkGeQKAyJjSvH5R00w+Yl2eHtyKzRuWoHZH520ddgNjsLNgHMnFPhwZmNbh+Iwzp9WYOQDrY3b871b2jokuyfPLoXPL1egDnU1tjlpDJBfKMe1gSHIeTMGl16IhPPlSjRe8keNY/jsuFxX4dZLN88isXQzR1lZGdq2bYslS5bUuH/+/PlYuHAhlixZggMHDkCpVKJv374oKSkx9klMTMTGjRuxfv167NmzB6WlpRg0aBD0er1Zsdg8wVCpVHjppZcQGRkJhUKB4OBgPPTQQ1i2bBnKy8sBAIcPH8agQYMQFBQEhUKBpk2bYsSIEbh69SoyMjIgkUiwZ8+eGsePj4/H4MGDIZFI7rmNHTu2Dl+1dez/2R8Hf/HDxfNuuHjeDZ8uborKcilati1BeakMMxPi8OvWQFzMdkPWES8sfbs5omJLEdio0tahNygHf/ZC+vxG+O0HH1uH4jD0eqDwirNxK7rO5WWWkFTq0eiTc7j8dFPo3W69lwY3GS6+Eo3STn7QKl1R2dwDBf9qAkVOOWTXTKuhLrnl8N2ugmpsRF2H79AGDBiAt99+G0OHDq22TxAELF68GDNnzsTQoUMRGxuL9PR0lJeXY926dQCAoqIirFy5EgsWLECfPn3Qrl07rF27FseOHcOOHTvMisWmv4Xnzp1D9+7d4ePjg5SUFMTFxUGn0+HMmTNYtWoVQkJC0KVLF/Tp0wePPfYYfvzxR/j4+CA7OxubN29GeXk5OnTogLZt22L16tV46KGHTMbPzc3Fjh078M0332D58uXG9g0bNmDWrFnIysoytrm6uqIhcXIS8FD/K1C46XEq07PGPu6eOhgMQGkxP4zJvjWO0GBdxnFoNU44fdgNq99pBNUFua3DsltB63JQ1sYH5THe8Ptv/j37Siv0ECRVycdNErUejVacRcGocOi9na0dbv0l4iLP4uJik2a5XA653Lyf8ezsbKhUKvTr189knB49emDv3r2YMGECMjIyoNVqTfqEhIQgNjYWe/fuRXx8fK2fz6Z/WSZOnAiZTIaDBw/C3d3d2B4XF4cnnngCgiDg22+/RXFxMT755BPIZFXhRkREoFevXsb+CQkJmDFjBt5//32TcdLS0hAYGIiBAwcajwUAb29vSCQSKJXKOniVdatpizIs+DwTLnIDKsql+PeLMcg9616tn7OLAc9MPY9dWwJRUcYEg+zX6cPuePclV+Sdk8M3UId/TVFh0bd/4LleLVFSyJ9tc3nuvwbFhXJcmBnzt30lWgMCvslDyYN+MLhKje2BX+SisrkHyh7wtWao9Z+ICUZYWJhJ8+zZs5GcnGzWUCqVCgAQHBxs0h4cHIycnBxjHxcXF/j6+lbrc/P42rLZFMm1a9ewbds2TJo0ySQpuN3NJECn02Hjxo0Q7vIPNXr0aGi1Wnz55ZfGNkEQkJaWhjFjxpgkF+ZSq9UoLi422eqzvGxXvPh/7fHKyAfw/fpGmPpOFsKal5n0kcoMeH3haUgkAj58K9JGkRKJ4+DPXtjzvQ/On3bF4V898ebTzQAAfYddt3Fk9kd2XY3A9ReQn9AMgvPf/HnQGdBo+VlAAApGNzU2u2cWwu10MQpGNLFusA4mNzcXRUVFxm369On3PZZEIjF5LAhCtbY71abPnWyWYPz5558QBAHR0dEm7QEBAfDw8ICHhwdee+01dOnSBTNmzMCoUaMQEBCAAQMG4N1338Xly7cWD/n5+eHxxx/H6tWrjW27du3CuXPn8Oyzz1oUZ2pqKry9vY3bnVlkfaPTOiH/giv+OO6JtIUROHfaA0OevmTcL5UZMH3RaQSHVmJmQhyrF9TgqCukOH9agcYRPEPKXPKccshKdAh/+wSiJhxA1IQDcDtTAp+fLiNqwgHA8NeXPJ0BIR+fhfNVNfJejjapXridLoHzFTUiXzpkHAMAQpb+idB3T9viZdnOzQqGpRsALy8vk83c6REAxqr9nZWIgoICY1VDqVRCo9GgsLDwrn1qy+aLPO/MiPbv34/MzEy0bt0aanXVB8TcuXOhUqmwbNkyxMTEYNmyZWjZsiWOHTtmPC4hIQG//PIL/vzzTwDAqlWr0L1792oJjLmmT59ukjXm5uZaNF5dk0gEOLtUned0M7kICa/AjGdiUXLDgedGqcFydjEgLEqN65f5822u8lZeOJ/cGjmzbm2V4W4o6eyPnFmtASfJreSiQI28V6Jh8DD9knJ9QCPkzDYdAwCujGjieAs+bXCa6r1ERERAqVRi+/btxjaNRoPdu3ejW7duAIAOHTrA2dnZpE9+fj6OHz9u7FNbNvv6GhkZCYlEgtOnTTPaZs2qypt3Lrr09/fHsGHDMGzYMKSmpqJdu3b4z3/+g/T0dABAnz59EB4ejrS0NEybNg3ffPPNXU/TMcf9LKSxlTEvn8fBX3xxRSWHm7seDz96BXEPFmHW+Fg4SQXMeO8UImNKkfx8a0ilgG9A1bnrJUUy6LQ2zzUbDIWbHiERt64LoAzToFnrCpTckOLKRRcbRtYwjX/zIvZt90bBRWf4BOgw6qXLcPPQY/uXfrYOze4ICik0jd1M2gxyKfTusqp2vYCQZWchv1CGi5NbAAZAWqQFAOjdpYDMCXpv5xoXdmr9XKALtI/PUrHY4mZnpaWlxi/aQNXCzszMTPj5+aFJkyZITExESkoKoqKiEBUVhZSUFLi5uWHUqFEAqtYoJiQkYOrUqfD394efnx+SkpIQFxeHPn36mBWLzRIMf39/9O3bF0uWLMHkyZPvug6jJi4uLmjevDnKym6tLZBIJHjmmWfwySefIDQ0FE5OThg+fLg1Qq+3fPw1SJqfBb9ADcpKZMjOcses8bE4vNcXQY0r0bV31Zz0h98eNjnutafjcGy/jw0ibphatK3Au1+fNT5+/q2qKaptG3yx4GXOS4stoJEW0z88Dy8/PYquyXD6kBsSH2uBAiZzopMVauBx5AYAoOkc04sz5SZFoyLaywZR0e0OHjyIRx55xPj4lVdeAQCMGTPG+AW8oqICEydORGFhITp37oxt27bB0/PW2YaLFi2CTCbD8OHDUVFRgd69eyMtLQ1SqbTa892LRLjbysk6cPbsWXTv3h2+vr5ITk5GmzZt4OTkhAMHDiApKQmjR4/GI488gvXr12PkyJFo0aIFBEHAd999h9dffx2rV6/GU089ZRzvwoULiIiIgLe3N5544gmsWLGixudNS0tDYmIibty4YXbMxcXF8Pb2Rm+vJyGT8AOsLujr+cLaBsnMxVxkmTPLO9o6BIdiqKhE3uTZKCoqgpeX+EnRzb8TfaJehkxqWdVGp1djxx+LrBarNdl0hV/z5s1x+PBhpKSkYPr06cjLy4NcLkdMTAySkpIwceJEqFQquLm5YerUqcjNzYVcLkdUVBQ++eQTk+QCAJo0aYI+ffpg27ZtFi/uJCIisohBACQWfoc32KwGYDGbVjDsESsYdY8VDBtgBaNOsYJRt+qsgtE8UZwKxtnFrGAQERHRXxz8du1MMIiIiKxChAQD9ptg8NxEIiIiEh0rGERERNbAKRIiIiISnUGAxVMcdnwWCadIiIiISHSsYBAREVmDYKjaLB3DTjHBICIisgauwSAiIiLRcQ0GERERkbhYwSAiIrIGTpEQERGR6ASIkGCIEolNcIqEiIiIRMcKBhERkTVwioSIiIhEZzAAsPA6Fgb7vQ4Gp0iIiIhIdKxgEBERWQOnSIiIiEh0Dp5gcIqEiIiIRMcKBhERkTU4+KXCmWAQERFZgSAYIFh4N1RLj7clJhhERETWIAiWVyC4BoOIiIjoFlYwiIiIrEEQYQ2GHVcwmGAQERFZg8EASCxcQ2HHazA4RUJERESiYwWDiIjIGjhFQkRERGITDAYIFk6R2PNpqpwiISIiItGxgkFERGQNnCIhIiIi0RkEQOK4CQanSIiIiEh0rGAQERFZgyAAsPQ6GPZbwWCCQUREZAWCQYBg4RSJwASDiIiITAgGWF7B4GmqREREVA989NFHiIiIgEKhQIcOHfDrr7/aJA4mGERERFYgGARRNnNs2LABiYmJmDlzJg4fPox//OMfGDBgAC5cuGClV3l3TDCIiIisQTCIs5lh4cKFSEhIwLhx49CqVSssXrwYYWFhWLp0qZVe5N1xDYaZbi640QkaG0fiOPSC1tYhOCCJrQNwKIaKSluH4FBuvt/WXkCpg9bi62zpUPX5V1xcbNIul8shl8tN2jQaDTIyMvD666+btPfr1w979+61LJD7wATDTCUlJQCA3SVf2DgSIiuy34Xr9mnyJltH4JBKSkrg7e0t+rguLi5QKpXYo/pelPE8PDwQFhZm0jZ79mwkJyebtF29ehV6vR7BwcEm7cHBwVCpVKLEYg4mGGYKCQlBbm4uPD09IZHYz7e84uJihIWFITc3F15eXrYOxyHwPa9bfL/rlj2/34IgoKSkBCEhIVYZX6FQIDs7GxqNOJVuQRCq/b25s3pxuzv71nR8XWCCYSYnJyeEhobaOoz75uXlZXcfBvaO73nd4vtdt+z1/bZG5eJ2CoUCCoXCqs9xp4CAAEil0mrVioKCgmpVjbrARZ5EREQNgIuLCzp06IDt27ebtG/fvh3dunWr83hYwSAiImogXnnlFTz11FPo2LEjunbtiuXLl+PChQt4/vnn6zwWJhgOQi6XY/bs2fectyNx8T2vW3y/6xbf7/ppxIgRuHbtGubMmYP8/HzExsbi+++/R3h4eJ3HIhHs+ULnREREVC9xDQYRERGJjgkGERERiY4JBhEREYmOCQYRERGJjgmGHdu7dy+kUin69+9v0n7+/HlIJJJq25NPPmmyPzMzs8b+Li4uiIyMxNtvv231a/Xbu4KCAkyYMAFNmjSBXC6HUqlEfHw8fv/9dwBA06ZNje+rVCpFSEgIEhISUFhYaOPI7Zc577mrqytatmyJd999lz/LNVCpVHjppZcQGRkJhUKB4OBgPPTQQ1i2bBnKy8sBAIcPH8agQYMQFBQEhUKBpk2bYsSIEbh69SoyMjIgkUiwZ8+eGsePj4/H4MGDa/w8un0bO3ZsHb5qqis8TdWOrVq1CpMnT8Ynn3yCCxcuoEmTJib7d+zYgdatWxsfu7q63nO8m/3VajX27NmDcePGoVGjRkhISLBK/A3BE088Aa1Wi/T0dDRr1gyXL1/Gzp07cf36dWOfOXPmYPz48dDr9Thz5gyee+45TJkyBWvWrLFh5PbLnPe8srISO3bswAsvvAAvLy9MmDDBhpHXL+fOnUP37t3h4+ODlJQUxMXFQafT4cyZM1i1ahVCQkLQpUsX9OnTB4899hh+/PFH+Pj4IDs7G5s3b0Z5eTk6dOiAtm3bYvXq1XjooYdMxs/NzcWOHTvwzTffYPny5cb2DRs2YNasWcjKyjK2/d1nE9kpgexSaWmp4OnpKZw+fVoYMWKE8NZbbxn3ZWdnCwCEw4cP13jsnfvv1r9Xr17CxIkTrfQK7F9hYaEAQNi1a9dd+4SHhwuLFi0yaZszZ44QExNj5egapvt9z9u3by8MHTrUytHZl/j4eCE0NFQoLS2tcb/BYBA2btwoyGQyQavV3nWc999/X/Dw8Kg2zpw5c4Tg4OBqx65evVrw9va2OH6q/zhFYqc2bNiA6OhoREdH48knn8Tq1atFLQEfPHgQhw4dQufOnUUbs6Hx8PCAh4cHNm3aBLVaXatjLl68iC1btvB9vU/mvueCIGDXrl04deoUnJ2d6yBC+3Dt2jVs27YNkyZNgru7e419JBIJlEoldDodNm7ceNfPl9GjR0Or1eLLL780tgmCgLS0NIwZMwYyGQvlDsu2+Q3dr27dugmLFy8WBEEQtFqtEBAQIGzfvl0QhFsVCVdXV8Hd3d24HTp0yGT/nRWMm/2dnZ0FAMJzzz1nk9dmT7766ivB19dXUCgUQrdu3YTp06cLR44cMe4PDw8XXFxcBHd3d0GhUAgAhM6dOwuFhYW2C9rOmfOe3/xZVigUwm+//WbDqOuXffv2CQCEb775xqTd39/f+Hkxbdo0QRAEYcaMGYJMJhP8/PyE/v37C/PnzxdUKpXJcSNGjBAefvhh4+OffvpJACCcPn262nOzguE4WMGwQ1lZWdi/fz9GjhwJAJDJZBgxYgRWrVpl0m/Dhg3IzMw0bjExMfcc92b/I0eOYMOGDfj222/x+uuvW+11NARPPPEELl26hM2bNyM+Ph67du1C+/btkZaWZuzz6quvIjMzE0ePHsXOnTsBAAMHDoRer7dR1PbNnPd89+7deOSRRzBz5kyb3OypvrvzFt779+9HZmamcS0WAMydOxcqlQrLli1DTEwMli1bhpYtW+LYsWPG4xISEvDLL7/gzz//BFC1Pqx79+6Ijo6uuxdD9Y+tMxwy36uvvioAEKRSqXFzcnIS5HK5cP36ddHWYKSmpgoymUyoqKiw7gtqYBISEoQmTZoIglDzeoDff/9dAGCsOJHl7vWeX79+XfDz8+P7fZurV68KEolESE1NrXF/jx49hJdeeqnGfWq1WoiJiRGefvppY5vBYBDCw8OFmTNnCkVFRYKbm5uwatWqGo9nBcNxsIJhZ3Q6HT799FMsWLDApDpx5MgRhIeH47PPPhPtuaRSKXQ6HTQajWhjOoKYmBiUlZXddb9UKgUAVFRU1FVIDd693nNfX19MnjwZSUlJPFX1L/7+/ujbty+WLFlyz5/Vmri4uKB58+Ymx0kkEjzzzDNIT0/HunXr4OTkhOHDh4sdNtkZJhh2ZsuWLSgsLERCQgJiY2NNtn/+859YuXLlfY997do1qFQq5OXl4YcffsB7772HRx55BF5eXiK+gobj2rVr6NWrF9auXYujR48iOzsbX375JebPn48hQ4YY+5WUlEClUiE/Px/79+/Hq6++ioCAAJbs70Nt3/M7TZo0CVlZWfj666/rMNr67aOPPoJOp0PHjh2xYcMGnDp1CllZWVi7di1Onz4NqVSKLVu24Mknn8SWLVtw5swZZGVl4T//+Q++//77au/3M888g0uXLmHGjBkYOXLkXRePkgOxdQmFzDNo0CDh0UcfrXFfRkaGAMD4X3OnSG5uUqlUCA0NFcaPHy8UFBRY6ZXYv8rKSuH1118X2rdvL3h7ewtubm5CdHS08MYbbwjl5eWCIFSV629/bwMDA4VHH330rv82dG+1fc/vnJYSBEEYP3680Lp1a0Gv19dx1PXXpUuXhBdffFGIiIgQnJ2dBQ8PD+HBBx8U3n33XaGsrEw4e/asMH78eKFFixaCq6ur4OPjI3Tq1ElYvXp1jeP169dPACDs3bv3rs/JKRLHwdu1ExERkeg4RUJERESiY4JBREREomOCQURERKJjgkFERESiY4JBREREomOCQURERKJjgkFERESiY4JBREREomOCQWSHkpOT8cADDxgfjx07Fo8//nidx3H+/HlIJBJkZmbetU/Tpk2xePHiWo+ZlpYGHx8fi2OTSCTYtGmTxeMQ0f1hgkEkkrFjx0IikUAikcDZ2RnNmjVDUlKS2TeTuh/vvfeeye3K76U2SQERkaVktg6AqCHp378/Vq9eDa1Wi19//RXjxo1DWVkZli5dWq2vVquFs7OzKM/r7e0tyjhERGJhBYNIRHK5HEqlEmFhYRg1ahRGjx5tLNPfnNZYtWoVmjVrBrlcDkEQUFRUhOeeew5BQUHw8vJCr169cOTIEZNx33nnHQQHB8PT0xMJCQmorKw02X/nFInBYMC8efMQGRkJuVyOJk2aYO7cuQCAiIgIAEC7du0gkUjQs2dP43GrV69Gq1atoFAo0LJlS3z00Ucmz7N//360a9cOCoUCHTt2xOHDh81+jxYuXIi4uDi4u7sjLCwMEydORGlpabV+mzZtQosWLaBQKNC3b1/k5uaa7P/uu+/QoUMHKBQKNGvWDG+99RZ0Op3Z8RCRdTDBILIiV1dXaLVa4+M///wTX3zxBb7++mvjFMXAgQOhUqnw/fffIyMjA+3bt0fv3r1x/fp1AMAXX3yB2bNnY+7cuTh48CAaNWpU7Q//naZPn4558+bhzTffxMmTJ7Fu3ToEBwcDqEoSAGDHjh3Iz8/HN998AwBYsWIFZs6ciblz5+LUqVNISUnBm2++ifT0dABAWVkZBg0ahOjoaGRkZCA5ORlJSUlmvydOTk54//33cfz4caSnp+Onn37CtGnTTPqUl5dj7ty5SE9Px2+//Ybi4mKMHDnSuP/HH3/Ek08+iSlTpuDkyZP4+OOPkZaWZkyiiKgesPHdXIkajDFjxghDhgwxPv7f//4n+Pv7C8OHDxcEQRBmz54tODs7CwUFBcY+O3fuFLy8vITKykqTsZo3by58/PHHgiAIQteuXYXnn3/eZH/nzp2Ftm3b1vjcxcXFglwuF1asWFFjnNnZ2QKAareMDwsLE9atW2fS9u9//1vo2rWrIAiC8PHHHwt+fn5CWVmZcf/SpUtrHOt2d7t9+k1ffPGF4O/vb3y8evVqAYCwb98+Y9upU6cEAML//vc/QRAE4R//+IeQkpJiMs6aNWuERo0aGR8DEDZu3HjX5yUi6+IaDCIRbdmyBR4eHtDpdNBqtRgyZAg++OAD4/7w8HAEBgYaH2dkZKC0tBT+/v4m41RUVODs2bMAgFOnTuH555832d+1a1f8/PPPNcZw6tQpqNVq9O7du9ZxX7lyBbm5uUhISMD48eON7Tqdzri+49SpU2jbti3c3NxM4jDXzz//jJSUFJw8eRLFxcXQ6XSorKxEWVkZ3N3dAQAymQwdO3Y0HtOyZUv4+Pjg1KlTePDBB5GRkYEDBw6YVCz0ej0qKytRXl5uEiMR2QYTDCIRPfLII1i6dCmcnZ0REhJSbRHnzT+gNxkMBjRq1Ai7du2qNtb9nqrp6upq9jEGgwFA1TRJ586dTfZJpVIAgCAI9xXP7XJycvDoo4/i+eefx7///W/4+flhz549SEhIMJlKAqpOM73TzTaDwYC33noLQ4cOrdZHoVBYHCcRWY4JBpGI3N3dERkZWev+7du3h0qlgkwmQ9OmTWvs06pVK+zbtw9PP/20sW3fvn13HTMqKgqurq7YuXMnxo0bV22/i4sLgKpv/DcFBwejcePGOHfuHEaPHl3juDExMVizZg0qKiqMScy94qjJwYMHodPpsGDBAjg5VS0B++KLL6r10+l0OHjwIB588EEAQFZWFm7cuIGWLVsCqHrfsrKyzHqviahuMcEgsqE+ffqga9euePzxxzFv3jxER0fj0qVL+P777/H444+jY8eOeOmllzBmzBh07NgRDz30ED777DOcOHECzZo1q3FMhUKB1157DdOmTYOLiwu6d++OK1eu4MSJE0hISEBQUBBcXV2xdetWhIaGQqFQwNvbG8nJyZgyZQq8vLwwYMAAqNVqHDx4EIWFhXjllVcwatQozJw5EwkJCXjjjTdw/vx5/Oc//zHr9TZv3hw6nQ4ffPABHnvsMfz2229YtmxZtX7Ozs6YPHky3n//fTg7O+PFF19Ely5djAnHrFmzMGjQIISFhWHYsGFwcnLC0aNHcezYMbz99tvm/0MQkeh4FgmRDUkkEnz//fd4+OGH8eyzz6JFixYYOXIkzp8/bzzrY8SIEZg1axZee+01dOjQATk5OXjhhRfuOe6bb76JqVOnYtasWWjVqhVGjBiBgoICAFXrG95//318/PHHCAkJwZAhQwAA48aNwyeffIK0tDTExcWhR48eSEtLM57W6uHhge+++w4nT55Eu3btMHPmTMybN8+s1/vAAw9g4cKFmDdvHmJjY/HZZ58hNTW1Wj83Nze89tprGDVqFLp27QpXV1esX7/euD8+Ph5btmzB9u3b0alTJ3Tp0gULFy5EeHi4WfEQkfVIBDEmVomIiIhuwwoGERERiY4JBhEREYmOCQYRERGJjgkGERERiY4JBhEREYmOCQYRERGJjgkGERERiY4JBhEREYmOCQYRERGJjgkGERERiY4JBhEREYnu/wHeU4MNneUtmgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rhythm Group</th>\n",
       "      <th>ACC</th>\n",
       "      <th>F1-score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>specificity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AFIB</td>\n",
       "      <td>0.961502</td>\n",
       "      <td>0.912360</td>\n",
       "      <td>0.904232</td>\n",
       "      <td>0.908277</td>\n",
       "      <td>0.974481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SB</td>\n",
       "      <td>0.982629</td>\n",
       "      <td>0.985861</td>\n",
       "      <td>0.967213</td>\n",
       "      <td>0.976448</td>\n",
       "      <td>0.980769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SR</td>\n",
       "      <td>0.976056</td>\n",
       "      <td>0.919101</td>\n",
       "      <td>0.964623</td>\n",
       "      <td>0.941312</td>\n",
       "      <td>0.991098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GSVT</td>\n",
       "      <td>0.963380</td>\n",
       "      <td>0.917749</td>\n",
       "      <td>0.913793</td>\n",
       "      <td>0.915767</td>\n",
       "      <td>0.976019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>macro avg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.935451</td>\n",
       "      <td>0.937465</td>\n",
       "      <td>0.933768</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>micro avg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.941784</td>\n",
       "      <td>0.941784</td>\n",
       "      <td>0.941784</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>weighted avg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.941703</td>\n",
       "      <td>0.941927</td>\n",
       "      <td>0.941784</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rhythm Group       ACC  F1-score  Precision    Recall  specificity\n",
       "0          AFIB  0.961502  0.912360   0.904232  0.908277     0.974481\n",
       "1            SB  0.982629  0.985861   0.967213  0.976448     0.980769\n",
       "2            SR  0.976056  0.919101   0.964623  0.941312     0.991098\n",
       "3          GSVT  0.963380  0.917749   0.913793  0.915767     0.976019\n",
       "4     macro avg       NaN  0.935451   0.937465  0.933768          NaN\n",
       "5     micro avg       NaN  0.941784   0.941784  0.941784          NaN\n",
       "6  weighted avg       NaN  0.941703   0.941927  0.941784          NaN"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_test = evaluation_test(y_test,result_test)\n",
    "df_evaluation_test = pd.DataFrame(data=evaluation_test,columns=[\"Rhythm Group\",\"ACC\",\"F1-score\",\"Precision\",\"Recall\",\"specificity\"])\n",
    "df_evaluation_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_evaluation_test.to_csv(\"./Result/Blending_RF_PCA.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
