{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>2</th>\n",
       "      <th>5</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>...</th>\n",
       "      <th>181</th>\n",
       "      <th>186</th>\n",
       "      <th>187</th>\n",
       "      <th>193</th>\n",
       "      <th>197</th>\n",
       "      <th>203</th>\n",
       "      <th>204</th>\n",
       "      <th>205</th>\n",
       "      <th>209</th>\n",
       "      <th>212</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>274.986868</td>\n",
       "      <td>-0.319753</td>\n",
       "      <td>-1.432466</td>\n",
       "      <td>325.821586</td>\n",
       "      <td>84.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>437.750000</td>\n",
       "      <td>...</td>\n",
       "      <td>2383.209877</td>\n",
       "      <td>650.000000</td>\n",
       "      <td>-0.338865</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>950.222222</td>\n",
       "      <td>991.309467</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>172.000000</td>\n",
       "      <td>10656.395062</td>\n",
       "      <td>135.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>104.913059</td>\n",
       "      <td>0.158313</td>\n",
       "      <td>-0.696295</td>\n",
       "      <td>336.569414</td>\n",
       "      <td>32.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>312.222222</td>\n",
       "      <td>...</td>\n",
       "      <td>2193.439446</td>\n",
       "      <td>384.000000</td>\n",
       "      <td>0.095966</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>574.500000</td>\n",
       "      <td>912.932251</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>-15.000000</td>\n",
       "      <td>3944.000000</td>\n",
       "      <td>-1.066667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.687572</td>\n",
       "      <td>0.396421</td>\n",
       "      <td>-0.312612</td>\n",
       "      <td>94.909877</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>329.600000</td>\n",
       "      <td>...</td>\n",
       "      <td>49.109375</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.217133</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>593.733333</td>\n",
       "      <td>338.632833</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>2058.773333</td>\n",
       "      <td>95.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>3.591772</td>\n",
       "      <td>-0.021014</td>\n",
       "      <td>-0.856142</td>\n",
       "      <td>254.059787</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.036070</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045933</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>-0.026782</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>420.181818</td>\n",
       "      <td>325.369999</td>\n",
       "      <td>0.739130</td>\n",
       "      <td>-9.000000</td>\n",
       "      <td>1120.888889</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>25.118469</td>\n",
       "      <td>-0.276816</td>\n",
       "      <td>-1.271399</td>\n",
       "      <td>461.130814</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>427.250000</td>\n",
       "      <td>...</td>\n",
       "      <td>736.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>-0.285584</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1068.750000</td>\n",
       "      <td>877.512514</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>671.000000</td>\n",
       "      <td>136.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8511</th>\n",
       "      <td>3.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>51.114860</td>\n",
       "      <td>2.153820</td>\n",
       "      <td>2.645687</td>\n",
       "      <td>365.256750</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>342.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>22201.000000</td>\n",
       "      <td>0.003006</td>\n",
       "      <td>0.003006</td>\n",
       "      <td>0.003006</td>\n",
       "      <td>0.003757</td>\n",
       "      <td>0.003757</td>\n",
       "      <td>0.003757</td>\n",
       "      <td>0.022262</td>\n",
       "      <td>0.044242</td>\n",
       "      <td>0.037385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8512</th>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.894913</td>\n",
       "      <td>-0.311206</td>\n",
       "      <td>-1.184514</td>\n",
       "      <td>358.414529</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>481.250000</td>\n",
       "      <td>...</td>\n",
       "      <td>2913.580247</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>-0.625647</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1091.750000</td>\n",
       "      <td>267.711052</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>1294.530612</td>\n",
       "      <td>155.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8513</th>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>107.653355</td>\n",
       "      <td>0.475616</td>\n",
       "      <td>0.784000</td>\n",
       "      <td>180.045117</td>\n",
       "      <td>20.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>341.714286</td>\n",
       "      <td>...</td>\n",
       "      <td>2954.775510</td>\n",
       "      <td>136.000000</td>\n",
       "      <td>0.385381</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>654.571429</td>\n",
       "      <td>484.863590</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>2213.551020</td>\n",
       "      <td>104.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8514</th>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>24.535688</td>\n",
       "      <td>-0.263431</td>\n",
       "      <td>-1.567800</td>\n",
       "      <td>251.455499</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>449.750000</td>\n",
       "      <td>...</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>-0.320207</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1075.000000</td>\n",
       "      <td>704.569046</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>4933.551020</td>\n",
       "      <td>88.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8515</th>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.242421</td>\n",
       "      <td>0.214800</td>\n",
       "      <td>-1.575835</td>\n",
       "      <td>505.203302</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>463.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>11.358025</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.218367</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1041.000000</td>\n",
       "      <td>1946.010560</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>350.000000</td>\n",
       "      <td>150.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8516 rows × 99 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label     2           5         7         8           9    11    12  \\\n",
       "0       0.0  10.0  274.986868 -0.319753 -1.432466  325.821586  84.0   9.0   \n",
       "1       0.0  17.0  104.913059  0.158313 -0.696295  336.569414  32.0  10.0   \n",
       "2       3.0  16.0    4.687572  0.396421 -0.312612   94.909877  -2.0  16.0   \n",
       "3       3.0  23.0    3.591772 -0.021014 -0.856142  254.059787   9.0  10.0   \n",
       "4       1.0   9.0   25.118469 -0.276816 -1.271399  461.130814  -1.0   9.0   \n",
       "...     ...   ...         ...       ...       ...         ...   ...   ...   \n",
       "8511    3.0  16.0   51.114860  2.153820  2.645687  365.256750  -2.0  15.0   \n",
       "8512    1.0   9.0    5.894913 -0.311206 -1.184514  358.414529  -5.0   9.0   \n",
       "8513    2.0  15.0  107.653355  0.475616  0.784000  180.045117  20.0  14.0   \n",
       "8514    1.0   9.0   24.535688 -0.263431 -1.567800  251.455499  -1.0   9.0   \n",
       "8515    1.0   9.0    8.242421  0.214800 -1.575835  505.203302  -2.0   9.0   \n",
       "\n",
       "        13          14  ...           181         186       187        193  \\\n",
       "0      9.0  437.750000  ...   2383.209877  650.000000 -0.338865   9.000000   \n",
       "1     16.0  312.222222  ...   2193.439446  384.000000  0.095966   3.000000   \n",
       "2     15.0  329.600000  ...     49.109375   14.000000  0.217133  15.000000   \n",
       "3      0.0    0.036070  ...      0.045933   14.000000 -0.026782   6.000000   \n",
       "4      8.0  427.250000  ...    736.000000   72.000000 -0.285584   8.000000   \n",
       "...    ...         ...  ...           ...         ...       ...        ...   \n",
       "8511  15.0  342.666667  ...  22201.000000    0.003006  0.003006   0.003006   \n",
       "8512   8.0  481.250000  ...   2913.580247   24.000000 -0.625647   8.000000   \n",
       "8513  14.0  341.714286  ...   2954.775510  136.000000  0.385381  12.000000   \n",
       "8514   8.0  449.750000  ...     24.000000   64.000000 -0.320207   7.000000   \n",
       "8515   8.0  463.500000  ...     11.358025   22.000000  0.218367   7.000000   \n",
       "\n",
       "              197          203       204         205           209         212  \n",
       "0      950.222222   991.309467  1.000000  172.000000  10656.395062  135.800000  \n",
       "1      574.500000   912.932251  0.882353  -15.000000   3944.000000   -1.066667  \n",
       "2      593.733333   338.632833  1.000000   -4.000000   2058.773333   95.500000  \n",
       "3      420.181818   325.369999  0.739130   -9.000000   1120.888889   12.000000  \n",
       "4     1068.750000   877.512514  1.000000    2.000000    671.000000  136.444444  \n",
       "...           ...          ...       ...         ...           ...         ...  \n",
       "8511     0.003757     0.003757  0.003757    0.022262      0.044242    0.037385  \n",
       "8512  1091.750000   267.711052  0.888889   -3.000000   1294.530612  155.333333  \n",
       "8513   654.571429   484.863590  1.000000   -4.000000   2213.551020  104.000000  \n",
       "8514  1075.000000   704.569046  1.000000   14.000000   4933.551020   88.222222  \n",
       "8515  1041.000000  1946.010560  1.000000    0.000000    350.000000  150.000000  \n",
       "\n",
       "[8516 rows x 99 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"./data_train_pso.csv\")\n",
    "df_train.drop(columns=[\"Unnamed: 0\"],inplace=True)\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>4</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>...</th>\n",
       "      <th>180</th>\n",
       "      <th>185</th>\n",
       "      <th>186</th>\n",
       "      <th>192</th>\n",
       "      <th>196</th>\n",
       "      <th>202</th>\n",
       "      <th>203</th>\n",
       "      <th>204</th>\n",
       "      <th>208</th>\n",
       "      <th>211</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>153.204817</td>\n",
       "      <td>0.996355</td>\n",
       "      <td>0.207174</td>\n",
       "      <td>459.037295</td>\n",
       "      <td>-28.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>358.307692</td>\n",
       "      <td>...</td>\n",
       "      <td>92.686391</td>\n",
       "      <td>554.0</td>\n",
       "      <td>0.999941</td>\n",
       "      <td>10.0</td>\n",
       "      <td>710.615385</td>\n",
       "      <td>794.307350</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>-10.000000</td>\n",
       "      <td>729.000000</td>\n",
       "      <td>127.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>266.399867</td>\n",
       "      <td>0.979352</td>\n",
       "      <td>0.388359</td>\n",
       "      <td>398.464564</td>\n",
       "      <td>-60.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>532.800000</td>\n",
       "      <td>...</td>\n",
       "      <td>7281.937500</td>\n",
       "      <td>932.0</td>\n",
       "      <td>0.965488</td>\n",
       "      <td>4.0</td>\n",
       "      <td>968.222222</td>\n",
       "      <td>900.143486</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>15314.750000</td>\n",
       "      <td>112.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>251.329664</td>\n",
       "      <td>0.260470</td>\n",
       "      <td>-1.002325</td>\n",
       "      <td>340.802438</td>\n",
       "      <td>-52.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>403.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2661.728395</td>\n",
       "      <td>784.0</td>\n",
       "      <td>0.270281</td>\n",
       "      <td>6.0</td>\n",
       "      <td>796.400000</td>\n",
       "      <td>1236.308241</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>1944.489796</td>\n",
       "      <td>131.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>8.986100</td>\n",
       "      <td>0.048579</td>\n",
       "      <td>-1.449012</td>\n",
       "      <td>412.324324</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>322.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>787.638889</td>\n",
       "      <td>26.0</td>\n",
       "      <td>-0.014314</td>\n",
       "      <td>12.0</td>\n",
       "      <td>757.333333</td>\n",
       "      <td>803.828940</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>6122.750000</td>\n",
       "      <td>121.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>82.344017</td>\n",
       "      <td>3.023659</td>\n",
       "      <td>10.404884</td>\n",
       "      <td>168.041577</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>223.750000</td>\n",
       "      <td>...</td>\n",
       "      <td>1288.640000</td>\n",
       "      <td>398.0</td>\n",
       "      <td>1.761865</td>\n",
       "      <td>16.0</td>\n",
       "      <td>396.173913</td>\n",
       "      <td>316.582423</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.022262</td>\n",
       "      <td>0.044242</td>\n",
       "      <td>45.818182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2125</th>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>36.509417</td>\n",
       "      <td>1.263183</td>\n",
       "      <td>0.543003</td>\n",
       "      <td>364.303573</td>\n",
       "      <td>-18.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>438.571429</td>\n",
       "      <td>...</td>\n",
       "      <td>4051.918367</td>\n",
       "      <td>118.0</td>\n",
       "      <td>1.263183</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1071.250000</td>\n",
       "      <td>856.643246</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2843.265306</td>\n",
       "      <td>96.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2126</th>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>33.839959</td>\n",
       "      <td>-0.454057</td>\n",
       "      <td>-1.036905</td>\n",
       "      <td>181.876516</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>463.142857</td>\n",
       "      <td>...</td>\n",
       "      <td>10.750000</td>\n",
       "      <td>104.0</td>\n",
       "      <td>-0.457057</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1196.000000</td>\n",
       "      <td>746.905354</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-26.000000</td>\n",
       "      <td>228.555556</td>\n",
       "      <td>169.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2127</th>\n",
       "      <td>3.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>23.734082</td>\n",
       "      <td>0.371174</td>\n",
       "      <td>-0.657132</td>\n",
       "      <td>137.696567</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>335.733333</td>\n",
       "      <td>...</td>\n",
       "      <td>739.982222</td>\n",
       "      <td>82.0</td>\n",
       "      <td>0.348673</td>\n",
       "      <td>14.0</td>\n",
       "      <td>595.600000</td>\n",
       "      <td>333.718093</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-8.000000</td>\n",
       "      <td>1270.061224</td>\n",
       "      <td>90.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2128</th>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>180.470587</td>\n",
       "      <td>0.587475</td>\n",
       "      <td>-1.363827</td>\n",
       "      <td>561.988537</td>\n",
       "      <td>12.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>462.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>431.750000</td>\n",
       "      <td>448.0</td>\n",
       "      <td>0.587475</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1080.571429</td>\n",
       "      <td>588.205240</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>51.840000</td>\n",
       "      <td>101.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2129</th>\n",
       "      <td>3.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2.569857</td>\n",
       "      <td>0.605786</td>\n",
       "      <td>-0.869886</td>\n",
       "      <td>654.123072</td>\n",
       "      <td>46.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.036070</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045933</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.989051</td>\n",
       "      <td>0.0</td>\n",
       "      <td>391.250000</td>\n",
       "      <td>1293.658260</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.044242</td>\n",
       "      <td>0.037385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2130 rows × 99 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0     1           4         6          7           8    10    11  \\\n",
       "0     0.0  14.0  153.204817  0.996355   0.207174  459.037295 -28.0  13.0   \n",
       "1     0.0  10.0  266.399867  0.979352   0.388359  398.464564 -60.0  10.0   \n",
       "2     0.0  11.0  251.329664  0.260470  -1.002325  340.802438 -52.0   8.0   \n",
       "3     2.0  13.0    8.986100  0.048579  -1.449012  412.324324  -5.0  12.0   \n",
       "4     0.0  23.0   82.344017  3.023659  10.404884  168.041577   7.0   9.0   \n",
       "...   ...   ...         ...       ...        ...         ...   ...   ...   \n",
       "2125  1.0   9.0   36.509417  1.263183   0.543003  364.303573 -18.0   9.0   \n",
       "2126  1.0   8.0   33.839959 -0.454057  -1.036905  181.876516   0.0   8.0   \n",
       "2127  3.0  16.0   23.734082  0.371174  -0.657132  137.696567 -10.0  16.0   \n",
       "2128  1.0   8.0  180.470587  0.587475  -1.363827  561.988537  12.0   8.0   \n",
       "2129  3.0  25.0    2.569857  0.605786  -0.869886  654.123072  46.0   0.0   \n",
       "\n",
       "        12          13  ...          180    185       186   192          196  \\\n",
       "0     13.0  358.307692  ...    92.686391  554.0  0.999941  10.0   710.615385   \n",
       "1      6.0  532.800000  ...  7281.937500  932.0  0.965488   4.0   968.222222   \n",
       "2      7.0  403.000000  ...  2661.728395  784.0  0.270281   6.0   796.400000   \n",
       "3     12.0  322.333333  ...   787.638889   26.0 -0.014314  12.0   757.333333   \n",
       "4     20.0  223.750000  ...  1288.640000  398.0  1.761865  16.0   396.173913   \n",
       "...    ...         ...  ...          ...    ...       ...   ...          ...   \n",
       "2125   8.0  438.571429  ...  4051.918367  118.0  1.263183   8.0  1071.250000   \n",
       "2126   7.0  463.142857  ...    10.750000  104.0 -0.457057   5.0  1196.000000   \n",
       "2127  15.0  335.733333  ...   739.982222   82.0  0.348673  14.0   595.600000   \n",
       "2128   7.0  462.000000  ...   431.750000  448.0  0.587475   5.0  1080.571429   \n",
       "2129   5.0    0.036070  ...     0.045933   10.0  0.989051   0.0   391.250000   \n",
       "\n",
       "              202       203        204           208         211  \n",
       "0      794.307350  0.928571 -10.000000    729.000000  127.600000  \n",
       "1      900.143486  0.600000  64.000000  15314.750000  112.285714  \n",
       "2     1236.308241  1.000000  26.000000   1944.489796  131.111111  \n",
       "3      803.828940  1.000000  -4.000000   6122.750000  121.833333  \n",
       "4      316.582423  0.083333   0.022262      0.044242   45.818182  \n",
       "...           ...       ...        ...           ...         ...  \n",
       "2125   856.643246  0.777778   0.000000   2843.265306   96.000000  \n",
       "2126   746.905354  1.000000 -26.000000    228.555556  169.142857  \n",
       "2127   333.718093  1.000000  -8.000000   1270.061224   90.400000  \n",
       "2128   588.205240  1.000000  18.000000     51.840000  101.000000  \n",
       "2129  1293.658260  0.240000   4.000000      0.044242    0.037385  \n",
       "\n",
       "[2130 rows x 99 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv(\"./data_test_pso.csv\")\n",
    "df_test.drop(columns=[\"Unnamed: 0\"],inplace=True)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = df_train.iloc[:,1:].values\n",
    "y_train = df_train.iloc[:,0].values\n",
    "x_test = df_test.iloc[:,1:].values\n",
    "y_test = df_test.iloc[:,0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = MinMaxScaler()\n",
    "x_train = scale.fit_transform(x_train)\n",
    "x_test = scale.fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (4258, 98)\n",
      "Vallidation: (4258, 98)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train , test_size=0.5, shuffle=True, stratify=y_train, random_state=119)\n",
    "print(f\"Train: {x_train.shape}\")\n",
    "print(f\"Vallidation: {x_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier(criterion= 'log_loss', max_depth= 5, max_features= 'sqrt', n_estimators= 1000)\n",
    "ab_clf = AdaBoostClassifier(algorithm= 'SAMME.R', learning_rate= 0.1, n_estimators= 50)\n",
    "knn_clf = KNeighborsClassifier(algorithm= 'auto', n_neighbors= 5, p= 1, weights= 'uniform')\n",
    "svc_clf = SVC(C= 100, gamma= 'scale', kernel= 'rbf', probability= True)\n",
    "xgb_clf = XGBClassifier(gamma= 0,learning_rate= 0.1,max_depth= 5,min_child_weight= 1,n_estimators= 1000)\n",
    "dt_clf = DecisionTreeClassifier(criterion= 'entropy',max_depth= 5,max_features= 'sqrt',splitter= 'best')\n",
    "lgb_clf = LGBMClassifier(boosting = 'gbdt', data_sample_strategy= 'goss', estimators=50, learning_rate = 0.1, objective= 'multiclass')\n",
    "cb_clf = CatBoostClassifier(iterations = 10, learning_rate= 0.1)\n",
    "gb_clf = GradientBoostingClassifier(criterion='squared_error', learning_rate = 0.1, loss= 'log_loss', n_estimators=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(criterion=&#x27;entropy&#x27;, max_depth=5, max_features=&#x27;sqrt&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;DecisionTreeClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.tree.DecisionTreeClassifier.html\">?<span>Documentation for DecisionTreeClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>DecisionTreeClassifier(criterion=&#x27;entropy&#x27;, max_depth=5, max_features=&#x27;sqrt&#x27;)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier(criterion='entropy', max_depth=5, max_features='sqrt')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Huấn luyện các mô hình con\n",
    "rf_clf.fit(x_train,y_train)\n",
    "ab_clf.fit(x_train, y_train)\n",
    "knn_clf.fit(x_train, y_train)\n",
    "svc_clf.fit(x_train, y_train)\n",
    "xgb_clf.fit(x_train, y_train)\n",
    "dt_clf.fit(x_train,y_train)\n",
    "# lgb_clf.fit(x_train, y_train)\n",
    "# cb_clf.fit(x_train, y_train)\n",
    "# gb_clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dự đoán trên tập huấn luyện để tạo đặc trưng mới cho mô hình blending\n",
    "X_train_meta = np.column_stack((\n",
    "    rf_clf.predict_proba(x_val),\n",
    "    ab_clf.predict_proba(x_val),\n",
    "    knn_clf.predict_proba(x_val),\n",
    "    svc_clf.predict_proba(x_val),\n",
    "    xgb_clf.predict_proba(x_val),\n",
    "    dt_clf.predict_proba(x_val),\n",
    "    # lgb_clf.predict_proba(x_val),\n",
    "    # cb_clf.predict_proba(x_val)\n",
    "    # gb_clf.predict_proba(x_val)\n",
    "))\n",
    "# Dự đoán trên tập kiểm tra để tạo đặc trưng mới cho mô hình blending\n",
    "X_test_meta = np.column_stack((\n",
    "    rf_clf.predict_proba(x_test),\n",
    "    ab_clf.predict_proba(x_test),\n",
    "    knn_clf.predict_proba(x_test),\n",
    "    svc_clf.predict_proba(x_test),\n",
    "    xgb_clf.predict_proba(x_test),\n",
    "    dt_clf.predict_proba(x_test),\n",
    "    # lgb_clf.predict_proba(x_test),\n",
    "    # cb_clf.predict_proba(x_test)\n",
    "    # gb_clf.predict_proba(x_test)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_meta:(4258, 24)\n",
      "X_test_meta:(2130, 24)\n"
     ]
    }
   ],
   "source": [
    "print(f\"X_train_meta:{X_train_meta.shape}\")\n",
    "print(f\"X_test_meta:{X_test_meta.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 180 candidates, totalling 540 fits\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.05, loss=log_loss, n_estimators=50;, score=(train=0.970, test=0.956) total time=   2.2s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.05, loss=log_loss, n_estimators=50;, score=(train=0.972, test=0.960) total time=   2.2s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.05, loss=log_loss, n_estimators=50;, score=(train=0.976, test=0.946) total time=   2.2s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.05, loss=log_loss, n_estimators=75;, score=(train=0.975, test=0.956) total time=   3.6s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.05, loss=log_loss, n_estimators=75;, score=(train=0.976, test=0.958) total time=   3.5s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.05, loss=log_loss, n_estimators=75;, score=(train=0.982, test=0.947) total time=   3.3s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.05, loss=log_loss, n_estimators=100;, score=(train=0.982, test=0.956) total time=   4.8s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.05, loss=log_loss, n_estimators=100;, score=(train=0.981, test=0.957) total time=   4.5s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.05, loss=log_loss, n_estimators=100;, score=(train=0.987, test=0.949) total time=   5.8s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.05, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.05, loss=exponential, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.05, loss=exponential, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.05, loss=exponential, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.05, loss=exponential, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.05, loss=exponential, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.05, loss=exponential, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.05, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.05, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.05, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.06666666666666668, loss=log_loss, n_estimators=50;, score=(train=0.974, test=0.956) total time=   2.9s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.06666666666666668, loss=log_loss, n_estimators=50;, score=(train=0.974, test=0.960) total time=   2.8s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.06666666666666668, loss=log_loss, n_estimators=50;, score=(train=0.979, test=0.947) total time=   2.6s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.06666666666666668, loss=log_loss, n_estimators=75;, score=(train=0.984, test=0.958) total time=   3.7s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.06666666666666668, loss=log_loss, n_estimators=75;, score=(train=0.981, test=0.959) total time=   3.5s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.06666666666666668, loss=log_loss, n_estimators=75;, score=(train=0.987, test=0.949) total time=   3.5s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.06666666666666668, loss=log_loss, n_estimators=100;, score=(train=0.992, test=0.956) total time=   4.5s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.06666666666666668, loss=log_loss, n_estimators=100;, score=(train=0.987, test=0.958) total time=   4.6s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.06666666666666668, loss=log_loss, n_estimators=100;, score=(train=0.991, test=0.948) total time=   4.5s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.06666666666666668, loss=deviance, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.06666666666666668, loss=deviance, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.06666666666666668, loss=deviance, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.06666666666666668, loss=deviance, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.06666666666666668, loss=deviance, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.06666666666666668, loss=deviance, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.06666666666666668, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.06666666666666668, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.06666666666666668, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.06666666666666668, loss=exponential, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.06666666666666668, loss=exponential, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.06666666666666668, loss=exponential, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.06666666666666668, loss=exponential, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.06666666666666668, loss=exponential, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.06666666666666668, loss=exponential, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.06666666666666668, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.06666666666666668, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.06666666666666668, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.08333333333333334, loss=log_loss, n_estimators=50;, score=(train=0.978, test=0.956) total time=   2.2s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.08333333333333334, loss=log_loss, n_estimators=50;, score=(train=0.979, test=0.958) total time=   2.2s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.08333333333333334, loss=log_loss, n_estimators=50;, score=(train=0.983, test=0.949) total time=   2.2s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.08333333333333334, loss=log_loss, n_estimators=75;, score=(train=0.988, test=0.957) total time=   3.5s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.08333333333333334, loss=log_loss, n_estimators=75;, score=(train=0.985, test=0.956) total time=   4.5s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.08333333333333334, loss=log_loss, n_estimators=75;, score=(train=0.992, test=0.948) total time=   5.2s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.08333333333333334, loss=log_loss, n_estimators=100;, score=(train=0.995, test=0.957) total time=   6.6s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.08333333333333334, loss=log_loss, n_estimators=100;, score=(train=0.993, test=0.958) total time=   6.4s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.08333333333333334, loss=log_loss, n_estimators=100;, score=(train=0.995, test=0.948) total time=   6.3s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.08333333333333334, loss=deviance, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.08333333333333334, loss=deviance, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.08333333333333334, loss=deviance, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.08333333333333334, loss=deviance, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.08333333333333334, loss=deviance, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.08333333333333334, loss=deviance, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.08333333333333334, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.08333333333333334, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.08333333333333334, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.08333333333333334, loss=exponential, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.08333333333333334, loss=exponential, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.08333333333333334, loss=exponential, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.08333333333333334, loss=exponential, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.08333333333333334, loss=exponential, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.08333333333333334, loss=exponential, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.08333333333333334, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.08333333333333334, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.08333333333333334, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.1, loss=log_loss, n_estimators=50;, score=(train=0.984, test=0.957) total time=   3.2s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.1, loss=log_loss, n_estimators=50;, score=(train=0.983, test=0.958) total time=   3.2s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.1, loss=log_loss, n_estimators=50;, score=(train=0.987, test=0.947) total time=   3.2s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.1, loss=log_loss, n_estimators=75;, score=(train=0.994, test=0.957) total time=   4.8s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.1, loss=log_loss, n_estimators=75;, score=(train=0.992, test=0.958) total time=   4.8s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.1, loss=log_loss, n_estimators=75;, score=(train=0.995, test=0.947) total time=   4.8s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.1, loss=log_loss, n_estimators=100;, score=(train=0.997, test=0.958) total time=   6.4s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.1, loss=log_loss, n_estimators=100;, score=(train=0.996, test=0.958) total time=   6.6s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.1, loss=log_loss, n_estimators=100;, score=(train=0.999, test=0.948) total time=   6.4s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.1, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.1, loss=exponential, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.1, loss=exponential, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.1, loss=exponential, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.1, loss=exponential, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.1, loss=exponential, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.1, loss=exponential, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.1, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.1, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.1, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.11666666666666668, loss=log_loss, n_estimators=50;, score=(train=0.990, test=0.958) total time=   3.5s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.11666666666666668, loss=log_loss, n_estimators=50;, score=(train=0.986, test=0.959) total time=   3.2s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.11666666666666668, loss=log_loss, n_estimators=50;, score=(train=0.993, test=0.949) total time=   3.2s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.11666666666666668, loss=log_loss, n_estimators=75;, score=(train=0.996, test=0.958) total time=   4.8s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.11666666666666668, loss=log_loss, n_estimators=75;, score=(train=0.994, test=0.959) total time=   4.8s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.11666666666666668, loss=log_loss, n_estimators=75;, score=(train=0.998, test=0.949) total time=   4.8s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.11666666666666668, loss=log_loss, n_estimators=100;, score=(train=0.999, test=0.959) total time=   6.4s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.11666666666666668, loss=log_loss, n_estimators=100;, score=(train=0.998, test=0.960) total time=   6.4s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.11666666666666668, loss=log_loss, n_estimators=100;, score=(train=1.000, test=0.949) total time=   6.7s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.11666666666666668, loss=deviance, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.11666666666666668, loss=deviance, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.11666666666666668, loss=deviance, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.11666666666666668, loss=deviance, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.11666666666666668, loss=deviance, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.11666666666666668, loss=deviance, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.11666666666666668, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.11666666666666668, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.11666666666666668, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.11666666666666668, loss=exponential, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.11666666666666668, loss=exponential, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.11666666666666668, loss=exponential, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.11666666666666668, loss=exponential, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.11666666666666668, loss=exponential, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.11666666666666668, loss=exponential, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.11666666666666668, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.11666666666666668, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.11666666666666668, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.13333333333333336, loss=log_loss, n_estimators=50;, score=(train=0.992, test=0.958) total time=   3.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.13333333333333336, loss=log_loss, n_estimators=50;, score=(train=0.990, test=0.959) total time=   2.3s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.13333333333333336, loss=log_loss, n_estimators=50;, score=(train=0.995, test=0.945) total time=   2.2s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.13333333333333336, loss=log_loss, n_estimators=75;, score=(train=0.999, test=0.958) total time=   3.6s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.13333333333333336, loss=log_loss, n_estimators=75;, score=(train=0.997, test=0.960) total time=   3.5s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.13333333333333336, loss=log_loss, n_estimators=75;, score=(train=0.999, test=0.946) total time=   3.5s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.13333333333333336, loss=log_loss, n_estimators=100;, score=(train=1.000, test=0.957) total time=   4.9s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.13333333333333336, loss=log_loss, n_estimators=100;, score=(train=1.000, test=0.958) total time=   4.8s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.13333333333333336, loss=log_loss, n_estimators=100;, score=(train=1.000, test=0.948) total time=   4.7s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.13333333333333336, loss=deviance, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.13333333333333336, loss=deviance, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.13333333333333336, loss=deviance, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.13333333333333336, loss=deviance, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.13333333333333336, loss=deviance, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.13333333333333336, loss=deviance, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.13333333333333336, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.13333333333333336, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.13333333333333336, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.13333333333333336, loss=exponential, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.13333333333333336, loss=exponential, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.13333333333333336, loss=exponential, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.13333333333333336, loss=exponential, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.13333333333333336, loss=exponential, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.13333333333333336, loss=exponential, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.13333333333333336, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.13333333333333336, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.13333333333333336, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.15000000000000002, loss=log_loss, n_estimators=50;, score=(train=0.994, test=0.960) total time=   2.3s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.15000000000000002, loss=log_loss, n_estimators=50;, score=(train=0.993, test=0.956) total time=   2.2s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.15000000000000002, loss=log_loss, n_estimators=50;, score=(train=0.996, test=0.947) total time=   2.3s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.15000000000000002, loss=log_loss, n_estimators=75;, score=(train=0.999, test=0.960) total time=   3.5s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.15000000000000002, loss=log_loss, n_estimators=75;, score=(train=0.997, test=0.956) total time=   5.6s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.15000000000000002, loss=log_loss, n_estimators=75;, score=(train=0.999, test=0.946) total time=   5.7s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.15000000000000002, loss=log_loss, n_estimators=100;, score=(train=1.000, test=0.959) total time=   4.8s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.15000000000000002, loss=log_loss, n_estimators=100;, score=(train=1.000, test=0.956) total time=   4.7s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.15000000000000002, loss=log_loss, n_estimators=100;, score=(train=1.000, test=0.945) total time=   4.7s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.15000000000000002, loss=deviance, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.15000000000000002, loss=deviance, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.15000000000000002, loss=deviance, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.15000000000000002, loss=deviance, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.15000000000000002, loss=deviance, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.15000000000000002, loss=deviance, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.15000000000000002, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.15000000000000002, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.15000000000000002, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.15000000000000002, loss=exponential, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.15000000000000002, loss=exponential, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.15000000000000002, loss=exponential, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.15000000000000002, loss=exponential, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.15000000000000002, loss=exponential, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.15000000000000002, loss=exponential, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.15000000000000002, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.15000000000000002, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.15000000000000002, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.16666666666666669, loss=log_loss, n_estimators=50;, score=(train=0.996, test=0.958) total time=   2.3s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.16666666666666669, loss=log_loss, n_estimators=50;, score=(train=0.994, test=0.957) total time=   2.3s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.16666666666666669, loss=log_loss, n_estimators=50;, score=(train=0.998, test=0.947) total time=   2.3s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.16666666666666669, loss=log_loss, n_estimators=75;, score=(train=1.000, test=0.958) total time=   3.5s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.16666666666666669, loss=log_loss, n_estimators=75;, score=(train=0.999, test=0.957) total time=   3.4s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.16666666666666669, loss=log_loss, n_estimators=75;, score=(train=1.000, test=0.945) total time=   3.5s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.16666666666666669, loss=log_loss, n_estimators=100;, score=(train=1.000, test=0.958) total time=   4.7s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.16666666666666669, loss=log_loss, n_estimators=100;, score=(train=1.000, test=0.957) total time=   7.7s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.16666666666666669, loss=log_loss, n_estimators=100;, score=(train=1.000, test=0.946) total time=   8.1s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.16666666666666669, loss=deviance, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.16666666666666669, loss=deviance, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.16666666666666669, loss=deviance, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.16666666666666669, loss=deviance, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.16666666666666669, loss=deviance, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.16666666666666669, loss=deviance, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.16666666666666669, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.16666666666666669, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.16666666666666669, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.16666666666666669, loss=exponential, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.16666666666666669, loss=exponential, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.16666666666666669, loss=exponential, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.16666666666666669, loss=exponential, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.16666666666666669, loss=exponential, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.16666666666666669, loss=exponential, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.16666666666666669, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.16666666666666669, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.16666666666666669, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.18333333333333335, loss=log_loss, n_estimators=50;, score=(train=0.998, test=0.958) total time=   3.9s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.18333333333333335, loss=log_loss, n_estimators=50;, score=(train=0.997, test=0.957) total time=   4.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.18333333333333335, loss=log_loss, n_estimators=50;, score=(train=0.999, test=0.948) total time=   3.9s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.18333333333333335, loss=log_loss, n_estimators=75;, score=(train=1.000, test=0.958) total time=   6.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.18333333333333335, loss=log_loss, n_estimators=75;, score=(train=1.000, test=0.956) total time=   5.9s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.18333333333333335, loss=log_loss, n_estimators=75;, score=(train=1.000, test=0.948) total time=   6.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.18333333333333335, loss=log_loss, n_estimators=100;, score=(train=1.000, test=0.961) total time=   8.1s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.18333333333333335, loss=log_loss, n_estimators=100;, score=(train=1.000, test=0.958) total time=   8.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.18333333333333335, loss=log_loss, n_estimators=100;, score=(train=1.000, test=0.949) total time=   8.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.18333333333333335, loss=deviance, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.18333333333333335, loss=deviance, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.18333333333333335, loss=deviance, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.18333333333333335, loss=deviance, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.18333333333333335, loss=deviance, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.18333333333333335, loss=deviance, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.18333333333333335, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.18333333333333335, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.18333333333333335, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.18333333333333335, loss=exponential, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.18333333333333335, loss=exponential, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.18333333333333335, loss=exponential, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.18333333333333335, loss=exponential, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.18333333333333335, loss=exponential, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.18333333333333335, loss=exponential, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.18333333333333335, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.18333333333333335, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.18333333333333335, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.2, loss=log_loss, n_estimators=50;, score=(train=0.999, test=0.956) total time=   3.9s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.2, loss=log_loss, n_estimators=50;, score=(train=0.998, test=0.958) total time=   3.9s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.2, loss=log_loss, n_estimators=50;, score=(train=0.999, test=0.944) total time=   3.9s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.2, loss=log_loss, n_estimators=75;, score=(train=1.000, test=0.960) total time=   5.9s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.2, loss=log_loss, n_estimators=75;, score=(train=1.000, test=0.958) total time=   5.9s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.2, loss=log_loss, n_estimators=75;, score=(train=1.000, test=0.944) total time=   6.5s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.2, loss=log_loss, n_estimators=100;, score=(train=1.000, test=0.959) total time=   8.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.2, loss=log_loss, n_estimators=100;, score=(train=1.000, test=0.955) total time=   7.8s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.2, loss=log_loss, n_estimators=100;, score=(train=1.000, test=0.946) total time=   7.9s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.2, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.2, loss=exponential, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.2, loss=exponential, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.2, loss=exponential, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.2, loss=exponential, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.2, loss=exponential, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.2, loss=exponential, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=friedman_mse, learning_rate=0.2, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=friedman_mse, learning_rate=0.2, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=friedman_mse, learning_rate=0.2, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.05, loss=log_loss, n_estimators=50;, score=(train=0.970, test=0.956) total time=   3.9s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.05, loss=log_loss, n_estimators=50;, score=(train=0.971, test=0.959) total time=   3.9s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.05, loss=log_loss, n_estimators=50;, score=(train=0.976, test=0.947) total time=   3.9s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.05, loss=log_loss, n_estimators=75;, score=(train=0.975, test=0.956) total time=   5.8s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.05, loss=log_loss, n_estimators=75;, score=(train=0.976, test=0.958) total time=   5.9s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.05, loss=log_loss, n_estimators=75;, score=(train=0.982, test=0.949) total time=   5.9s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.05, loss=log_loss, n_estimators=100;, score=(train=0.982, test=0.956) total time=   7.6s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.05, loss=log_loss, n_estimators=100;, score=(train=0.981, test=0.957) total time=   5.3s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.05, loss=log_loss, n_estimators=100;, score=(train=0.987, test=0.950) total time=   4.9s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.05, loss=deviance, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.05, loss=deviance, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.05, loss=deviance, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.05, loss=deviance, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.05, loss=deviance, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.05, loss=deviance, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.05, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.05, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.05, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.05, loss=exponential, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.05, loss=exponential, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.05, loss=exponential, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.05, loss=exponential, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.05, loss=exponential, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.05, loss=exponential, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.05, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.05, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.05, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.06666666666666668, loss=log_loss, n_estimators=50;, score=(train=0.974, test=0.956) total time=   2.4s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.06666666666666668, loss=log_loss, n_estimators=50;, score=(train=0.974, test=0.960) total time=   2.4s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.06666666666666668, loss=log_loss, n_estimators=50;, score=(train=0.979, test=0.948) total time=   2.3s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.06666666666666668, loss=log_loss, n_estimators=75;, score=(train=0.984, test=0.957) total time=   4.5s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.06666666666666668, loss=log_loss, n_estimators=75;, score=(train=0.981, test=0.959) total time=   4.4s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.06666666666666668, loss=log_loss, n_estimators=75;, score=(train=0.985, test=0.950) total time=   3.8s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.06666666666666668, loss=log_loss, n_estimators=100;, score=(train=0.992, test=0.955) total time=   5.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.06666666666666668, loss=log_loss, n_estimators=100;, score=(train=0.987, test=0.958) total time=   4.8s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.06666666666666668, loss=log_loss, n_estimators=100;, score=(train=0.991, test=0.949) total time=   4.7s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.06666666666666668, loss=deviance, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.06666666666666668, loss=deviance, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.06666666666666668, loss=deviance, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.06666666666666668, loss=deviance, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.06666666666666668, loss=deviance, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.06666666666666668, loss=deviance, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.06666666666666668, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.06666666666666668, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.06666666666666668, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.06666666666666668, loss=exponential, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.06666666666666668, loss=exponential, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.06666666666666668, loss=exponential, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.06666666666666668, loss=exponential, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.06666666666666668, loss=exponential, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.06666666666666668, loss=exponential, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.06666666666666668, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.06666666666666668, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.06666666666666668, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.08333333333333334, loss=log_loss, n_estimators=50;, score=(train=0.978, test=0.956) total time=   2.3s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.08333333333333334, loss=log_loss, n_estimators=50;, score=(train=0.979, test=0.958) total time=   2.3s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.08333333333333334, loss=log_loss, n_estimators=50;, score=(train=0.984, test=0.948) total time=   2.4s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.08333333333333334, loss=log_loss, n_estimators=75;, score=(train=0.990, test=0.957) total time=   3.5s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.08333333333333334, loss=log_loss, n_estimators=75;, score=(train=0.985, test=0.956) total time=   3.5s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.08333333333333334, loss=log_loss, n_estimators=75;, score=(train=0.992, test=0.949) total time=   3.5s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.08333333333333334, loss=log_loss, n_estimators=100;, score=(train=0.995, test=0.959) total time=   5.4s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.08333333333333334, loss=log_loss, n_estimators=100;, score=(train=0.993, test=0.958) total time=   5.4s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.08333333333333334, loss=log_loss, n_estimators=100;, score=(train=0.995, test=0.947) total time=   5.4s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.08333333333333334, loss=deviance, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.08333333333333334, loss=deviance, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.08333333333333334, loss=deviance, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.08333333333333334, loss=deviance, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.08333333333333334, loss=deviance, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.08333333333333334, loss=deviance, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.08333333333333334, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.08333333333333334, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.08333333333333334, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.08333333333333334, loss=exponential, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.08333333333333334, loss=exponential, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.08333333333333334, loss=exponential, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.08333333333333334, loss=exponential, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.08333333333333334, loss=exponential, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.08333333333333334, loss=exponential, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.08333333333333334, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.08333333333333334, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.08333333333333334, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.1, loss=log_loss, n_estimators=50;, score=(train=0.982, test=0.956) total time=   2.6s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.1, loss=log_loss, n_estimators=50;, score=(train=0.983, test=0.958) total time=   2.6s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.1, loss=log_loss, n_estimators=50;, score=(train=0.988, test=0.948) total time=   2.6s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.1, loss=log_loss, n_estimators=75;, score=(train=0.994, test=0.958) total time=   4.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.1, loss=log_loss, n_estimators=75;, score=(train=0.992, test=0.958) total time=   4.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.1, loss=log_loss, n_estimators=75;, score=(train=0.995, test=0.949) total time=   3.7s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.1, loss=log_loss, n_estimators=100;, score=(train=0.997, test=0.958) total time=   4.7s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.1, loss=log_loss, n_estimators=100;, score=(train=0.996, test=0.958) total time=   5.1s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.1, loss=log_loss, n_estimators=100;, score=(train=0.999, test=0.949) total time=   5.4s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.1, loss=deviance, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.1, loss=deviance, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.1, loss=deviance, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.1, loss=deviance, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.1, loss=deviance, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.1, loss=deviance, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.1, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.1, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.1, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.1, loss=exponential, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.1, loss=exponential, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.1, loss=exponential, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.1, loss=exponential, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.1, loss=exponential, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.1, loss=exponential, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.1, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.1, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.1, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.11666666666666668, loss=log_loss, n_estimators=50;, score=(train=0.990, test=0.958) total time=   2.2s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.11666666666666668, loss=log_loss, n_estimators=50;, score=(train=0.985, test=0.958) total time=   2.2s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.11666666666666668, loss=log_loss, n_estimators=50;, score=(train=0.992, test=0.951) total time=   2.4s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.11666666666666668, loss=log_loss, n_estimators=75;, score=(train=0.996, test=0.958) total time=   4.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.11666666666666668, loss=log_loss, n_estimators=75;, score=(train=0.994, test=0.959) total time=   4.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.11666666666666668, loss=log_loss, n_estimators=75;, score=(train=0.997, test=0.951) total time=   4.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.11666666666666668, loss=log_loss, n_estimators=100;, score=(train=0.999, test=0.958) total time=   5.3s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.11666666666666668, loss=log_loss, n_estimators=100;, score=(train=0.998, test=0.958) total time=   4.5s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.11666666666666668, loss=log_loss, n_estimators=100;, score=(train=1.000, test=0.950) total time=   5.4s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.11666666666666668, loss=deviance, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.11666666666666668, loss=deviance, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.11666666666666668, loss=deviance, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.11666666666666668, loss=deviance, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.11666666666666668, loss=deviance, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.11666666666666668, loss=deviance, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.11666666666666668, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.11666666666666668, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.11666666666666668, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.11666666666666668, loss=exponential, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.11666666666666668, loss=exponential, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.11666666666666668, loss=exponential, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.11666666666666668, loss=exponential, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.11666666666666668, loss=exponential, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.11666666666666668, loss=exponential, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.11666666666666668, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.11666666666666668, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.11666666666666668, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.13333333333333336, loss=log_loss, n_estimators=50;, score=(train=0.993, test=0.958) total time=   2.9s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.13333333333333336, loss=log_loss, n_estimators=50;, score=(train=0.990, test=0.959) total time=   2.8s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.13333333333333336, loss=log_loss, n_estimators=50;, score=(train=0.994, test=0.947) total time=   2.3s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.13333333333333336, loss=log_loss, n_estimators=75;, score=(train=0.999, test=0.958) total time=   3.5s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.13333333333333336, loss=log_loss, n_estimators=75;, score=(train=0.997, test=0.961) total time=   4.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.13333333333333336, loss=log_loss, n_estimators=75;, score=(train=0.998, test=0.949) total time=   4.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.13333333333333336, loss=log_loss, n_estimators=100;, score=(train=1.000, test=0.958) total time=   5.1s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.13333333333333336, loss=log_loss, n_estimators=100;, score=(train=1.000, test=0.959) total time=   4.7s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.13333333333333336, loss=log_loss, n_estimators=100;, score=(train=1.000, test=0.950) total time=   5.4s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.13333333333333336, loss=deviance, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.13333333333333336, loss=deviance, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.13333333333333336, loss=deviance, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.13333333333333336, loss=deviance, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.13333333333333336, loss=deviance, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.13333333333333336, loss=deviance, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.13333333333333336, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.13333333333333336, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.13333333333333336, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.13333333333333336, loss=exponential, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.13333333333333336, loss=exponential, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.13333333333333336, loss=exponential, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.13333333333333336, loss=exponential, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.13333333333333336, loss=exponential, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.13333333333333336, loss=exponential, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.13333333333333336, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.13333333333333336, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.13333333333333336, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.15000000000000002, loss=log_loss, n_estimators=50;, score=(train=0.995, test=0.956) total time=   2.6s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.15000000000000002, loss=log_loss, n_estimators=50;, score=(train=0.993, test=0.956) total time=   2.6s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.15000000000000002, loss=log_loss, n_estimators=50;, score=(train=0.996, test=0.946) total time=   2.6s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.15000000000000002, loss=log_loss, n_estimators=75;, score=(train=1.000, test=0.958) total time=   4.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.15000000000000002, loss=log_loss, n_estimators=75;, score=(train=0.997, test=0.956) total time=   4.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.15000000000000002, loss=log_loss, n_estimators=75;, score=(train=1.000, test=0.944) total time=   4.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.15000000000000002, loss=log_loss, n_estimators=100;, score=(train=1.000, test=0.958) total time=   4.6s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.15000000000000002, loss=log_loss, n_estimators=100;, score=(train=1.000, test=0.956) total time=   4.5s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.15000000000000002, loss=log_loss, n_estimators=100;, score=(train=1.000, test=0.946) total time=   4.5s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.15000000000000002, loss=deviance, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.15000000000000002, loss=deviance, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.15000000000000002, loss=deviance, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.15000000000000002, loss=deviance, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.15000000000000002, loss=deviance, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.15000000000000002, loss=deviance, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.15000000000000002, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.15000000000000002, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.15000000000000002, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.15000000000000002, loss=exponential, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.15000000000000002, loss=exponential, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.15000000000000002, loss=exponential, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.15000000000000002, loss=exponential, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.15000000000000002, loss=exponential, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.15000000000000002, loss=exponential, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.15000000000000002, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.15000000000000002, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.15000000000000002, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.16666666666666669, loss=log_loss, n_estimators=50;, score=(train=0.996, test=0.959) total time=   2.2s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.16666666666666669, loss=log_loss, n_estimators=50;, score=(train=0.994, test=0.955) total time=   2.2s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.16666666666666669, loss=log_loss, n_estimators=50;, score=(train=0.998, test=0.950) total time=   2.2s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.16666666666666669, loss=log_loss, n_estimators=75;, score=(train=1.000, test=0.957) total time=   3.3s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.16666666666666669, loss=log_loss, n_estimators=75;, score=(train=0.999, test=0.957) total time=   3.3s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.16666666666666669, loss=log_loss, n_estimators=75;, score=(train=1.000, test=0.948) total time=   3.3s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.16666666666666669, loss=log_loss, n_estimators=100;, score=(train=1.000, test=0.959) total time=   4.5s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.16666666666666669, loss=log_loss, n_estimators=100;, score=(train=1.000, test=0.957) total time=   4.5s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.16666666666666669, loss=log_loss, n_estimators=100;, score=(train=1.000, test=0.949) total time=   4.5s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.16666666666666669, loss=deviance, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.16666666666666669, loss=deviance, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.16666666666666669, loss=deviance, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.16666666666666669, loss=deviance, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.16666666666666669, loss=deviance, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.16666666666666669, loss=deviance, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.16666666666666669, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.16666666666666669, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.16666666666666669, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.16666666666666669, loss=exponential, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.16666666666666669, loss=exponential, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.16666666666666669, loss=exponential, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.16666666666666669, loss=exponential, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.16666666666666669, loss=exponential, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.16666666666666669, loss=exponential, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.16666666666666669, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.16666666666666669, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.16666666666666669, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.18333333333333335, loss=log_loss, n_estimators=50;, score=(train=0.999, test=0.959) total time=   2.2s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.18333333333333335, loss=log_loss, n_estimators=50;, score=(train=0.997, test=0.956) total time=   2.2s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.18333333333333335, loss=log_loss, n_estimators=50;, score=(train=0.998, test=0.943) total time=   2.2s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.18333333333333335, loss=log_loss, n_estimators=75;, score=(train=1.000, test=0.958) total time=   3.4s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.18333333333333335, loss=log_loss, n_estimators=75;, score=(train=1.000, test=0.956) total time=   3.3s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.18333333333333335, loss=log_loss, n_estimators=75;, score=(train=1.000, test=0.944) total time=   3.4s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.18333333333333335, loss=log_loss, n_estimators=100;, score=(train=1.000, test=0.957) total time=   4.5s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.18333333333333335, loss=log_loss, n_estimators=100;, score=(train=1.000, test=0.956) total time=   4.5s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.18333333333333335, loss=log_loss, n_estimators=100;, score=(train=1.000, test=0.946) total time=   4.5s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.18333333333333335, loss=deviance, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.18333333333333335, loss=deviance, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.18333333333333335, loss=deviance, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.18333333333333335, loss=deviance, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.18333333333333335, loss=deviance, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.18333333333333335, loss=deviance, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.18333333333333335, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.18333333333333335, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.18333333333333335, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.18333333333333335, loss=exponential, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.18333333333333335, loss=exponential, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.18333333333333335, loss=exponential, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.18333333333333335, loss=exponential, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.18333333333333335, loss=exponential, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.18333333333333335, loss=exponential, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.18333333333333335, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.18333333333333335, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.18333333333333335, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.2, loss=log_loss, n_estimators=50;, score=(train=0.999, test=0.959) total time=   2.2s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.2, loss=log_loss, n_estimators=50;, score=(train=0.998, test=0.958) total time=   2.2s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.2, loss=log_loss, n_estimators=50;, score=(train=0.999, test=0.941) total time=   2.2s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.2, loss=log_loss, n_estimators=75;, score=(train=1.000, test=0.957) total time=   3.4s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.2, loss=log_loss, n_estimators=75;, score=(train=1.000, test=0.958) total time=   3.5s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.2, loss=log_loss, n_estimators=75;, score=(train=1.000, test=0.944) total time=   3.4s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.2, loss=log_loss, n_estimators=100;, score=(train=1.000, test=0.956) total time=   4.5s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.2, loss=log_loss, n_estimators=100;, score=(train=1.000, test=0.954) total time=   4.5s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.2, loss=log_loss, n_estimators=100;, score=(train=1.000, test=0.945) total time=   4.6s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.2, loss=deviance, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.2, loss=deviance, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.2, loss=deviance, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.2, loss=deviance, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.2, loss=deviance, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.2, loss=deviance, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.2, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.2, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.2, loss=deviance, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.2, loss=exponential, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.2, loss=exponential, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.2, loss=exponential, n_estimators=50;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.2, loss=exponential, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.2, loss=exponential, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.2, loss=exponential, n_estimators=75;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=squared_error, learning_rate=0.2, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=squared_error, learning_rate=0.2, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=squared_error, learning_rate=0.2, loss=exponential, n_estimators=100;, score=(train=nan, test=nan) total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_validation.py:542: FitFailedWarning: \n",
      "360 fits failed out of a total of 540.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "180 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_validation.py\", line 890, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\base.py\", line 1344, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of GradientBoostingClassifier must be a str among {'log_loss', 'exponential'}. Got 'deviance' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "180 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_validation.py\", line 890, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\base.py\", line 1351, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\ensemble\\_gb.py\", line 673, in fit\n",
      "    self._loss = self._get_loss(sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\ensemble\\_gb.py\", line 1537, in _get_loss\n",
      "    raise ValueError(\n",
      "ValueError: loss='exponential' is only suitable for a binary classification problem, you have n_classes=4. Please use loss='log_loss' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the test scores are non-finite: [0.95420335 0.95373354 0.95396844        nan        nan        nan\n",
      "        nan        nan        nan 0.95443826 0.95537756 0.9537337\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.95467316 0.95373337 0.95443809        nan        nan        nan\n",
      "        nan        nan        nan 0.95420318 0.95396828 0.95490758\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.95537756 0.95537739 0.95584704        nan        nan        nan\n",
      "        nan        nan        nan 0.95396811 0.95443793 0.95443809\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.95443743 0.95396762 0.95349797        nan        nan        nan\n",
      "        nan        nan        nan 0.95420285 0.95349813 0.95396795\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.95420302 0.95396795 0.95584671        nan        nan        nan\n",
      "        nan        nan        nan 0.952559   0.95373271 0.95349797\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.95420335 0.95420335 0.95443826        nan        nan        nan\n",
      "        nan        nan        nan 0.95467316 0.95537772 0.95373387\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.95396861 0.95396828 0.95490741        nan        nan        nan\n",
      "        nan        nan        nan 0.95420335 0.95490774 0.95490758\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.9556123  0.95631702 0.9556123         nan        nan        nan\n",
      "        nan        nan        nan 0.95467283 0.95584737 0.95561246\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.95279391 0.95279341 0.95349813        nan        nan        nan\n",
      "        nan        nan        nan 0.9546725  0.95396828 0.95514232\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.95279325 0.95279358 0.95302865        nan        nan        nan\n",
      "        nan        nan        nan 0.95255834 0.95279374 0.95185428\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the train scores are non-finite: [0.9728743  0.9779236  0.98344281        nan        nan        nan\n",
      "        nan        nan        nan 0.97534033 0.98379525 0.98990155\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.98003731 0.98837482 0.99424622        nan        nan        nan\n",
      "        nan        nan        nan 0.98485192 0.99377645 0.99753401\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.98966652 0.99624235 0.99906062        nan        nan        nan\n",
      "        nan        nan        nan 0.99236725 0.99835615 0.99988259\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.99459838 0.99859097 0.99988259        nan        nan        nan\n",
      "        nan        nan        nan 0.99600753 0.9994129  1.\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.99788637 0.99976513 1.                nan        nan        nan\n",
      "        nan        nan        nan 0.99859093 0.99976513 1.\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.97252206 0.9779236  0.98344281        nan        nan        nan\n",
      "        nan        nan        nan 0.97557515 0.9833256  0.98990155\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.98027214 0.98907946 0.99424622        nan        nan        nan\n",
      "        nan        nan        nan 0.98449952 0.9938939  0.99729919\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.98860981 0.99589016 0.99894321        nan        nan        nan\n",
      "        nan        nan        nan 0.99236734 0.99800391 0.99988259\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.99495074 0.99894329 0.99988259        nan        nan        nan\n",
      "        nan        nan        nan 0.99589008 0.9994129  1.\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.99800387 0.99988259 1.                nan        nan        nan\n",
      "        nan        nan        nan 0.99847356 0.99988259 1.\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "model = GradientBoostingClassifier()\n",
    "params = {\n",
    "    'loss':['log_loss', 'deviance', 'exponential'],\n",
    "    'learning_rate':np.linspace(0.05, 0.2, 10),\n",
    "    'n_estimators':[50, 75, 100],\n",
    "    'criterion':['friedman_mse', 'squared_error']\n",
    "}\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=params, cv=3, verbose=5, return_train_score=True,refit=True)\n",
    "grid_model = grid_search.fit(X_train_meta,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_test = grid_model.predict(X_test_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'squared_error',\n",
       " 'learning_rate': 0.11666666666666668,\n",
       " 'loss': 'log_loss',\n",
       " 'n_estimators': 75}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9563170188620566"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_model.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay,multilabel_confusion_matrix,f1_score,precision_score,accuracy_score,recall_score,precision_recall_fscore_support\n",
    "def evaluation_test(y,y_pred):\n",
    "    cm = confusion_matrix(y,y_pred)\n",
    "    disp = ConfusionMatrixDisplay(cm,display_labels=['AFIB','SB','SR','GSVT'])\n",
    "    disp.plot()\n",
    "    plt.show()\n",
    "    n_classes = len(cm)\n",
    "    result = []\n",
    "    for c in range(n_classes):\n",
    "        tp = cm[c,c]\n",
    "        fp = sum(cm[:,c]) - cm[c,c]\n",
    "        fn = sum(cm[c,:]) - cm[c,c]\n",
    "        tn = sum(np.delete(sum(cm)-cm[c,:],c))\n",
    "        acc = (tp+tn) / (tp+fn+tn+fp)\n",
    "        recall = tp/(tp+fn)\n",
    "        precision = tp/(tp+fp)\n",
    "        specificity = tn/(tn+fp)\n",
    "        f1_score = 2*((precision*recall)/(precision+recall))\n",
    "        if c+1 == 1:\n",
    "            Rhythm = 'AFIB'\n",
    "        elif c+1 == 2:\n",
    "            Rhythm = 'SB'\n",
    "        elif c+1 == 3:\n",
    "            Rhythm = 'SR'\n",
    "        else:\n",
    "            Rhythm = 'GSVT'\n",
    "        result.append([Rhythm,acc,recall,precision,f1_score,specificity])\n",
    "    p_macro,r_macro,f_macro,support_macro = precision_recall_fscore_support(y,y_pred,average='macro')\n",
    "    p_micro,r_micro,f_micro,support_micro = precision_recall_fscore_support(y,y_pred,average='micro')\n",
    "    p_weighted,r_weighted,f_weighted,support_weighted = precision_recall_fscore_support(y,y_pred,average='weighted')\n",
    "    result.append(['macro avg',None,f_macro,p_macro,r_macro,None])\n",
    "    result.append(['micro avg',None,f_micro,p_micro,r_micro,None])\n",
    "    result.append(['weighted avg',None,f_weighted,p_weighted,r_weighted,None])\n",
    "    return result\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rhythm Group</th>\n",
       "      <th>ACC</th>\n",
       "      <th>F1-score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>specificity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AFIB</td>\n",
       "      <td>0.944131</td>\n",
       "      <td>0.923596</td>\n",
       "      <td>0.828629</td>\n",
       "      <td>0.873539</td>\n",
       "      <td>0.949555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SB</td>\n",
       "      <td>0.915023</td>\n",
       "      <td>0.989717</td>\n",
       "      <td>0.816543</td>\n",
       "      <td>0.894829</td>\n",
       "      <td>0.872041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SR</td>\n",
       "      <td>0.816432</td>\n",
       "      <td>0.561798</td>\n",
       "      <td>0.560538</td>\n",
       "      <td>0.561167</td>\n",
       "      <td>0.883680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GSVT</td>\n",
       "      <td>0.873709</td>\n",
       "      <td>0.474026</td>\n",
       "      <td>0.893878</td>\n",
       "      <td>0.619519</td>\n",
       "      <td>0.984412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>macro avg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.737263</td>\n",
       "      <td>0.774897</td>\n",
       "      <td>0.737284</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>micro avg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.774648</td>\n",
       "      <td>0.774648</td>\n",
       "      <td>0.774648</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>weighted avg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.760957</td>\n",
       "      <td>0.782357</td>\n",
       "      <td>0.774648</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rhythm Group       ACC  F1-score  Precision    Recall  specificity\n",
       "0          AFIB  0.944131  0.923596   0.828629  0.873539     0.949555\n",
       "1            SB  0.915023  0.989717   0.816543  0.894829     0.872041\n",
       "2            SR  0.816432  0.561798   0.560538  0.561167     0.883680\n",
       "3          GSVT  0.873709  0.474026   0.893878  0.619519     0.984412\n",
       "4     macro avg       NaN  0.737263   0.774897  0.737284          NaN\n",
       "5     micro avg       NaN  0.774648   0.774648  0.774648          NaN\n",
       "6  weighted avg       NaN  0.760957   0.782357  0.774648          NaN"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_test = evaluation_test(y_test,result_test)\n",
    "df_evaluation_test = pd.DataFrame(data=evaluation_test,columns=[\"Rhythm Group\",\"ACC\",\"F1-score\",\"Precision\",\"Recall\",\"specificity\"])\n",
    "df_evaluation_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_evaluation_test.to_csv(\"./Result/Blending_GB_pso.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
