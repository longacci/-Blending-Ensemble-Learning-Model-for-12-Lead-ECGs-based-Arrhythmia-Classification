{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>2</th>\n",
       "      <th>5</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>...</th>\n",
       "      <th>181</th>\n",
       "      <th>186</th>\n",
       "      <th>187</th>\n",
       "      <th>193</th>\n",
       "      <th>197</th>\n",
       "      <th>203</th>\n",
       "      <th>204</th>\n",
       "      <th>205</th>\n",
       "      <th>209</th>\n",
       "      <th>212</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>274.986868</td>\n",
       "      <td>-0.319753</td>\n",
       "      <td>-1.432466</td>\n",
       "      <td>325.821586</td>\n",
       "      <td>84.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>437.750000</td>\n",
       "      <td>...</td>\n",
       "      <td>2383.209877</td>\n",
       "      <td>650.000000</td>\n",
       "      <td>-0.338865</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>950.222222</td>\n",
       "      <td>991.309467</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>172.000000</td>\n",
       "      <td>10656.395062</td>\n",
       "      <td>135.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>104.913059</td>\n",
       "      <td>0.158313</td>\n",
       "      <td>-0.696295</td>\n",
       "      <td>336.569414</td>\n",
       "      <td>32.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>312.222222</td>\n",
       "      <td>...</td>\n",
       "      <td>2193.439446</td>\n",
       "      <td>384.000000</td>\n",
       "      <td>0.095966</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>574.500000</td>\n",
       "      <td>912.932251</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>-15.000000</td>\n",
       "      <td>3944.000000</td>\n",
       "      <td>-1.066667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.687572</td>\n",
       "      <td>0.396421</td>\n",
       "      <td>-0.312612</td>\n",
       "      <td>94.909877</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>329.600000</td>\n",
       "      <td>...</td>\n",
       "      <td>49.109375</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.217133</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>593.733333</td>\n",
       "      <td>338.632833</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>2058.773333</td>\n",
       "      <td>95.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>3.591772</td>\n",
       "      <td>-0.021014</td>\n",
       "      <td>-0.856142</td>\n",
       "      <td>254.059787</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.036070</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045933</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>-0.026782</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>420.181818</td>\n",
       "      <td>325.369999</td>\n",
       "      <td>0.739130</td>\n",
       "      <td>-9.000000</td>\n",
       "      <td>1120.888889</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>25.118469</td>\n",
       "      <td>-0.276816</td>\n",
       "      <td>-1.271399</td>\n",
       "      <td>461.130814</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>427.250000</td>\n",
       "      <td>...</td>\n",
       "      <td>736.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>-0.285584</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1068.750000</td>\n",
       "      <td>877.512514</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>671.000000</td>\n",
       "      <td>136.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8511</th>\n",
       "      <td>3.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>51.114860</td>\n",
       "      <td>2.153820</td>\n",
       "      <td>2.645687</td>\n",
       "      <td>365.256750</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>342.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>22201.000000</td>\n",
       "      <td>0.003006</td>\n",
       "      <td>0.003006</td>\n",
       "      <td>0.003006</td>\n",
       "      <td>0.003757</td>\n",
       "      <td>0.003757</td>\n",
       "      <td>0.003757</td>\n",
       "      <td>0.022262</td>\n",
       "      <td>0.044242</td>\n",
       "      <td>0.037385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8512</th>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.894913</td>\n",
       "      <td>-0.311206</td>\n",
       "      <td>-1.184514</td>\n",
       "      <td>358.414529</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>481.250000</td>\n",
       "      <td>...</td>\n",
       "      <td>2913.580247</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>-0.625647</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1091.750000</td>\n",
       "      <td>267.711052</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>1294.530612</td>\n",
       "      <td>155.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8513</th>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>107.653355</td>\n",
       "      <td>0.475616</td>\n",
       "      <td>0.784000</td>\n",
       "      <td>180.045117</td>\n",
       "      <td>20.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>341.714286</td>\n",
       "      <td>...</td>\n",
       "      <td>2954.775510</td>\n",
       "      <td>136.000000</td>\n",
       "      <td>0.385381</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>654.571429</td>\n",
       "      <td>484.863590</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>2213.551020</td>\n",
       "      <td>104.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8514</th>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>24.535688</td>\n",
       "      <td>-0.263431</td>\n",
       "      <td>-1.567800</td>\n",
       "      <td>251.455499</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>449.750000</td>\n",
       "      <td>...</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>-0.320207</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1075.000000</td>\n",
       "      <td>704.569046</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>4933.551020</td>\n",
       "      <td>88.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8515</th>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.242421</td>\n",
       "      <td>0.214800</td>\n",
       "      <td>-1.575835</td>\n",
       "      <td>505.203302</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>463.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>11.358025</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.218367</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1041.000000</td>\n",
       "      <td>1946.010560</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>350.000000</td>\n",
       "      <td>150.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8516 rows × 99 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label     2           5         7         8           9    11    12  \\\n",
       "0       0.0  10.0  274.986868 -0.319753 -1.432466  325.821586  84.0   9.0   \n",
       "1       0.0  17.0  104.913059  0.158313 -0.696295  336.569414  32.0  10.0   \n",
       "2       3.0  16.0    4.687572  0.396421 -0.312612   94.909877  -2.0  16.0   \n",
       "3       3.0  23.0    3.591772 -0.021014 -0.856142  254.059787   9.0  10.0   \n",
       "4       1.0   9.0   25.118469 -0.276816 -1.271399  461.130814  -1.0   9.0   \n",
       "...     ...   ...         ...       ...       ...         ...   ...   ...   \n",
       "8511    3.0  16.0   51.114860  2.153820  2.645687  365.256750  -2.0  15.0   \n",
       "8512    1.0   9.0    5.894913 -0.311206 -1.184514  358.414529  -5.0   9.0   \n",
       "8513    2.0  15.0  107.653355  0.475616  0.784000  180.045117  20.0  14.0   \n",
       "8514    1.0   9.0   24.535688 -0.263431 -1.567800  251.455499  -1.0   9.0   \n",
       "8515    1.0   9.0    8.242421  0.214800 -1.575835  505.203302  -2.0   9.0   \n",
       "\n",
       "        13          14  ...           181         186       187        193  \\\n",
       "0      9.0  437.750000  ...   2383.209877  650.000000 -0.338865   9.000000   \n",
       "1     16.0  312.222222  ...   2193.439446  384.000000  0.095966   3.000000   \n",
       "2     15.0  329.600000  ...     49.109375   14.000000  0.217133  15.000000   \n",
       "3      0.0    0.036070  ...      0.045933   14.000000 -0.026782   6.000000   \n",
       "4      8.0  427.250000  ...    736.000000   72.000000 -0.285584   8.000000   \n",
       "...    ...         ...  ...           ...         ...       ...        ...   \n",
       "8511  15.0  342.666667  ...  22201.000000    0.003006  0.003006   0.003006   \n",
       "8512   8.0  481.250000  ...   2913.580247   24.000000 -0.625647   8.000000   \n",
       "8513  14.0  341.714286  ...   2954.775510  136.000000  0.385381  12.000000   \n",
       "8514   8.0  449.750000  ...     24.000000   64.000000 -0.320207   7.000000   \n",
       "8515   8.0  463.500000  ...     11.358025   22.000000  0.218367   7.000000   \n",
       "\n",
       "              197          203       204         205           209         212  \n",
       "0      950.222222   991.309467  1.000000  172.000000  10656.395062  135.800000  \n",
       "1      574.500000   912.932251  0.882353  -15.000000   3944.000000   -1.066667  \n",
       "2      593.733333   338.632833  1.000000   -4.000000   2058.773333   95.500000  \n",
       "3      420.181818   325.369999  0.739130   -9.000000   1120.888889   12.000000  \n",
       "4     1068.750000   877.512514  1.000000    2.000000    671.000000  136.444444  \n",
       "...           ...          ...       ...         ...           ...         ...  \n",
       "8511     0.003757     0.003757  0.003757    0.022262      0.044242    0.037385  \n",
       "8512  1091.750000   267.711052  0.888889   -3.000000   1294.530612  155.333333  \n",
       "8513   654.571429   484.863590  1.000000   -4.000000   2213.551020  104.000000  \n",
       "8514  1075.000000   704.569046  1.000000   14.000000   4933.551020   88.222222  \n",
       "8515  1041.000000  1946.010560  1.000000    0.000000    350.000000  150.000000  \n",
       "\n",
       "[8516 rows x 99 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"./data_train_pso.csv\")\n",
    "df_train.drop(columns=[\"Unnamed: 0\"],inplace=True)\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = df_train.iloc[:,1:].values\n",
    "y_train = df_train.iloc[:,0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = MinMaxScaler()\n",
    "x_train = scale.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>4</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>...</th>\n",
       "      <th>180</th>\n",
       "      <th>185</th>\n",
       "      <th>186</th>\n",
       "      <th>192</th>\n",
       "      <th>196</th>\n",
       "      <th>202</th>\n",
       "      <th>203</th>\n",
       "      <th>204</th>\n",
       "      <th>208</th>\n",
       "      <th>211</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>153.204817</td>\n",
       "      <td>0.996355</td>\n",
       "      <td>0.207174</td>\n",
       "      <td>459.037295</td>\n",
       "      <td>-28.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>358.307692</td>\n",
       "      <td>...</td>\n",
       "      <td>92.686391</td>\n",
       "      <td>554.0</td>\n",
       "      <td>0.999941</td>\n",
       "      <td>10.0</td>\n",
       "      <td>710.615385</td>\n",
       "      <td>794.307350</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>-10.000000</td>\n",
       "      <td>729.000000</td>\n",
       "      <td>127.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>266.399867</td>\n",
       "      <td>0.979352</td>\n",
       "      <td>0.388359</td>\n",
       "      <td>398.464564</td>\n",
       "      <td>-60.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>532.800000</td>\n",
       "      <td>...</td>\n",
       "      <td>7281.937500</td>\n",
       "      <td>932.0</td>\n",
       "      <td>0.965488</td>\n",
       "      <td>4.0</td>\n",
       "      <td>968.222222</td>\n",
       "      <td>900.143486</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>15314.750000</td>\n",
       "      <td>112.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>251.329664</td>\n",
       "      <td>0.260470</td>\n",
       "      <td>-1.002325</td>\n",
       "      <td>340.802438</td>\n",
       "      <td>-52.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>403.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2661.728395</td>\n",
       "      <td>784.0</td>\n",
       "      <td>0.270281</td>\n",
       "      <td>6.0</td>\n",
       "      <td>796.400000</td>\n",
       "      <td>1236.308241</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>1944.489796</td>\n",
       "      <td>131.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>8.986100</td>\n",
       "      <td>0.048579</td>\n",
       "      <td>-1.449012</td>\n",
       "      <td>412.324324</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>322.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>787.638889</td>\n",
       "      <td>26.0</td>\n",
       "      <td>-0.014314</td>\n",
       "      <td>12.0</td>\n",
       "      <td>757.333333</td>\n",
       "      <td>803.828940</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>6122.750000</td>\n",
       "      <td>121.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>82.344017</td>\n",
       "      <td>3.023659</td>\n",
       "      <td>10.404884</td>\n",
       "      <td>168.041577</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>223.750000</td>\n",
       "      <td>...</td>\n",
       "      <td>1288.640000</td>\n",
       "      <td>398.0</td>\n",
       "      <td>1.761865</td>\n",
       "      <td>16.0</td>\n",
       "      <td>396.173913</td>\n",
       "      <td>316.582423</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.022262</td>\n",
       "      <td>0.044242</td>\n",
       "      <td>45.818182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2125</th>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>36.509417</td>\n",
       "      <td>1.263183</td>\n",
       "      <td>0.543003</td>\n",
       "      <td>364.303573</td>\n",
       "      <td>-18.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>438.571429</td>\n",
       "      <td>...</td>\n",
       "      <td>4051.918367</td>\n",
       "      <td>118.0</td>\n",
       "      <td>1.263183</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1071.250000</td>\n",
       "      <td>856.643246</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2843.265306</td>\n",
       "      <td>96.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2126</th>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>33.839959</td>\n",
       "      <td>-0.454057</td>\n",
       "      <td>-1.036905</td>\n",
       "      <td>181.876516</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>463.142857</td>\n",
       "      <td>...</td>\n",
       "      <td>10.750000</td>\n",
       "      <td>104.0</td>\n",
       "      <td>-0.457057</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1196.000000</td>\n",
       "      <td>746.905354</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-26.000000</td>\n",
       "      <td>228.555556</td>\n",
       "      <td>169.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2127</th>\n",
       "      <td>3.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>23.734082</td>\n",
       "      <td>0.371174</td>\n",
       "      <td>-0.657132</td>\n",
       "      <td>137.696567</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>335.733333</td>\n",
       "      <td>...</td>\n",
       "      <td>739.982222</td>\n",
       "      <td>82.0</td>\n",
       "      <td>0.348673</td>\n",
       "      <td>14.0</td>\n",
       "      <td>595.600000</td>\n",
       "      <td>333.718093</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-8.000000</td>\n",
       "      <td>1270.061224</td>\n",
       "      <td>90.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2128</th>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>180.470587</td>\n",
       "      <td>0.587475</td>\n",
       "      <td>-1.363827</td>\n",
       "      <td>561.988537</td>\n",
       "      <td>12.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>462.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>431.750000</td>\n",
       "      <td>448.0</td>\n",
       "      <td>0.587475</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1080.571429</td>\n",
       "      <td>588.205240</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>51.840000</td>\n",
       "      <td>101.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2129</th>\n",
       "      <td>3.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2.569857</td>\n",
       "      <td>0.605786</td>\n",
       "      <td>-0.869886</td>\n",
       "      <td>654.123072</td>\n",
       "      <td>46.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.036070</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045933</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.989051</td>\n",
       "      <td>0.0</td>\n",
       "      <td>391.250000</td>\n",
       "      <td>1293.658260</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.044242</td>\n",
       "      <td>0.037385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2130 rows × 99 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0     1           4         6          7           8    10    11  \\\n",
       "0     0.0  14.0  153.204817  0.996355   0.207174  459.037295 -28.0  13.0   \n",
       "1     0.0  10.0  266.399867  0.979352   0.388359  398.464564 -60.0  10.0   \n",
       "2     0.0  11.0  251.329664  0.260470  -1.002325  340.802438 -52.0   8.0   \n",
       "3     2.0  13.0    8.986100  0.048579  -1.449012  412.324324  -5.0  12.0   \n",
       "4     0.0  23.0   82.344017  3.023659  10.404884  168.041577   7.0   9.0   \n",
       "...   ...   ...         ...       ...        ...         ...   ...   ...   \n",
       "2125  1.0   9.0   36.509417  1.263183   0.543003  364.303573 -18.0   9.0   \n",
       "2126  1.0   8.0   33.839959 -0.454057  -1.036905  181.876516   0.0   8.0   \n",
       "2127  3.0  16.0   23.734082  0.371174  -0.657132  137.696567 -10.0  16.0   \n",
       "2128  1.0   8.0  180.470587  0.587475  -1.363827  561.988537  12.0   8.0   \n",
       "2129  3.0  25.0    2.569857  0.605786  -0.869886  654.123072  46.0   0.0   \n",
       "\n",
       "        12          13  ...          180    185       186   192          196  \\\n",
       "0     13.0  358.307692  ...    92.686391  554.0  0.999941  10.0   710.615385   \n",
       "1      6.0  532.800000  ...  7281.937500  932.0  0.965488   4.0   968.222222   \n",
       "2      7.0  403.000000  ...  2661.728395  784.0  0.270281   6.0   796.400000   \n",
       "3     12.0  322.333333  ...   787.638889   26.0 -0.014314  12.0   757.333333   \n",
       "4     20.0  223.750000  ...  1288.640000  398.0  1.761865  16.0   396.173913   \n",
       "...    ...         ...  ...          ...    ...       ...   ...          ...   \n",
       "2125   8.0  438.571429  ...  4051.918367  118.0  1.263183   8.0  1071.250000   \n",
       "2126   7.0  463.142857  ...    10.750000  104.0 -0.457057   5.0  1196.000000   \n",
       "2127  15.0  335.733333  ...   739.982222   82.0  0.348673  14.0   595.600000   \n",
       "2128   7.0  462.000000  ...   431.750000  448.0  0.587475   5.0  1080.571429   \n",
       "2129   5.0    0.036070  ...     0.045933   10.0  0.989051   0.0   391.250000   \n",
       "\n",
       "              202       203        204           208         211  \n",
       "0      794.307350  0.928571 -10.000000    729.000000  127.600000  \n",
       "1      900.143486  0.600000  64.000000  15314.750000  112.285714  \n",
       "2     1236.308241  1.000000  26.000000   1944.489796  131.111111  \n",
       "3      803.828940  1.000000  -4.000000   6122.750000  121.833333  \n",
       "4      316.582423  0.083333   0.022262      0.044242   45.818182  \n",
       "...           ...       ...        ...           ...         ...  \n",
       "2125   856.643246  0.777778   0.000000   2843.265306   96.000000  \n",
       "2126   746.905354  1.000000 -26.000000    228.555556  169.142857  \n",
       "2127   333.718093  1.000000  -8.000000   1270.061224   90.400000  \n",
       "2128   588.205240  1.000000  18.000000     51.840000  101.000000  \n",
       "2129  1293.658260  0.240000   4.000000      0.044242    0.037385  \n",
       "\n",
       "[2130 rows x 99 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv(\"./data_test_pso.csv\")\n",
    "df_test.drop(columns=[\"Unnamed: 0\"],inplace=True)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = df_test.iloc[:,1:].values\n",
    "y_test = df_test.iloc[:,0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = scale.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (4258, 98)\n",
      "Vallidation: (4258, 98)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train , test_size=0.5, shuffle=True, stratify=y_train, random_state=119)\n",
    "print(f\"Train: {x_train.shape}\")\n",
    "print(f\"Vallidation: {x_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.28120569, 0.01306759, 0.48213118, ..., 0.60941828, 0.01836296,\n",
       "        0.46590445],\n",
       "       [0.4062134 , 0.15362717, 0.44048528, ..., 0.63896584, 0.01548119,\n",
       "        0.42343814],\n",
       "       [0.31245762, 0.41254636, 0.48209595, ..., 0.65373961, 0.        ,\n",
       "        0.24423963],\n",
       "       ...,\n",
       "       [0.62497688, 0.10881596, 0.49735911, ..., 0.60757156, 0.04293483,\n",
       "        0.41363822],\n",
       "       [0.28120569, 0.0417466 , 0.51399398, ..., 0.62696214, 0.00140005,\n",
       "        0.49816252],\n",
       "       [0.24995376, 0.02368805, 0.37511248, ..., 0.62234534, 0.00408517,\n",
       "        0.49897918]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf_clf = RandomForestClassifier(criterion= 'log_loss', max_depth= 5, max_features= 'sqrt', n_estimators= 1000)\n",
    "ab_clf = AdaBoostClassifier(algorithm= 'SAMME.R', learning_rate= 0.1, n_estimators= 50)\n",
    "knn_clf = KNeighborsClassifier(algorithm= 'auto', n_neighbors= 5, p= 1, weights= 'uniform')\n",
    "svc_clf = SVC(C= 100, gamma= 'scale', kernel= 'rbf', probability= True)\n",
    "xgb_clf = XGBClassifier(gamma= 0,learning_rate= 0.1,max_depth= 5,min_child_weight= 1,n_estimators= 1000)\n",
    "dt_clf = DecisionTreeClassifier(criterion= 'entropy',max_depth= 5,max_features= 'sqrt',splitter= 'best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(criterion=&#x27;entropy&#x27;, max_depth=5, max_features=&#x27;sqrt&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;DecisionTreeClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.tree.DecisionTreeClassifier.html\">?<span>Documentation for DecisionTreeClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>DecisionTreeClassifier(criterion=&#x27;entropy&#x27;, max_depth=5, max_features=&#x27;sqrt&#x27;)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier(criterion='entropy', max_depth=5, max_features='sqrt')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Huấn luyện các mô hình con\n",
    "# rf_clf.fit(x_train,y_train)\n",
    "ab_clf.fit(x_train, y_train)\n",
    "knn_clf.fit(x_train, y_train)\n",
    "svc_clf.fit(x_train, y_train)\n",
    "xgb_clf.fit(x_train, y_train)\n",
    "dt_clf.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dự đoán trên tập huấn luyện để tạo đặc trưng mới cho mô hình blending\n",
    "X_train_meta = np.column_stack((\n",
    "    # rf_clf.predict_proba(x_val),\n",
    "    ab_clf.predict_proba(x_val),\n",
    "    knn_clf.predict_proba(x_val),\n",
    "    svc_clf.predict_proba(x_val),\n",
    "    xgb_clf.predict_proba(x_val),\n",
    "    dt_clf.predict_proba(x_val)\n",
    "))\n",
    "# Dự đoán trên tập kiểm tra để tạo đặc trưng mới cho mô hình blending\n",
    "X_test_meta = np.column_stack((\n",
    "    # rf_clf.predict_proba(x_test),\n",
    "    ab_clf.predict_proba(x_test),\n",
    "    knn_clf.predict_proba(x_test),\n",
    "    svc_clf.predict_proba(x_test),\n",
    "    xgb_clf.predict_proba(x_test),\n",
    "    dt_clf.predict_proba(x_test)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_meta:(4258, 20)\n",
      "X_test_meta:(2130, 20)\n"
     ]
    }
   ],
   "source": [
    "print(f\"X_train_meta:{X_train_meta.shape}\")\n",
    "print(f\"X_test_meta:{X_test_meta.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 72 candidates, totalling 216 fits\n",
      "[CV 1/3] END criterion=gini, max_depth=3, max_features=sqrt, n_estimators=10;, score=(train=0.956, test=0.951) total time=   0.0s\n",
      "[CV 2/3] END criterion=gini, max_depth=3, max_features=sqrt, n_estimators=10;, score=(train=0.954, test=0.954) total time=   0.0s\n",
      "[CV 3/3] END criterion=gini, max_depth=3, max_features=sqrt, n_estimators=10;, score=(train=0.956, test=0.949) total time=   0.0s\n",
      "[CV 1/3] END criterion=gini, max_depth=3, max_features=sqrt, n_estimators=50;, score=(train=0.958, test=0.954) total time=   0.1s\n",
      "[CV 2/3] END criterion=gini, max_depth=3, max_features=sqrt, n_estimators=50;, score=(train=0.957, test=0.958) total time=   0.1s\n",
      "[CV 3/3] END criterion=gini, max_depth=3, max_features=sqrt, n_estimators=50;, score=(train=0.959, test=0.949) total time=   0.1s\n",
      "[CV 1/3] END criterion=gini, max_depth=3, max_features=sqrt, n_estimators=100;, score=(train=0.957, test=0.955) total time=   0.4s\n",
      "[CV 2/3] END criterion=gini, max_depth=3, max_features=sqrt, n_estimators=100;, score=(train=0.956, test=0.956) total time=   0.2s\n",
      "[CV 3/3] END criterion=gini, max_depth=3, max_features=sqrt, n_estimators=100;, score=(train=0.960, test=0.951) total time=   0.2s\n",
      "[CV 1/3] END criterion=gini, max_depth=3, max_features=sqrt, n_estimators=1000;, score=(train=0.958, test=0.955) total time=   3.0s\n",
      "[CV 2/3] END criterion=gini, max_depth=3, max_features=sqrt, n_estimators=1000;, score=(train=0.957, test=0.957) total time=   2.7s\n",
      "[CV 3/3] END criterion=gini, max_depth=3, max_features=sqrt, n_estimators=1000;, score=(train=0.960, test=0.952) total time=   2.8s\n",
      "[CV 1/3] END criterion=gini, max_depth=3, max_features=log2, n_estimators=10;, score=(train=0.957, test=0.957) total time=   0.0s\n",
      "[CV 2/3] END criterion=gini, max_depth=3, max_features=log2, n_estimators=10;, score=(train=0.956, test=0.956) total time=   0.0s\n",
      "[CV 3/3] END criterion=gini, max_depth=3, max_features=log2, n_estimators=10;, score=(train=0.958, test=0.950) total time=   0.0s\n",
      "[CV 1/3] END criterion=gini, max_depth=3, max_features=log2, n_estimators=50;, score=(train=0.957, test=0.956) total time=   0.1s\n",
      "[CV 2/3] END criterion=gini, max_depth=3, max_features=log2, n_estimators=50;, score=(train=0.958, test=0.958) total time=   0.1s\n",
      "[CV 3/3] END criterion=gini, max_depth=3, max_features=log2, n_estimators=50;, score=(train=0.960, test=0.951) total time=   0.1s\n",
      "[CV 1/3] END criterion=gini, max_depth=3, max_features=log2, n_estimators=100;, score=(train=0.958, test=0.956) total time=   0.2s\n",
      "[CV 2/3] END criterion=gini, max_depth=3, max_features=log2, n_estimators=100;, score=(train=0.957, test=0.957) total time=   0.2s\n",
      "[CV 3/3] END criterion=gini, max_depth=3, max_features=log2, n_estimators=100;, score=(train=0.959, test=0.951) total time=   0.2s\n",
      "[CV 1/3] END criterion=gini, max_depth=3, max_features=log2, n_estimators=1000;, score=(train=0.959, test=0.956) total time=   3.3s\n",
      "[CV 2/3] END criterion=gini, max_depth=3, max_features=log2, n_estimators=1000;, score=(train=0.958, test=0.957) total time=   3.5s\n",
      "[CV 3/3] END criterion=gini, max_depth=3, max_features=log2, n_estimators=1000;, score=(train=0.960, test=0.952) total time=   3.5s\n",
      "[CV 1/3] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=10;, score=(train=0.959, test=0.954) total time=   0.0s\n",
      "[CV 2/3] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=10;, score=(train=0.958, test=0.956) total time=   0.0s\n",
      "[CV 3/3] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=10;, score=(train=0.962, test=0.950) total time=   0.0s\n",
      "[CV 1/3] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=50;, score=(train=0.961, test=0.956) total time=   0.1s\n",
      "[CV 2/3] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=50;, score=(train=0.961, test=0.958) total time=   0.1s\n",
      "[CV 3/3] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=50;, score=(train=0.962, test=0.951) total time=   0.1s\n",
      "[CV 1/3] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=100;, score=(train=0.961, test=0.956) total time=   0.3s\n",
      "[CV 2/3] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=100;, score=(train=0.960, test=0.957) total time=   0.3s\n",
      "[CV 3/3] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=100;, score=(train=0.964, test=0.951) total time=   0.3s\n",
      "[CV 1/3] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=1000;, score=(train=0.960, test=0.956) total time=   3.6s\n",
      "[CV 2/3] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=1000;, score=(train=0.961, test=0.958) total time=   3.7s\n",
      "[CV 3/3] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=1000;, score=(train=0.964, test=0.951) total time=   3.7s\n",
      "[CV 1/3] END criterion=gini, max_depth=4, max_features=log2, n_estimators=10;, score=(train=0.960, test=0.954) total time=   0.0s\n",
      "[CV 2/3] END criterion=gini, max_depth=4, max_features=log2, n_estimators=10;, score=(train=0.962, test=0.957) total time=   0.0s\n",
      "[CV 3/3] END criterion=gini, max_depth=4, max_features=log2, n_estimators=10;, score=(train=0.961, test=0.949) total time=   0.0s\n",
      "[CV 1/3] END criterion=gini, max_depth=4, max_features=log2, n_estimators=50;, score=(train=0.962, test=0.956) total time=   0.1s\n",
      "[CV 2/3] END criterion=gini, max_depth=4, max_features=log2, n_estimators=50;, score=(train=0.960, test=0.958) total time=   0.1s\n",
      "[CV 3/3] END criterion=gini, max_depth=4, max_features=log2, n_estimators=50;, score=(train=0.962, test=0.951) total time=   0.1s\n",
      "[CV 1/3] END criterion=gini, max_depth=4, max_features=log2, n_estimators=100;, score=(train=0.959, test=0.956) total time=   0.3s\n",
      "[CV 2/3] END criterion=gini, max_depth=4, max_features=log2, n_estimators=100;, score=(train=0.961, test=0.958) total time=   0.3s\n",
      "[CV 3/3] END criterion=gini, max_depth=4, max_features=log2, n_estimators=100;, score=(train=0.961, test=0.951) total time=   0.3s\n",
      "[CV 1/3] END criterion=gini, max_depth=4, max_features=log2, n_estimators=1000;, score=(train=0.960, test=0.956) total time=   3.9s\n",
      "[CV 2/3] END criterion=gini, max_depth=4, max_features=log2, n_estimators=1000;, score=(train=0.961, test=0.958) total time=   4.7s\n",
      "[CV 3/3] END criterion=gini, max_depth=4, max_features=log2, n_estimators=1000;, score=(train=0.964, test=0.952) total time=   5.1s\n",
      "[CV 1/3] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=10;, score=(train=0.965, test=0.955) total time=   0.0s\n",
      "[CV 2/3] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=10;, score=(train=0.963, test=0.958) total time=   0.0s\n",
      "[CV 3/3] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=10;, score=(train=0.965, test=0.950) total time=   0.0s\n",
      "[CV 1/3] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=50;, score=(train=0.964, test=0.957) total time=   0.2s\n",
      "[CV 2/3] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=50;, score=(train=0.963, test=0.959) total time=   0.2s\n",
      "[CV 3/3] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=50;, score=(train=0.967, test=0.951) total time=   0.2s\n",
      "[CV 1/3] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=100;, score=(train=0.964, test=0.956) total time=   0.5s\n",
      "[CV 2/3] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=100;, score=(train=0.964, test=0.958) total time=   0.5s\n",
      "[CV 3/3] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=100;, score=(train=0.965, test=0.950) total time=   0.5s\n",
      "[CV 1/3] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=1000;, score=(train=0.964, test=0.957) total time=   5.6s\n",
      "[CV 2/3] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=1000;, score=(train=0.965, test=0.958) total time=   5.5s\n",
      "[CV 3/3] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=1000;, score=(train=0.966, test=0.950) total time=   5.4s\n",
      "[CV 1/3] END criterion=gini, max_depth=5, max_features=log2, n_estimators=10;, score=(train=0.966, test=0.953) total time=   0.0s\n",
      "[CV 2/3] END criterion=gini, max_depth=5, max_features=log2, n_estimators=10;, score=(train=0.963, test=0.959) total time=   0.0s\n",
      "[CV 3/3] END criterion=gini, max_depth=5, max_features=log2, n_estimators=10;, score=(train=0.967, test=0.950) total time=   0.0s\n",
      "[CV 1/3] END criterion=gini, max_depth=5, max_features=log2, n_estimators=50;, score=(train=0.964, test=0.957) total time=   0.2s\n",
      "[CV 2/3] END criterion=gini, max_depth=5, max_features=log2, n_estimators=50;, score=(train=0.963, test=0.959) total time=   0.2s\n",
      "[CV 3/3] END criterion=gini, max_depth=5, max_features=log2, n_estimators=50;, score=(train=0.967, test=0.951) total time=   0.2s\n",
      "[CV 1/3] END criterion=gini, max_depth=5, max_features=log2, n_estimators=100;, score=(train=0.964, test=0.955) total time=   0.4s\n",
      "[CV 2/3] END criterion=gini, max_depth=5, max_features=log2, n_estimators=100;, score=(train=0.964, test=0.961) total time=   0.4s\n",
      "[CV 3/3] END criterion=gini, max_depth=5, max_features=log2, n_estimators=100;, score=(train=0.967, test=0.952) total time=   0.4s\n",
      "[CV 1/3] END criterion=gini, max_depth=5, max_features=log2, n_estimators=1000;, score=(train=0.964, test=0.956) total time=   5.5s\n",
      "[CV 2/3] END criterion=gini, max_depth=5, max_features=log2, n_estimators=1000;, score=(train=0.964, test=0.958) total time=   5.4s\n",
      "[CV 3/3] END criterion=gini, max_depth=5, max_features=log2, n_estimators=1000;, score=(train=0.967, test=0.950) total time=   5.5s\n",
      "[CV 1/3] END criterion=entropy, max_depth=3, max_features=sqrt, n_estimators=10;, score=(train=0.957, test=0.954) total time=   0.0s\n",
      "[CV 2/3] END criterion=entropy, max_depth=3, max_features=sqrt, n_estimators=10;, score=(train=0.956, test=0.954) total time=   0.0s\n",
      "[CV 3/3] END criterion=entropy, max_depth=3, max_features=sqrt, n_estimators=10;, score=(train=0.960, test=0.954) total time=   0.0s\n",
      "[CV 1/3] END criterion=entropy, max_depth=3, max_features=sqrt, n_estimators=50;, score=(train=0.958, test=0.952) total time=   0.2s\n",
      "[CV 2/3] END criterion=entropy, max_depth=3, max_features=sqrt, n_estimators=50;, score=(train=0.958, test=0.956) total time=   0.2s\n",
      "[CV 3/3] END criterion=entropy, max_depth=3, max_features=sqrt, n_estimators=50;, score=(train=0.960, test=0.949) total time=   0.2s\n",
      "[CV 1/3] END criterion=entropy, max_depth=3, max_features=sqrt, n_estimators=100;, score=(train=0.959, test=0.953) total time=   0.5s\n",
      "[CV 2/3] END criterion=entropy, max_depth=3, max_features=sqrt, n_estimators=100;, score=(train=0.957, test=0.958) total time=   0.5s\n",
      "[CV 3/3] END criterion=entropy, max_depth=3, max_features=sqrt, n_estimators=100;, score=(train=0.959, test=0.952) total time=   0.5s\n",
      "[CV 1/3] END criterion=entropy, max_depth=3, max_features=sqrt, n_estimators=1000;, score=(train=0.959, test=0.955) total time=   5.5s\n",
      "[CV 2/3] END criterion=entropy, max_depth=3, max_features=sqrt, n_estimators=1000;, score=(train=0.957, test=0.957) total time=   5.9s\n",
      "[CV 3/3] END criterion=entropy, max_depth=3, max_features=sqrt, n_estimators=1000;, score=(train=0.960, test=0.951) total time=   5.5s\n",
      "[CV 1/3] END criterion=entropy, max_depth=3, max_features=log2, n_estimators=10;, score=(train=0.957, test=0.953) total time=   0.0s\n",
      "[CV 2/3] END criterion=entropy, max_depth=3, max_features=log2, n_estimators=10;, score=(train=0.957, test=0.957) total time=   0.0s\n",
      "[CV 3/3] END criterion=entropy, max_depth=3, max_features=log2, n_estimators=10;, score=(train=0.957, test=0.951) total time=   0.0s\n",
      "[CV 1/3] END criterion=entropy, max_depth=3, max_features=log2, n_estimators=50;, score=(train=0.958, test=0.956) total time=   0.2s\n",
      "[CV 2/3] END criterion=entropy, max_depth=3, max_features=log2, n_estimators=50;, score=(train=0.958, test=0.956) total time=   0.2s\n",
      "[CV 3/3] END criterion=entropy, max_depth=3, max_features=log2, n_estimators=50;, score=(train=0.959, test=0.950) total time=   0.2s\n",
      "[CV 1/3] END criterion=entropy, max_depth=3, max_features=log2, n_estimators=100;, score=(train=0.959, test=0.953) total time=   0.5s\n",
      "[CV 2/3] END criterion=entropy, max_depth=3, max_features=log2, n_estimators=100;, score=(train=0.956, test=0.953) total time=   0.5s\n",
      "[CV 3/3] END criterion=entropy, max_depth=3, max_features=log2, n_estimators=100;, score=(train=0.959, test=0.950) total time=   0.4s\n",
      "[CV 1/3] END criterion=entropy, max_depth=3, max_features=log2, n_estimators=1000;, score=(train=0.958, test=0.955) total time=   5.6s\n",
      "[CV 2/3] END criterion=entropy, max_depth=3, max_features=log2, n_estimators=1000;, score=(train=0.958, test=0.958) total time=   5.9s\n",
      "[CV 3/3] END criterion=entropy, max_depth=3, max_features=log2, n_estimators=1000;, score=(train=0.960, test=0.953) total time=   5.9s\n",
      "[CV 1/3] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=10;, score=(train=0.959, test=0.956) total time=   0.0s\n",
      "[CV 2/3] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=10;, score=(train=0.959, test=0.956) total time=   0.0s\n",
      "[CV 3/3] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=10;, score=(train=0.962, test=0.946) total time=   0.0s\n",
      "[CV 1/3] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=50;, score=(train=0.959, test=0.957) total time=   0.3s\n",
      "[CV 2/3] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=50;, score=(train=0.960, test=0.959) total time=   0.2s\n",
      "[CV 3/3] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=50;, score=(train=0.964, test=0.955) total time=   0.2s\n",
      "[CV 1/3] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=100;, score=(train=0.960, test=0.956) total time=   0.8s\n",
      "[CV 2/3] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=100;, score=(train=0.959, test=0.957) total time=   0.6s\n",
      "[CV 3/3] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=100;, score=(train=0.964, test=0.951) total time=   0.6s\n",
      "[CV 1/3] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=1000;, score=(train=0.960, test=0.955) total time=   5.6s\n",
      "[CV 2/3] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=1000;, score=(train=0.961, test=0.958) total time=   3.8s\n",
      "[CV 3/3] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=1000;, score=(train=0.963, test=0.951) total time=   4.1s\n",
      "[CV 1/3] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=10;, score=(train=0.959, test=0.956) total time=   0.0s\n",
      "[CV 2/3] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=10;, score=(train=0.960, test=0.954) total time=   0.0s\n",
      "[CV 3/3] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=10;, score=(train=0.962, test=0.951) total time=   0.0s\n",
      "[CV 1/3] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=50;, score=(train=0.961, test=0.957) total time=   0.1s\n",
      "[CV 2/3] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=50;, score=(train=0.959, test=0.956) total time=   0.1s\n",
      "[CV 3/3] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=50;, score=(train=0.962, test=0.952) total time=   0.1s\n",
      "[CV 1/3] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=100;, score=(train=0.959, test=0.956) total time=   0.3s\n",
      "[CV 2/3] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=100;, score=(train=0.959, test=0.960) total time=   0.3s\n",
      "[CV 3/3] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=100;, score=(train=0.963, test=0.951) total time=   0.4s\n",
      "[CV 1/3] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=1000;, score=(train=0.959, test=0.955) total time=   4.2s\n",
      "[CV 2/3] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=1000;, score=(train=0.961, test=0.957) total time=   3.3s\n",
      "[CV 3/3] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=1000;, score=(train=0.963, test=0.951) total time=   3.1s\n",
      "[CV 1/3] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=10;, score=(train=0.964, test=0.954) total time=   0.0s\n",
      "[CV 2/3] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=10;, score=(train=0.962, test=0.958) total time=   0.0s\n",
      "[CV 3/3] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=10;, score=(train=0.964, test=0.952) total time=   0.0s\n",
      "[CV 1/3] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=50;, score=(train=0.964, test=0.956) total time=   0.1s\n",
      "[CV 2/3] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=50;, score=(train=0.963, test=0.958) total time=   0.1s\n",
      "[CV 3/3] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=50;, score=(train=0.967, test=0.950) total time=   0.1s\n",
      "[CV 1/3] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=100;, score=(train=0.965, test=0.956) total time=   0.3s\n",
      "[CV 2/3] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=100;, score=(train=0.964, test=0.957) total time=   0.3s\n",
      "[CV 3/3] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=100;, score=(train=0.966, test=0.951) total time=   0.2s\n",
      "[CV 1/3] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=1000;, score=(train=0.965, test=0.956) total time=   3.6s\n",
      "[CV 2/3] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=1000;, score=(train=0.964, test=0.958) total time=   3.5s\n",
      "[CV 3/3] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=1000;, score=(train=0.966, test=0.951) total time=   3.4s\n",
      "[CV 1/3] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=10;, score=(train=0.963, test=0.956) total time=   0.0s\n",
      "[CV 2/3] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=10;, score=(train=0.964, test=0.957) total time=   0.0s\n",
      "[CV 3/3] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=10;, score=(train=0.966, test=0.953) total time=   0.0s\n",
      "[CV 1/3] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=50;, score=(train=0.965, test=0.956) total time=   0.1s\n",
      "[CV 2/3] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=50;, score=(train=0.964, test=0.959) total time=   0.1s\n",
      "[CV 3/3] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=50;, score=(train=0.967, test=0.951) total time=   0.1s\n",
      "[CV 1/3] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=100;, score=(train=0.963, test=0.958) total time=   0.3s\n",
      "[CV 2/3] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=100;, score=(train=0.964, test=0.957) total time=   0.3s\n",
      "[CV 3/3] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=100;, score=(train=0.966, test=0.951) total time=   0.3s\n",
      "[CV 1/3] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=1000;, score=(train=0.964, test=0.956) total time=   3.6s\n",
      "[CV 2/3] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=1000;, score=(train=0.964, test=0.958) total time=   4.2s\n",
      "[CV 3/3] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=1000;, score=(train=0.965, test=0.951) total time=   3.9s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=3, max_features=sqrt, n_estimators=10;, score=(train=0.958, test=0.954) total time=   0.0s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=3, max_features=sqrt, n_estimators=10;, score=(train=0.954, test=0.952) total time=   0.0s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=3, max_features=sqrt, n_estimators=10;, score=(train=0.958, test=0.949) total time=   0.0s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=3, max_features=sqrt, n_estimators=50;, score=(train=0.957, test=0.955) total time=   0.1s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=3, max_features=sqrt, n_estimators=50;, score=(train=0.955, test=0.958) total time=   0.1s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=3, max_features=sqrt, n_estimators=50;, score=(train=0.959, test=0.951) total time=   0.0s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=3, max_features=sqrt, n_estimators=100;, score=(train=0.958, test=0.954) total time=   0.2s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=3, max_features=sqrt, n_estimators=100;, score=(train=0.956, test=0.958) total time=   0.3s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=3, max_features=sqrt, n_estimators=100;, score=(train=0.959, test=0.950) total time=   0.2s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=3, max_features=sqrt, n_estimators=1000;, score=(train=0.958, test=0.953) total time=   3.3s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=3, max_features=sqrt, n_estimators=1000;, score=(train=0.957, test=0.957) total time=   2.8s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=3, max_features=sqrt, n_estimators=1000;, score=(train=0.960, test=0.952) total time=   3.0s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=3, max_features=log2, n_estimators=10;, score=(train=0.959, test=0.951) total time=   0.0s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=3, max_features=log2, n_estimators=10;, score=(train=0.958, test=0.958) total time=   0.0s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=3, max_features=log2, n_estimators=10;, score=(train=0.958, test=0.951) total time=   0.0s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=3, max_features=log2, n_estimators=50;, score=(train=0.957, test=0.952) total time=   0.1s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=3, max_features=log2, n_estimators=50;, score=(train=0.956, test=0.958) total time=   0.1s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=3, max_features=log2, n_estimators=50;, score=(train=0.959, test=0.953) total time=   0.1s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=3, max_features=log2, n_estimators=100;, score=(train=0.957, test=0.954) total time=   0.2s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=3, max_features=log2, n_estimators=100;, score=(train=0.957, test=0.956) total time=   0.2s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=3, max_features=log2, n_estimators=100;, score=(train=0.959, test=0.951) total time=   0.2s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=3, max_features=log2, n_estimators=1000;, score=(train=0.958, test=0.955) total time=   2.8s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=3, max_features=log2, n_estimators=1000;, score=(train=0.958, test=0.957) total time=   2.8s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=3, max_features=log2, n_estimators=1000;, score=(train=0.960, test=0.952) total time=   2.7s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=4, max_features=sqrt, n_estimators=10;, score=(train=0.957, test=0.948) total time=   0.0s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=4, max_features=sqrt, n_estimators=10;, score=(train=0.957, test=0.960) total time=   0.0s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=4, max_features=sqrt, n_estimators=10;, score=(train=0.962, test=0.949) total time=   0.0s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=4, max_features=sqrt, n_estimators=50;, score=(train=0.961, test=0.955) total time=   0.1s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=4, max_features=sqrt, n_estimators=50;, score=(train=0.961, test=0.957) total time=   0.1s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=4, max_features=sqrt, n_estimators=50;, score=(train=0.964, test=0.953) total time=   0.1s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=4, max_features=sqrt, n_estimators=100;, score=(train=0.960, test=0.956) total time=   0.3s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=4, max_features=sqrt, n_estimators=100;, score=(train=0.961, test=0.956) total time=   0.2s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=4, max_features=sqrt, n_estimators=100;, score=(train=0.962, test=0.951) total time=   0.2s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=4, max_features=sqrt, n_estimators=1000;, score=(train=0.960, test=0.956) total time=   3.2s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=4, max_features=sqrt, n_estimators=1000;, score=(train=0.960, test=0.958) total time=   3.5s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=4, max_features=sqrt, n_estimators=1000;, score=(train=0.964, test=0.951) total time=   3.2s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=4, max_features=log2, n_estimators=10;, score=(train=0.960, test=0.955) total time=   0.0s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=4, max_features=log2, n_estimators=10;, score=(train=0.958, test=0.958) total time=   0.0s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=4, max_features=log2, n_estimators=10;, score=(train=0.961, test=0.950) total time=   0.0s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=4, max_features=log2, n_estimators=50;, score=(train=0.959, test=0.955) total time=   0.1s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=4, max_features=log2, n_estimators=50;, score=(train=0.962, test=0.956) total time=   0.1s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=4, max_features=log2, n_estimators=50;, score=(train=0.961, test=0.950) total time=   0.1s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=4, max_features=log2, n_estimators=100;, score=(train=0.961, test=0.956) total time=   0.3s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=4, max_features=log2, n_estimators=100;, score=(train=0.960, test=0.956) total time=   0.3s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=4, max_features=log2, n_estimators=100;, score=(train=0.962, test=0.952) total time=   0.3s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=4, max_features=log2, n_estimators=1000;, score=(train=0.960, test=0.956) total time=   3.9s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=4, max_features=log2, n_estimators=1000;, score=(train=0.961, test=0.957) total time=   3.4s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=4, max_features=log2, n_estimators=1000;, score=(train=0.963, test=0.952) total time=   3.7s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=5, max_features=sqrt, n_estimators=10;, score=(train=0.963, test=0.955) total time=   0.0s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=5, max_features=sqrt, n_estimators=10;, score=(train=0.960, test=0.955) total time=   0.0s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=5, max_features=sqrt, n_estimators=10;, score=(train=0.965, test=0.953) total time=   0.0s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=5, max_features=sqrt, n_estimators=50;, score=(train=0.964, test=0.958) total time=   0.1s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=5, max_features=sqrt, n_estimators=50;, score=(train=0.964, test=0.958) total time=   0.1s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=5, max_features=sqrt, n_estimators=50;, score=(train=0.965, test=0.951) total time=   0.1s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=5, max_features=sqrt, n_estimators=100;, score=(train=0.964, test=0.954) total time=   0.3s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=5, max_features=sqrt, n_estimators=100;, score=(train=0.963, test=0.961) total time=   0.4s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=5, max_features=sqrt, n_estimators=100;, score=(train=0.966, test=0.951) total time=   0.3s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=5, max_features=sqrt, n_estimators=1000;, score=(train=0.964, test=0.956) total time=   4.1s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=5, max_features=sqrt, n_estimators=1000;, score=(train=0.963, test=0.960) total time=   4.0s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=5, max_features=sqrt, n_estimators=1000;, score=(train=0.966, test=0.951) total time=   3.7s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=5, max_features=log2, n_estimators=10;, score=(train=0.962, test=0.954) total time=   0.0s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=5, max_features=log2, n_estimators=10;, score=(train=0.962, test=0.959) total time=   0.0s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=5, max_features=log2, n_estimators=10;, score=(train=0.964, test=0.949) total time=   0.0s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=5, max_features=log2, n_estimators=50;, score=(train=0.964, test=0.956) total time=   0.1s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=5, max_features=log2, n_estimators=50;, score=(train=0.964, test=0.960) total time=   0.1s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=5, max_features=log2, n_estimators=50;, score=(train=0.967, test=0.951) total time=   0.1s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=5, max_features=log2, n_estimators=100;, score=(train=0.964, test=0.956) total time=   0.3s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=5, max_features=log2, n_estimators=100;, score=(train=0.963, test=0.960) total time=   0.3s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=5, max_features=log2, n_estimators=100;, score=(train=0.966, test=0.951) total time=   0.3s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=5, max_features=log2, n_estimators=1000;, score=(train=0.963, test=0.956) total time=   4.5s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=5, max_features=log2, n_estimators=1000;, score=(train=0.964, test=0.959) total time=   5.4s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=5, max_features=log2, n_estimators=1000;, score=(train=0.966, test=0.951) total time=   5.2s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "model = RandomForestClassifier()\n",
    "params = {\n",
    "    'n_estimators': [10,50,100,1000],\n",
    "    'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "    'max_depth': [3,4,5],\n",
    "    'max_features':['sqrt', 'log2'],\n",
    "}\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=params, cv=3, verbose=5, return_train_score=True,refit=True)\n",
    "grid_model = grid_search.fit(X_train_meta,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_test = grid_model.predict(X_test_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'entropy',\n",
       " 'max_depth': 4,\n",
       " 'max_features': 'sqrt',\n",
       " 'n_estimators': 50}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9570220713522385"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_model.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay,multilabel_confusion_matrix,f1_score,precision_score,accuracy_score,recall_score,precision_recall_fscore_support\n",
    "def evaluation_test(y,y_pred):\n",
    "    cm = confusion_matrix(y,y_pred)\n",
    "    disp = ConfusionMatrixDisplay(cm,display_labels=['AFIB','SB','SR','GSVT'])\n",
    "    disp.plot()\n",
    "    plt.show()\n",
    "    n_classes = len(cm)\n",
    "    result = []\n",
    "    for c in range(n_classes):\n",
    "        tp = cm[c,c]\n",
    "        fp = sum(cm[:,c]) - cm[c,c]\n",
    "        fn = sum(cm[c,:]) - cm[c,c]\n",
    "        tn = sum(np.delete(sum(cm)-cm[c,:],c))\n",
    "        acc = (tp+tn) / (tp+fn+tn+fp)\n",
    "        recall = tp/(tp+fn)\n",
    "        precision = tp/(tp+fp)\n",
    "        specificity = tn/(tn+fp)\n",
    "        f1_score = 2*((precision*recall)/(precision+recall))\n",
    "        if c+1 == 1:\n",
    "            Rhythm = 'AFIB'\n",
    "        elif c+1 == 2:\n",
    "            Rhythm = 'SB'\n",
    "        elif c+1 == 3:\n",
    "            Rhythm = 'SR'\n",
    "        else:\n",
    "            Rhythm = 'GSVT'\n",
    "        result.append([Rhythm,acc,recall,precision,f1_score,specificity])\n",
    "    p_macro,r_macro,f_macro,support_macro = precision_recall_fscore_support(y,y_pred,average='macro')\n",
    "    p_micro,r_micro,f_micro,support_micro = precision_recall_fscore_support(y,y_pred,average='micro')\n",
    "    p_weighted,r_weighted,f_weighted,support_weighted = precision_recall_fscore_support(y,y_pred,average='weighted')\n",
    "    result.append(['macro avg',None,f_macro,p_macro,r_macro,None])\n",
    "    result.append(['micro avg',None,f_micro,p_micro,r_micro,None])\n",
    "    result.append(['weighted avg',None,f_weighted,p_weighted,r_weighted,None])\n",
    "    return result\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rhythm Group</th>\n",
       "      <th>ACC</th>\n",
       "      <th>F1-score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>specificity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AFIB</td>\n",
       "      <td>0.964319</td>\n",
       "      <td>0.919101</td>\n",
       "      <td>0.910913</td>\n",
       "      <td>0.914989</td>\n",
       "      <td>0.976261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SB</td>\n",
       "      <td>0.989671</td>\n",
       "      <td>0.984576</td>\n",
       "      <td>0.987113</td>\n",
       "      <td>0.985843</td>\n",
       "      <td>0.992604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SR</td>\n",
       "      <td>0.988263</td>\n",
       "      <td>0.970787</td>\n",
       "      <td>0.972973</td>\n",
       "      <td>0.971879</td>\n",
       "      <td>0.992878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GSVT</td>\n",
       "      <td>0.970423</td>\n",
       "      <td>0.930736</td>\n",
       "      <td>0.932755</td>\n",
       "      <td>0.931744</td>\n",
       "      <td>0.981415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>macro avg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.951114</td>\n",
       "      <td>0.950939</td>\n",
       "      <td>0.951300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>micro avg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.956338</td>\n",
       "      <td>0.956338</td>\n",
       "      <td>0.956338</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>weighted avg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.956389</td>\n",
       "      <td>0.956449</td>\n",
       "      <td>0.956338</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rhythm Group       ACC  F1-score  Precision    Recall  specificity\n",
       "0          AFIB  0.964319  0.919101   0.910913  0.914989     0.976261\n",
       "1            SB  0.989671  0.984576   0.987113  0.985843     0.992604\n",
       "2            SR  0.988263  0.970787   0.972973  0.971879     0.992878\n",
       "3          GSVT  0.970423  0.930736   0.932755  0.931744     0.981415\n",
       "4     macro avg       NaN  0.951114   0.950939  0.951300          NaN\n",
       "5     micro avg       NaN  0.956338   0.956338  0.956338          NaN\n",
       "6  weighted avg       NaN  0.956389   0.956449  0.956338          NaN"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_test = evaluation_test(y_test,result_test)\n",
    "df_evaluation_test = pd.DataFrame(data=evaluation_test,columns=[\"Rhythm Group\",\"ACC\",\"F1-score\",\"Precision\",\"Recall\",\"specificity\"])\n",
    "df_evaluation_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_evaluation_test.to_csv(\"./Result/Blending_RF_pso.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
