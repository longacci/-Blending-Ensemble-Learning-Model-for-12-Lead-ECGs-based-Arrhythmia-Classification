{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>204</th>\n",
       "      <th>205</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>950.000000</td>\n",
       "      <td>1074.0</td>\n",
       "      <td>274.986868</td>\n",
       "      <td>782.0</td>\n",
       "      <td>-0.319753</td>\n",
       "      <td>-1.432466</td>\n",
       "      <td>325.821586</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>172.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>252.222222</td>\n",
       "      <td>10656.395062</td>\n",
       "      <td>87.777778</td>\n",
       "      <td>10339.061728</td>\n",
       "      <td>135.800000</td>\n",
       "      <td>4315.560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>574.500000</td>\n",
       "      <td>582.0</td>\n",
       "      <td>104.913059</td>\n",
       "      <td>378.0</td>\n",
       "      <td>0.158313</td>\n",
       "      <td>-0.696295</td>\n",
       "      <td>336.569414</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>-15.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>158.000000</td>\n",
       "      <td>3944.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>6555.000000</td>\n",
       "      <td>-1.066667</td>\n",
       "      <td>697.528889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>593.600000</td>\n",
       "      <td>594.0</td>\n",
       "      <td>4.687572</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.396421</td>\n",
       "      <td>-0.312612</td>\n",
       "      <td>94.909877</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>122.400000</td>\n",
       "      <td>2058.773333</td>\n",
       "      <td>12.533333</td>\n",
       "      <td>1360.782222</td>\n",
       "      <td>95.500000</td>\n",
       "      <td>68.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>420.090909</td>\n",
       "      <td>420.0</td>\n",
       "      <td>3.591772</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-0.021014</td>\n",
       "      <td>-0.856142</td>\n",
       "      <td>254.059787</td>\n",
       "      <td>0.826087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.739130</td>\n",
       "      <td>-9.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>40.666667</td>\n",
       "      <td>1120.888889</td>\n",
       "      <td>5.333333</td>\n",
       "      <td>1504.888889</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>1464.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1068.750000</td>\n",
       "      <td>1075.0</td>\n",
       "      <td>25.118469</td>\n",
       "      <td>76.0</td>\n",
       "      <td>-0.276816</td>\n",
       "      <td>-1.271399</td>\n",
       "      <td>461.130814</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>671.000000</td>\n",
       "      <td>19.750000</td>\n",
       "      <td>569.437500</td>\n",
       "      <td>136.444444</td>\n",
       "      <td>43.358025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8511</th>\n",
       "      <td>3.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>615.733333</td>\n",
       "      <td>596.0</td>\n",
       "      <td>51.114860</td>\n",
       "      <td>152.0</td>\n",
       "      <td>2.153820</td>\n",
       "      <td>2.645687</td>\n",
       "      <td>365.256750</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003757</td>\n",
       "      <td>0.022262</td>\n",
       "      <td>0.003757</td>\n",
       "      <td>0.003757</td>\n",
       "      <td>0.044242</td>\n",
       "      <td>0.044242</td>\n",
       "      <td>0.043021</td>\n",
       "      <td>0.043021</td>\n",
       "      <td>0.037385</td>\n",
       "      <td>0.037385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8512</th>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1091.500000</td>\n",
       "      <td>1093.0</td>\n",
       "      <td>5.894913</td>\n",
       "      <td>18.0</td>\n",
       "      <td>-0.311206</td>\n",
       "      <td>-1.184514</td>\n",
       "      <td>358.414529</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>81.428571</td>\n",
       "      <td>1294.530612</td>\n",
       "      <td>-40.000000</td>\n",
       "      <td>1746.285714</td>\n",
       "      <td>155.333333</td>\n",
       "      <td>4722.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8513</th>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>654.428571</td>\n",
       "      <td>648.0</td>\n",
       "      <td>107.653355</td>\n",
       "      <td>458.0</td>\n",
       "      <td>0.475616</td>\n",
       "      <td>0.784000</td>\n",
       "      <td>180.045117</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>77.142857</td>\n",
       "      <td>2213.551020</td>\n",
       "      <td>-1.714286</td>\n",
       "      <td>2686.204082</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>3602.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8514</th>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1075.000000</td>\n",
       "      <td>1083.0</td>\n",
       "      <td>24.535688</td>\n",
       "      <td>66.0</td>\n",
       "      <td>-0.263431</td>\n",
       "      <td>-1.567800</td>\n",
       "      <td>251.455499</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>101.142857</td>\n",
       "      <td>4933.551020</td>\n",
       "      <td>-10.750000</td>\n",
       "      <td>7259.937500</td>\n",
       "      <td>88.222222</td>\n",
       "      <td>202.172840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8515</th>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1041.250000</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>8.242421</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.214800</td>\n",
       "      <td>-1.575835</td>\n",
       "      <td>505.203302</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>350.000000</td>\n",
       "      <td>-20.000000</td>\n",
       "      <td>588.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8516 rows × 213 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        1     2            3       4           5      6         7         8  \\\n",
       "0     0.0  10.0   950.000000  1074.0  274.986868  782.0 -0.319753 -1.432466   \n",
       "1     0.0  17.0   574.500000   582.0  104.913059  378.0  0.158313 -0.696295   \n",
       "2     3.0  16.0   593.600000   594.0    4.687572   18.0  0.396421 -0.312612   \n",
       "3     3.0  23.0   420.090909   420.0    3.591772   12.0 -0.021014 -0.856142   \n",
       "4     1.0   9.0  1068.750000  1075.0   25.118469   76.0 -0.276816 -1.271399   \n",
       "...   ...   ...          ...     ...         ...    ...       ...       ...   \n",
       "8511  3.0  16.0   615.733333   596.0   51.114860  152.0  2.153820  2.645687   \n",
       "8512  1.0   9.0  1091.500000  1093.0    5.894913   18.0 -0.311206 -1.184514   \n",
       "8513  2.0  15.0   654.428571   648.0  107.653355  458.0  0.475616  0.784000   \n",
       "8514  1.0   9.0  1075.000000  1083.0   24.535688   66.0 -0.263431 -1.567800   \n",
       "8515  1.0   9.0  1041.250000  1040.0    8.242421   22.0  0.214800 -1.575835   \n",
       "\n",
       "               9        10  ...       204         205        206        207  \\\n",
       "0     325.821586  1.000000  ...  1.000000  172.000000  10.000000   9.000000   \n",
       "1     336.569414  1.000000  ...  0.882353  -15.000000  15.000000   4.000000   \n",
       "2      94.909877  1.000000  ...  1.000000   -4.000000  16.000000  15.000000   \n",
       "3     254.059787  0.826087  ...  0.739130   -9.000000   6.000000   4.000000   \n",
       "4     461.130814  1.000000  ...  1.000000    2.000000   9.000000   8.000000   \n",
       "...          ...       ...  ...       ...         ...        ...        ...   \n",
       "8511  365.256750  1.000000  ...  0.003757    0.022262   0.003757   0.003757   \n",
       "8512  358.414529  1.000000  ...  0.888889   -3.000000   9.000000   8.000000   \n",
       "8513  180.045117  1.000000  ...  1.000000   -4.000000  15.000000  14.000000   \n",
       "8514  251.455499  1.000000  ...  1.000000   14.000000   9.000000   8.000000   \n",
       "8515  505.203302  1.000000  ...  1.000000    0.000000   9.000000   8.000000   \n",
       "\n",
       "             208           209        210           211         212  \\\n",
       "0     252.222222  10656.395062  87.777778  10339.061728  135.800000   \n",
       "1     158.000000   3944.000000  73.000000   6555.000000   -1.066667   \n",
       "2     122.400000   2058.773333  12.533333   1360.782222   95.500000   \n",
       "3      40.666667   1120.888889   5.333333   1504.888889   12.000000   \n",
       "4     122.000000    671.000000  19.750000    569.437500  136.444444   \n",
       "...          ...           ...        ...           ...         ...   \n",
       "8511    0.044242      0.044242   0.043021      0.043021    0.037385   \n",
       "8512   81.428571   1294.530612 -40.000000   1746.285714  155.333333   \n",
       "8513   77.142857   2213.551020  -1.714286   2686.204082  104.000000   \n",
       "8514  101.142857   4933.551020 -10.750000   7259.937500   88.222222   \n",
       "8515  102.000000    350.000000 -20.000000    588.000000  150.000000   \n",
       "\n",
       "              213  \n",
       "0     4315.560000  \n",
       "1      697.528889  \n",
       "2       68.750000  \n",
       "3     1464.000000  \n",
       "4       43.358025  \n",
       "...           ...  \n",
       "8511     0.037385  \n",
       "8512  4722.666667  \n",
       "8513  3602.666667  \n",
       "8514   202.172840  \n",
       "8515     0.000000  \n",
       "\n",
       "[8516 rows x 213 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"../data_train_frequency.csv\")\n",
    "df_train.drop(columns=[\"Unnamed: 0\"],inplace=True)\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = df_train.iloc[:,1:].values\n",
    "y_train = df_train.iloc[:,0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = MinMaxScaler()\n",
    "x_train = scale.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>203</th>\n",
       "      <th>204</th>\n",
       "      <th>205</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>710.769231</td>\n",
       "      <td>628.0</td>\n",
       "      <td>153.204817</td>\n",
       "      <td>556.0</td>\n",
       "      <td>0.996355</td>\n",
       "      <td>0.207174</td>\n",
       "      <td>459.037295</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>-10.000000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>729.000000</td>\n",
       "      <td>78.250000</td>\n",
       "      <td>3140.437500</td>\n",
       "      <td>127.600000</td>\n",
       "      <td>1041.440000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>968.666667</td>\n",
       "      <td>894.0</td>\n",
       "      <td>266.399867</td>\n",
       "      <td>932.0</td>\n",
       "      <td>0.979352</td>\n",
       "      <td>0.388359</td>\n",
       "      <td>398.464564</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>140.500000</td>\n",
       "      <td>15314.750000</td>\n",
       "      <td>-27.000000</td>\n",
       "      <td>5249.000000</td>\n",
       "      <td>112.285714</td>\n",
       "      <td>8081.632653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>797.000000</td>\n",
       "      <td>780.0</td>\n",
       "      <td>251.329664</td>\n",
       "      <td>794.0</td>\n",
       "      <td>0.260470</td>\n",
       "      <td>-1.002325</td>\n",
       "      <td>340.802438</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>154.285714</td>\n",
       "      <td>1944.489796</td>\n",
       "      <td>18.571429</td>\n",
       "      <td>8070.530612</td>\n",
       "      <td>131.111111</td>\n",
       "      <td>1078.320988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>757.500000</td>\n",
       "      <td>755.0</td>\n",
       "      <td>8.986100</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.048579</td>\n",
       "      <td>-1.449012</td>\n",
       "      <td>412.324324</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>108.500000</td>\n",
       "      <td>6122.750000</td>\n",
       "      <td>46.500000</td>\n",
       "      <td>7081.416667</td>\n",
       "      <td>121.833333</td>\n",
       "      <td>264.305556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>413.909091</td>\n",
       "      <td>409.0</td>\n",
       "      <td>82.344017</td>\n",
       "      <td>426.0</td>\n",
       "      <td>3.023659</td>\n",
       "      <td>10.404884</td>\n",
       "      <td>168.041577</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.022262</td>\n",
       "      <td>11.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.044242</td>\n",
       "      <td>0.044242</td>\n",
       "      <td>-50.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>45.818182</td>\n",
       "      <td>832.330579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2125</th>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1071.250000</td>\n",
       "      <td>1062.0</td>\n",
       "      <td>36.509417</td>\n",
       "      <td>118.0</td>\n",
       "      <td>1.263183</td>\n",
       "      <td>0.543003</td>\n",
       "      <td>364.303573</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>342.857143</td>\n",
       "      <td>2843.265306</td>\n",
       "      <td>205.142857</td>\n",
       "      <td>11207.836735</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>2281.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2126</th>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1196.000000</td>\n",
       "      <td>1202.0</td>\n",
       "      <td>33.839959</td>\n",
       "      <td>102.0</td>\n",
       "      <td>-0.454057</td>\n",
       "      <td>-1.036905</td>\n",
       "      <td>181.876516</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-26.000000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>137.666667</td>\n",
       "      <td>228.555556</td>\n",
       "      <td>87.714286</td>\n",
       "      <td>14282.775510</td>\n",
       "      <td>169.142857</td>\n",
       "      <td>46.693878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2127</th>\n",
       "      <td>3.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>595.600000</td>\n",
       "      <td>590.0</td>\n",
       "      <td>23.734082</td>\n",
       "      <td>82.0</td>\n",
       "      <td>0.371174</td>\n",
       "      <td>-0.657132</td>\n",
       "      <td>137.696567</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-8.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>102.714286</td>\n",
       "      <td>1270.061224</td>\n",
       "      <td>7.285714</td>\n",
       "      <td>361.489796</td>\n",
       "      <td>90.400000</td>\n",
       "      <td>2186.240000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2128</th>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1080.285714</td>\n",
       "      <td>996.0</td>\n",
       "      <td>180.470587</td>\n",
       "      <td>448.0</td>\n",
       "      <td>0.587475</td>\n",
       "      <td>-1.363827</td>\n",
       "      <td>561.988537</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>62.400000</td>\n",
       "      <td>51.840000</td>\n",
       "      <td>-45.200000</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>5002.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2129</th>\n",
       "      <td>3.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>391.250000</td>\n",
       "      <td>390.0</td>\n",
       "      <td>2.569857</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.605786</td>\n",
       "      <td>-0.869886</td>\n",
       "      <td>654.123072</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.044242</td>\n",
       "      <td>0.044242</td>\n",
       "      <td>0.043021</td>\n",
       "      <td>0.043021</td>\n",
       "      <td>0.037385</td>\n",
       "      <td>0.037385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2130 rows × 213 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0     1            2       3           4      5         6          7  \\\n",
       "0     0.0  14.0   710.769231   628.0  153.204817  556.0  0.996355   0.207174   \n",
       "1     0.0  10.0   968.666667   894.0  266.399867  932.0  0.979352   0.388359   \n",
       "2     0.0  11.0   797.000000   780.0  251.329664  794.0  0.260470  -1.002325   \n",
       "3     2.0  13.0   757.500000   755.0    8.986100   26.0  0.048579  -1.449012   \n",
       "4     0.0  23.0   413.909091   409.0   82.344017  426.0  3.023659  10.404884   \n",
       "...   ...   ...          ...     ...         ...    ...       ...        ...   \n",
       "2125  1.0   9.0  1071.250000  1062.0   36.509417  118.0  1.263183   0.543003   \n",
       "2126  1.0   8.0  1196.000000  1202.0   33.839959  102.0 -0.454057  -1.036905   \n",
       "2127  3.0  16.0   595.600000   590.0   23.734082   82.0  0.371174  -0.657132   \n",
       "2128  1.0   8.0  1080.285714   996.0  180.470587  448.0  0.587475  -1.363827   \n",
       "2129  3.0  25.0   391.250000   390.0    2.569857    8.0  0.605786  -0.869886   \n",
       "\n",
       "               8         9  ...       203        204   205   206         207  \\\n",
       "0     459.037295  1.000000  ...  0.928571 -10.000000  10.0   9.0  146.000000   \n",
       "1     398.464564  1.000000  ...  0.600000  64.000000   7.0   7.0  140.500000   \n",
       "2     340.802438  1.000000  ...  1.000000  26.000000   9.0   7.0  154.285714   \n",
       "3     412.324324  1.000000  ...  1.000000  -4.000000  12.0  12.0  108.500000   \n",
       "4     168.041577  0.956522  ...  0.083333   0.022262  11.0  12.0    0.044242   \n",
       "...          ...       ...  ...       ...        ...   ...   ...         ...   \n",
       "2125  364.303573  0.888889  ...  0.777778   0.000000   9.0   8.0  342.857143   \n",
       "2126  181.876516  1.000000  ...  1.000000 -26.000000   8.0   7.0  137.666667   \n",
       "2127  137.696567  1.000000  ...  1.000000  -8.000000  16.0  14.0  102.714286   \n",
       "2128  561.988537  1.000000  ...  1.000000  18.000000   8.0   5.0   62.400000   \n",
       "2129  654.123072  0.400000  ...  0.240000   4.000000   0.0   0.0    0.044242   \n",
       "\n",
       "               208         209           210         211          212  \n",
       "0       729.000000   78.250000   3140.437500  127.600000  1041.440000  \n",
       "1     15314.750000  -27.000000   5249.000000  112.285714  8081.632653  \n",
       "2      1944.489796   18.571429   8070.530612  131.111111  1078.320988  \n",
       "3      6122.750000   46.500000   7081.416667  121.833333   264.305556  \n",
       "4         0.044242  -50.000000      0.000000   45.818182   832.330579  \n",
       "...            ...         ...           ...         ...          ...  \n",
       "2125   2843.265306  205.142857  11207.836735   96.000000  2281.142857  \n",
       "2126    228.555556   87.714286  14282.775510  169.142857    46.693878  \n",
       "2127   1270.061224    7.285714    361.489796   90.400000  2186.240000  \n",
       "2128     51.840000  -45.200000      0.960000  101.000000  5002.000000  \n",
       "2129      0.044242    0.043021      0.043021    0.037385     0.037385  \n",
       "\n",
       "[2130 rows x 213 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv(\"../data_test_frequency.csv\")\n",
    "df_test.drop(columns=[\"Unnamed: 0\"],inplace=True)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = df_test.iloc[:,1:].values\n",
    "y_test = df_test.iloc[:,0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = scale.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (4258, 212)\n",
      "Vallidation: (4258, 212)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train , test_size=0.5, shuffle=True, stratify=y_train, random_state=119)\n",
    "print(f\"Train: {x_train.shape}\")\n",
    "print(f\"Vallidation: {x_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf_clf = RandomForestClassifier(criterion= 'log_loss', max_depth= 5, max_features= 'sqrt', n_estimators= 1000)\n",
    "# ab_clf = AdaBoostClassifier(algorithm= 'SAMME.R', learning_rate= 0.1, n_estimators= 50)\n",
    "# # knn_clf = KNeighborsClassifier(algorithm= 'auto', n_neighbors= 5, p= 1, weights= 'uniform')\n",
    "# svc_clf = SVC(C= 100, gamma= 'scale', kernel= 'rbf', probability= True)\n",
    "# xgb_clf = XGBClassifier(gamma= 0,learning_rate= 0.1,max_depth= 5,min_child_weight= 1,n_estimators= 1000)\n",
    "# dt_clf = DecisionTreeClassifier(criterion= 'entropy',max_depth= 5,max_features= 'sqrt',splitter= 'best')\n",
    "\n",
    "rf_clf = RandomForestClassifier(criterion= 'log_loss', max_depth= 15, max_features= 'sqrt', n_estimators= 40)\n",
    "ab_clf = AdaBoostClassifier(algorithm= 'SAMME.R', learning_rate= 0.1, n_estimators= 61)\n",
    "# knn_clf = KNeighborsClassifier(algorithm= 'auto', n_neighbors= 8, p= 1, weights= 'uniform')\n",
    "svc_clf = SVC(C= 15, gamma= 'scale', kernel= 'rbf', probability= True)\n",
    "xgb_clf = XGBClassifier(gamma= 0,learning_rate= 0.05,max_depth= 5,min_child_weight= 1,n_estimators= 1000)\n",
    "lr_clf = LogisticRegression(C= 0.14, max_iter= 20, multi_class= 'auto', n_jobs= -1, penalty= 'l2', solver= 'sag', tol= 0.015)\n",
    "dt_clf = DecisionTreeClassifier(criterion= 'gini',max_depth= 6,max_features= 'sqrt',splitter= 'best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=0.14, max_iter=20, n_jobs=-1, solver=&#x27;sag&#x27;, tol=0.015)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;LogisticRegression<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression(C=0.14, max_iter=20, n_jobs=-1, solver=&#x27;sag&#x27;, tol=0.015)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(C=0.14, max_iter=20, n_jobs=-1, solver='sag', tol=0.015)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Huấn luyện các mô hình con\n",
    "rf_clf.fit(x_train,y_train)\n",
    "ab_clf.fit(x_train, y_train)\n",
    "# knn_clf.fit(x_train, y_train)\n",
    "svc_clf.fit(x_train, y_train)\n",
    "xgb_clf.fit(x_train, y_train)\n",
    "dt_clf.fit(x_train,y_train)\n",
    "lr_clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dự đoán trên tập huấn luyện để tạo đặc trưng mới cho mô hình blending\n",
    "X_train_meta = np.column_stack((\n",
    "    rf_clf.predict_proba(x_val),\n",
    "    ab_clf.predict_proba(x_val),\n",
    "    # knn_clf.predict_proba(x_val),\n",
    "    svc_clf.predict_proba(x_val),\n",
    "    xgb_clf.predict_proba(x_val),\n",
    "    dt_clf.predict_proba(x_val),\n",
    "    lr_clf.predict_proba(x_val)\n",
    "))\n",
    "# Dự đoán trên tập kiểm tra để tạo đặc trưng mới cho mô hình blending\n",
    "X_test_meta = np.column_stack((\n",
    "    rf_clf.predict_proba(x_test),\n",
    "    ab_clf.predict_proba(x_test),\n",
    "    # knn_clf.predict_proba(x_test),\n",
    "    svc_clf.predict_proba(x_test),\n",
    "    xgb_clf.predict_proba(x_test),\n",
    "    dt_clf.predict_proba(x_test),\n",
    "    lr_clf.predict_proba(x_test)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_meta:(4258, 24)\n",
      "X_test_meta:(2130, 24)\n"
     ]
    }
   ],
   "source": [
    "print(f\"X_train_meta:{X_train_meta.shape}\")\n",
    "print(f\"X_test_meta:{X_test_meta.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 72 candidates, totalling 216 fits\n",
      "[CV 1/3] END algorithm=auto, n_neighbors=4, p=1, weights=uniform;, score=(train=0.964, test=0.951) total time=   0.1s\n",
      "[CV 2/3] END algorithm=auto, n_neighbors=4, p=1, weights=uniform;, score=(train=0.963, test=0.959) total time=   0.0s\n",
      "[CV 3/3] END algorithm=auto, n_neighbors=4, p=1, weights=uniform;, score=(train=0.968, test=0.953) total time=   0.0s\n",
      "[CV 1/3] END algorithm=auto, n_neighbors=4, p=1, weights=distance;, score=(train=1.000, test=0.951) total time=   0.0s\n",
      "[CV 2/3] END algorithm=auto, n_neighbors=4, p=1, weights=distance;, score=(train=1.000, test=0.960) total time=   0.0s\n",
      "[CV 3/3] END algorithm=auto, n_neighbors=4, p=1, weights=distance;, score=(train=1.000, test=0.952) total time=   0.0s\n",
      "[CV 1/3] END algorithm=auto, n_neighbors=4, p=2, weights=uniform;, score=(train=0.965, test=0.955) total time=   0.0s\n",
      "[CV 2/3] END algorithm=auto, n_neighbors=4, p=2, weights=uniform;, score=(train=0.961, test=0.961) total time=   0.0s\n",
      "[CV 3/3] END algorithm=auto, n_neighbors=4, p=2, weights=uniform;, score=(train=0.967, test=0.953) total time=   0.0s\n",
      "[CV 1/3] END algorithm=auto, n_neighbors=4, p=2, weights=distance;, score=(train=1.000, test=0.950) total time=   0.0s\n",
      "[CV 2/3] END algorithm=auto, n_neighbors=4, p=2, weights=distance;, score=(train=1.000, test=0.959) total time=   0.0s\n",
      "[CV 3/3] END algorithm=auto, n_neighbors=4, p=2, weights=distance;, score=(train=1.000, test=0.949) total time=   0.0s\n",
      "[CV 1/3] END algorithm=auto, n_neighbors=4, p=3, weights=uniform;, score=(train=0.963, test=0.953) total time=   0.4s\n",
      "[CV 2/3] END algorithm=auto, n_neighbors=4, p=3, weights=uniform;, score=(train=0.963, test=0.959) total time=   0.5s\n",
      "[CV 3/3] END algorithm=auto, n_neighbors=4, p=3, weights=uniform;, score=(train=0.968, test=0.951) total time=   0.4s\n",
      "[CV 1/3] END algorithm=auto, n_neighbors=4, p=3, weights=distance;, score=(train=1.000, test=0.952) total time=   0.4s\n",
      "[CV 2/3] END algorithm=auto, n_neighbors=4, p=3, weights=distance;, score=(train=1.000, test=0.961) total time=   0.5s\n",
      "[CV 3/3] END algorithm=auto, n_neighbors=4, p=3, weights=distance;, score=(train=1.000, test=0.949) total time=   0.4s\n",
      "[CV 1/3] END algorithm=auto, n_neighbors=5, p=1, weights=uniform;, score=(train=0.964, test=0.956) total time=   0.0s\n",
      "[CV 2/3] END algorithm=auto, n_neighbors=5, p=1, weights=uniform;, score=(train=0.962, test=0.962) total time=   0.0s\n",
      "[CV 3/3] END algorithm=auto, n_neighbors=5, p=1, weights=uniform;, score=(train=0.965, test=0.956) total time=   0.0s\n",
      "[CV 1/3] END algorithm=auto, n_neighbors=5, p=1, weights=distance;, score=(train=1.000, test=0.955) total time=   0.0s\n",
      "[CV 2/3] END algorithm=auto, n_neighbors=5, p=1, weights=distance;, score=(train=1.000, test=0.962) total time=   0.0s\n",
      "[CV 3/3] END algorithm=auto, n_neighbors=5, p=1, weights=distance;, score=(train=1.000, test=0.953) total time=   0.0s\n",
      "[CV 1/3] END algorithm=auto, n_neighbors=5, p=2, weights=uniform;, score=(train=0.963, test=0.955) total time=   0.0s\n",
      "[CV 2/3] END algorithm=auto, n_neighbors=5, p=2, weights=uniform;, score=(train=0.961, test=0.963) total time=   0.0s\n",
      "[CV 3/3] END algorithm=auto, n_neighbors=5, p=2, weights=uniform;, score=(train=0.964, test=0.953) total time=   0.0s\n",
      "[CV 1/3] END algorithm=auto, n_neighbors=5, p=2, weights=distance;, score=(train=1.000, test=0.953) total time=   0.0s\n",
      "[CV 2/3] END algorithm=auto, n_neighbors=5, p=2, weights=distance;, score=(train=1.000, test=0.961) total time=   0.0s\n",
      "[CV 3/3] END algorithm=auto, n_neighbors=5, p=2, weights=distance;, score=(train=1.000, test=0.952) total time=   0.0s\n",
      "[CV 1/3] END algorithm=auto, n_neighbors=5, p=3, weights=uniform;, score=(train=0.963, test=0.954) total time=   0.4s\n",
      "[CV 2/3] END algorithm=auto, n_neighbors=5, p=3, weights=uniform;, score=(train=0.962, test=0.961) total time=   0.4s\n",
      "[CV 3/3] END algorithm=auto, n_neighbors=5, p=3, weights=uniform;, score=(train=0.962, test=0.953) total time=   0.4s\n",
      "[CV 1/3] END algorithm=auto, n_neighbors=5, p=3, weights=distance;, score=(train=1.000, test=0.952) total time=   0.4s\n",
      "[CV 2/3] END algorithm=auto, n_neighbors=5, p=3, weights=distance;, score=(train=1.000, test=0.960) total time=   0.5s\n",
      "[CV 3/3] END algorithm=auto, n_neighbors=5, p=3, weights=distance;, score=(train=1.000, test=0.951) total time=   0.5s\n",
      "[CV 1/3] END algorithm=auto, n_neighbors=6, p=1, weights=uniform;, score=(train=0.963, test=0.956) total time=   0.0s\n",
      "[CV 2/3] END algorithm=auto, n_neighbors=6, p=1, weights=uniform;, score=(train=0.959, test=0.961) total time=   0.0s\n",
      "[CV 3/3] END algorithm=auto, n_neighbors=6, p=1, weights=uniform;, score=(train=0.961, test=0.954) total time=   0.0s\n",
      "[CV 1/3] END algorithm=auto, n_neighbors=6, p=1, weights=distance;, score=(train=1.000, test=0.954) total time=   0.0s\n",
      "[CV 2/3] END algorithm=auto, n_neighbors=6, p=1, weights=distance;, score=(train=1.000, test=0.963) total time=   0.0s\n",
      "[CV 3/3] END algorithm=auto, n_neighbors=6, p=1, weights=distance;, score=(train=1.000, test=0.956) total time=   0.0s\n",
      "[CV 1/3] END algorithm=auto, n_neighbors=6, p=2, weights=uniform;, score=(train=0.963, test=0.954) total time=   0.0s\n",
      "[CV 2/3] END algorithm=auto, n_neighbors=6, p=2, weights=uniform;, score=(train=0.959, test=0.963) total time=   0.0s\n",
      "[CV 3/3] END algorithm=auto, n_neighbors=6, p=2, weights=uniform;, score=(train=0.963, test=0.953) total time=   0.0s\n",
      "[CV 1/3] END algorithm=auto, n_neighbors=6, p=2, weights=distance;, score=(train=1.000, test=0.953) total time=   0.0s\n",
      "[CV 2/3] END algorithm=auto, n_neighbors=6, p=2, weights=distance;, score=(train=1.000, test=0.964) total time=   0.0s\n",
      "[CV 3/3] END algorithm=auto, n_neighbors=6, p=2, weights=distance;, score=(train=1.000, test=0.950) total time=   0.0s\n",
      "[CV 1/3] END algorithm=auto, n_neighbors=6, p=3, weights=uniform;, score=(train=0.961, test=0.956) total time=   0.5s\n",
      "[CV 2/3] END algorithm=auto, n_neighbors=6, p=3, weights=uniform;, score=(train=0.961, test=0.963) total time=   0.4s\n",
      "[CV 3/3] END algorithm=auto, n_neighbors=6, p=3, weights=uniform;, score=(train=0.962, test=0.953) total time=   0.4s\n",
      "[CV 1/3] END algorithm=auto, n_neighbors=6, p=3, weights=distance;, score=(train=1.000, test=0.955) total time=   0.4s\n",
      "[CV 2/3] END algorithm=auto, n_neighbors=6, p=3, weights=distance;, score=(train=1.000, test=0.962) total time=   0.6s\n",
      "[CV 3/3] END algorithm=auto, n_neighbors=6, p=3, weights=distance;, score=(train=1.000, test=0.951) total time=   0.8s\n",
      "[CV 1/3] END algorithm=ball_tree, n_neighbors=4, p=1, weights=uniform;, score=(train=0.964, test=0.951) total time=   0.0s\n",
      "[CV 2/3] END algorithm=ball_tree, n_neighbors=4, p=1, weights=uniform;, score=(train=0.963, test=0.959) total time=   0.0s\n",
      "[CV 3/3] END algorithm=ball_tree, n_neighbors=4, p=1, weights=uniform;, score=(train=0.968, test=0.953) total time=   0.0s\n",
      "[CV 1/3] END algorithm=ball_tree, n_neighbors=4, p=1, weights=distance;, score=(train=1.000, test=0.951) total time=   0.0s\n",
      "[CV 2/3] END algorithm=ball_tree, n_neighbors=4, p=1, weights=distance;, score=(train=1.000, test=0.960) total time=   0.0s\n",
      "[CV 3/3] END algorithm=ball_tree, n_neighbors=4, p=1, weights=distance;, score=(train=1.000, test=0.952) total time=   0.0s\n",
      "[CV 1/3] END algorithm=ball_tree, n_neighbors=4, p=2, weights=uniform;, score=(train=0.965, test=0.955) total time=   0.0s\n",
      "[CV 2/3] END algorithm=ball_tree, n_neighbors=4, p=2, weights=uniform;, score=(train=0.961, test=0.961) total time=   0.0s\n",
      "[CV 3/3] END algorithm=ball_tree, n_neighbors=4, p=2, weights=uniform;, score=(train=0.967, test=0.953) total time=   0.0s\n",
      "[CV 1/3] END algorithm=ball_tree, n_neighbors=4, p=2, weights=distance;, score=(train=1.000, test=0.950) total time=   0.0s\n",
      "[CV 2/3] END algorithm=ball_tree, n_neighbors=4, p=2, weights=distance;, score=(train=1.000, test=0.959) total time=   0.0s\n",
      "[CV 3/3] END algorithm=ball_tree, n_neighbors=4, p=2, weights=distance;, score=(train=1.000, test=0.949) total time=   0.0s\n",
      "[CV 1/3] END algorithm=ball_tree, n_neighbors=4, p=3, weights=uniform;, score=(train=0.963, test=0.953) total time=   0.7s\n",
      "[CV 2/3] END algorithm=ball_tree, n_neighbors=4, p=3, weights=uniform;, score=(train=0.963, test=0.959) total time=   1.2s\n",
      "[CV 3/3] END algorithm=ball_tree, n_neighbors=4, p=3, weights=uniform;, score=(train=0.968, test=0.951) total time=   1.3s\n",
      "[CV 1/3] END algorithm=ball_tree, n_neighbors=4, p=3, weights=distance;, score=(train=1.000, test=0.952) total time=   1.0s\n",
      "[CV 2/3] END algorithm=ball_tree, n_neighbors=4, p=3, weights=distance;, score=(train=1.000, test=0.961) total time=   1.0s\n",
      "[CV 3/3] END algorithm=ball_tree, n_neighbors=4, p=3, weights=distance;, score=(train=1.000, test=0.949) total time=   1.1s\n",
      "[CV 1/3] END algorithm=ball_tree, n_neighbors=5, p=1, weights=uniform;, score=(train=0.964, test=0.956) total time=   0.1s\n",
      "[CV 2/3] END algorithm=ball_tree, n_neighbors=5, p=1, weights=uniform;, score=(train=0.962, test=0.962) total time=   0.0s\n",
      "[CV 3/3] END algorithm=ball_tree, n_neighbors=5, p=1, weights=uniform;, score=(train=0.965, test=0.956) total time=   0.1s\n",
      "[CV 1/3] END algorithm=ball_tree, n_neighbors=5, p=1, weights=distance;, score=(train=1.000, test=0.955) total time=   0.0s\n",
      "[CV 2/3] END algorithm=ball_tree, n_neighbors=5, p=1, weights=distance;, score=(train=1.000, test=0.962) total time=   0.0s\n",
      "[CV 3/3] END algorithm=ball_tree, n_neighbors=5, p=1, weights=distance;, score=(train=1.000, test=0.953) total time=   0.0s\n",
      "[CV 1/3] END algorithm=ball_tree, n_neighbors=5, p=2, weights=uniform;, score=(train=0.963, test=0.955) total time=   0.1s\n",
      "[CV 2/3] END algorithm=ball_tree, n_neighbors=5, p=2, weights=uniform;, score=(train=0.961, test=0.963) total time=   0.0s\n",
      "[CV 3/3] END algorithm=ball_tree, n_neighbors=5, p=2, weights=uniform;, score=(train=0.964, test=0.953) total time=   0.0s\n",
      "[CV 1/3] END algorithm=ball_tree, n_neighbors=5, p=2, weights=distance;, score=(train=1.000, test=0.953) total time=   0.0s\n",
      "[CV 2/3] END algorithm=ball_tree, n_neighbors=5, p=2, weights=distance;, score=(train=1.000, test=0.961) total time=   0.0s\n",
      "[CV 3/3] END algorithm=ball_tree, n_neighbors=5, p=2, weights=distance;, score=(train=1.000, test=0.952) total time=   0.0s\n",
      "[CV 1/3] END algorithm=ball_tree, n_neighbors=5, p=3, weights=uniform;, score=(train=0.963, test=0.954) total time=   1.2s\n",
      "[CV 2/3] END algorithm=ball_tree, n_neighbors=5, p=3, weights=uniform;, score=(train=0.962, test=0.961) total time=   1.1s\n",
      "[CV 3/3] END algorithm=ball_tree, n_neighbors=5, p=3, weights=uniform;, score=(train=0.962, test=0.953) total time=   1.1s\n",
      "[CV 1/3] END algorithm=ball_tree, n_neighbors=5, p=3, weights=distance;, score=(train=1.000, test=0.952) total time=   1.1s\n",
      "[CV 2/3] END algorithm=ball_tree, n_neighbors=5, p=3, weights=distance;, score=(train=1.000, test=0.960) total time=   0.9s\n",
      "[CV 3/3] END algorithm=ball_tree, n_neighbors=5, p=3, weights=distance;, score=(train=1.000, test=0.951) total time=   1.0s\n",
      "[CV 1/3] END algorithm=ball_tree, n_neighbors=6, p=1, weights=uniform;, score=(train=0.963, test=0.956) total time=   0.1s\n",
      "[CV 2/3] END algorithm=ball_tree, n_neighbors=6, p=1, weights=uniform;, score=(train=0.959, test=0.961) total time=   0.0s\n",
      "[CV 3/3] END algorithm=ball_tree, n_neighbors=6, p=1, weights=uniform;, score=(train=0.961, test=0.954) total time=   0.1s\n",
      "[CV 1/3] END algorithm=ball_tree, n_neighbors=6, p=1, weights=distance;, score=(train=1.000, test=0.954) total time=   0.0s\n",
      "[CV 2/3] END algorithm=ball_tree, n_neighbors=6, p=1, weights=distance;, score=(train=1.000, test=0.963) total time=   0.0s\n",
      "[CV 3/3] END algorithm=ball_tree, n_neighbors=6, p=1, weights=distance;, score=(train=1.000, test=0.956) total time=   0.0s\n",
      "[CV 1/3] END algorithm=ball_tree, n_neighbors=6, p=2, weights=uniform;, score=(train=0.963, test=0.954) total time=   0.0s\n",
      "[CV 2/3] END algorithm=ball_tree, n_neighbors=6, p=2, weights=uniform;, score=(train=0.959, test=0.963) total time=   0.0s\n",
      "[CV 3/3] END algorithm=ball_tree, n_neighbors=6, p=2, weights=uniform;, score=(train=0.963, test=0.953) total time=   0.0s\n",
      "[CV 1/3] END algorithm=ball_tree, n_neighbors=6, p=2, weights=distance;, score=(train=1.000, test=0.953) total time=   0.0s\n",
      "[CV 2/3] END algorithm=ball_tree, n_neighbors=6, p=2, weights=distance;, score=(train=1.000, test=0.964) total time=   0.0s\n",
      "[CV 3/3] END algorithm=ball_tree, n_neighbors=6, p=2, weights=distance;, score=(train=1.000, test=0.950) total time=   0.0s\n",
      "[CV 1/3] END algorithm=ball_tree, n_neighbors=6, p=3, weights=uniform;, score=(train=0.961, test=0.956) total time=   1.2s\n",
      "[CV 2/3] END algorithm=ball_tree, n_neighbors=6, p=3, weights=uniform;, score=(train=0.961, test=0.963) total time=   1.2s\n",
      "[CV 3/3] END algorithm=ball_tree, n_neighbors=6, p=3, weights=uniform;, score=(train=0.962, test=0.953) total time=   1.3s\n",
      "[CV 1/3] END algorithm=ball_tree, n_neighbors=6, p=3, weights=distance;, score=(train=1.000, test=0.955) total time=   1.2s\n",
      "[CV 2/3] END algorithm=ball_tree, n_neighbors=6, p=3, weights=distance;, score=(train=1.000, test=0.962) total time=   1.1s\n",
      "[CV 3/3] END algorithm=ball_tree, n_neighbors=6, p=3, weights=distance;, score=(train=1.000, test=0.951) total time=   1.2s\n",
      "[CV 1/3] END algorithm=kd_tree, n_neighbors=4, p=1, weights=uniform;, score=(train=0.964, test=0.951) total time=   0.1s\n",
      "[CV 2/3] END algorithm=kd_tree, n_neighbors=4, p=1, weights=uniform;, score=(train=0.963, test=0.959) total time=   0.1s\n",
      "[CV 3/3] END algorithm=kd_tree, n_neighbors=4, p=1, weights=uniform;, score=(train=0.968, test=0.953) total time=   0.1s\n",
      "[CV 1/3] END algorithm=kd_tree, n_neighbors=4, p=1, weights=distance;, score=(train=1.000, test=0.951) total time=   0.0s\n",
      "[CV 2/3] END algorithm=kd_tree, n_neighbors=4, p=1, weights=distance;, score=(train=1.000, test=0.960) total time=   0.0s\n",
      "[CV 3/3] END algorithm=kd_tree, n_neighbors=4, p=1, weights=distance;, score=(train=1.000, test=0.952) total time=   0.0s\n",
      "[CV 1/3] END algorithm=kd_tree, n_neighbors=4, p=2, weights=uniform;, score=(train=0.965, test=0.955) total time=   0.1s\n",
      "[CV 2/3] END algorithm=kd_tree, n_neighbors=4, p=2, weights=uniform;, score=(train=0.961, test=0.961) total time=   0.1s\n",
      "[CV 3/3] END algorithm=kd_tree, n_neighbors=4, p=2, weights=uniform;, score=(train=0.967, test=0.953) total time=   0.1s\n",
      "[CV 1/3] END algorithm=kd_tree, n_neighbors=4, p=2, weights=distance;, score=(train=1.000, test=0.950) total time=   0.0s\n",
      "[CV 2/3] END algorithm=kd_tree, n_neighbors=4, p=2, weights=distance;, score=(train=1.000, test=0.959) total time=   0.0s\n",
      "[CV 3/3] END algorithm=kd_tree, n_neighbors=4, p=2, weights=distance;, score=(train=1.000, test=0.949) total time=   0.0s\n",
      "[CV 1/3] END algorithm=kd_tree, n_neighbors=4, p=3, weights=uniform;, score=(train=0.963, test=0.953) total time=   0.5s\n",
      "[CV 2/3] END algorithm=kd_tree, n_neighbors=4, p=3, weights=uniform;, score=(train=0.963, test=0.959) total time=   0.5s\n",
      "[CV 3/3] END algorithm=kd_tree, n_neighbors=4, p=3, weights=uniform;, score=(train=0.968, test=0.951) total time=   0.5s\n",
      "[CV 1/3] END algorithm=kd_tree, n_neighbors=4, p=3, weights=distance;, score=(train=1.000, test=0.952) total time=   0.4s\n",
      "[CV 2/3] END algorithm=kd_tree, n_neighbors=4, p=3, weights=distance;, score=(train=1.000, test=0.961) total time=   0.4s\n",
      "[CV 3/3] END algorithm=kd_tree, n_neighbors=4, p=3, weights=distance;, score=(train=1.000, test=0.949) total time=   0.4s\n",
      "[CV 1/3] END algorithm=kd_tree, n_neighbors=5, p=1, weights=uniform;, score=(train=0.964, test=0.956) total time=   0.1s\n",
      "[CV 2/3] END algorithm=kd_tree, n_neighbors=5, p=1, weights=uniform;, score=(train=0.962, test=0.962) total time=   0.0s\n",
      "[CV 3/3] END algorithm=kd_tree, n_neighbors=5, p=1, weights=uniform;, score=(train=0.965, test=0.956) total time=   0.1s\n",
      "[CV 1/3] END algorithm=kd_tree, n_neighbors=5, p=1, weights=distance;, score=(train=1.000, test=0.955) total time=   0.0s\n",
      "[CV 2/3] END algorithm=kd_tree, n_neighbors=5, p=1, weights=distance;, score=(train=1.000, test=0.962) total time=   0.0s\n",
      "[CV 3/3] END algorithm=kd_tree, n_neighbors=5, p=1, weights=distance;, score=(train=1.000, test=0.953) total time=   0.0s\n",
      "[CV 1/3] END algorithm=kd_tree, n_neighbors=5, p=2, weights=uniform;, score=(train=0.963, test=0.955) total time=   0.0s\n",
      "[CV 2/3] END algorithm=kd_tree, n_neighbors=5, p=2, weights=uniform;, score=(train=0.961, test=0.963) total time=   0.0s\n",
      "[CV 3/3] END algorithm=kd_tree, n_neighbors=5, p=2, weights=uniform;, score=(train=0.964, test=0.953) total time=   0.1s\n",
      "[CV 1/3] END algorithm=kd_tree, n_neighbors=5, p=2, weights=distance;, score=(train=1.000, test=0.953) total time=   0.0s\n",
      "[CV 2/3] END algorithm=kd_tree, n_neighbors=5, p=2, weights=distance;, score=(train=1.000, test=0.961) total time=   0.0s\n",
      "[CV 3/3] END algorithm=kd_tree, n_neighbors=5, p=2, weights=distance;, score=(train=1.000, test=0.952) total time=   0.0s\n",
      "[CV 1/3] END algorithm=kd_tree, n_neighbors=5, p=3, weights=uniform;, score=(train=0.963, test=0.954) total time=   0.5s\n",
      "[CV 2/3] END algorithm=kd_tree, n_neighbors=5, p=3, weights=uniform;, score=(train=0.962, test=0.961) total time=   0.5s\n",
      "[CV 3/3] END algorithm=kd_tree, n_neighbors=5, p=3, weights=uniform;, score=(train=0.962, test=0.953) total time=   0.5s\n",
      "[CV 1/3] END algorithm=kd_tree, n_neighbors=5, p=3, weights=distance;, score=(train=1.000, test=0.952) total time=   0.4s\n",
      "[CV 2/3] END algorithm=kd_tree, n_neighbors=5, p=3, weights=distance;, score=(train=1.000, test=0.960) total time=   0.5s\n",
      "[CV 3/3] END algorithm=kd_tree, n_neighbors=5, p=3, weights=distance;, score=(train=1.000, test=0.951) total time=   0.4s\n",
      "[CV 1/3] END algorithm=kd_tree, n_neighbors=6, p=1, weights=uniform;, score=(train=0.963, test=0.956) total time=   0.0s\n",
      "[CV 2/3] END algorithm=kd_tree, n_neighbors=6, p=1, weights=uniform;, score=(train=0.959, test=0.961) total time=   0.0s\n",
      "[CV 3/3] END algorithm=kd_tree, n_neighbors=6, p=1, weights=uniform;, score=(train=0.961, test=0.954) total time=   0.0s\n",
      "[CV 1/3] END algorithm=kd_tree, n_neighbors=6, p=1, weights=distance;, score=(train=1.000, test=0.954) total time=   0.0s\n",
      "[CV 2/3] END algorithm=kd_tree, n_neighbors=6, p=1, weights=distance;, score=(train=1.000, test=0.963) total time=   0.0s\n",
      "[CV 3/3] END algorithm=kd_tree, n_neighbors=6, p=1, weights=distance;, score=(train=1.000, test=0.956) total time=   0.0s\n",
      "[CV 1/3] END algorithm=kd_tree, n_neighbors=6, p=2, weights=uniform;, score=(train=0.963, test=0.954) total time=   0.0s\n",
      "[CV 2/3] END algorithm=kd_tree, n_neighbors=6, p=2, weights=uniform;, score=(train=0.959, test=0.963) total time=   0.0s\n",
      "[CV 3/3] END algorithm=kd_tree, n_neighbors=6, p=2, weights=uniform;, score=(train=0.963, test=0.953) total time=   0.0s\n",
      "[CV 1/3] END algorithm=kd_tree, n_neighbors=6, p=2, weights=distance;, score=(train=1.000, test=0.953) total time=   0.0s\n",
      "[CV 2/3] END algorithm=kd_tree, n_neighbors=6, p=2, weights=distance;, score=(train=1.000, test=0.964) total time=   0.0s\n",
      "[CV 3/3] END algorithm=kd_tree, n_neighbors=6, p=2, weights=distance;, score=(train=1.000, test=0.950) total time=   0.0s\n",
      "[CV 1/3] END algorithm=kd_tree, n_neighbors=6, p=3, weights=uniform;, score=(train=0.961, test=0.956) total time=   0.4s\n",
      "[CV 2/3] END algorithm=kd_tree, n_neighbors=6, p=3, weights=uniform;, score=(train=0.961, test=0.963) total time=   0.3s\n",
      "[CV 3/3] END algorithm=kd_tree, n_neighbors=6, p=3, weights=uniform;, score=(train=0.962, test=0.953) total time=   0.4s\n",
      "[CV 1/3] END algorithm=kd_tree, n_neighbors=6, p=3, weights=distance;, score=(train=1.000, test=0.955) total time=   0.3s\n",
      "[CV 2/3] END algorithm=kd_tree, n_neighbors=6, p=3, weights=distance;, score=(train=1.000, test=0.962) total time=   0.3s\n",
      "[CV 3/3] END algorithm=kd_tree, n_neighbors=6, p=3, weights=distance;, score=(train=1.000, test=0.951) total time=   0.3s\n",
      "[CV 1/3] END algorithm=brute, n_neighbors=4, p=1, weights=uniform;, score=(train=0.964, test=0.951) total time=   0.0s\n",
      "[CV 2/3] END algorithm=brute, n_neighbors=4, p=1, weights=uniform;, score=(train=0.963, test=0.959) total time=   0.0s\n",
      "[CV 3/3] END algorithm=brute, n_neighbors=4, p=1, weights=uniform;, score=(train=0.968, test=0.953) total time=   0.0s\n",
      "[CV 1/3] END algorithm=brute, n_neighbors=4, p=1, weights=distance;, score=(train=1.000, test=0.951) total time=   0.0s\n",
      "[CV 2/3] END algorithm=brute, n_neighbors=4, p=1, weights=distance;, score=(train=1.000, test=0.960) total time=   0.0s\n",
      "[CV 3/3] END algorithm=brute, n_neighbors=4, p=1, weights=distance;, score=(train=1.000, test=0.952) total time=   0.1s\n",
      "[CV 1/3] END algorithm=brute, n_neighbors=4, p=2, weights=uniform;, score=(train=0.965, test=0.955) total time=   0.0s\n",
      "[CV 2/3] END algorithm=brute, n_neighbors=4, p=2, weights=uniform;, score=(train=0.961, test=0.961) total time=   0.0s\n",
      "[CV 3/3] END algorithm=brute, n_neighbors=4, p=2, weights=uniform;, score=(train=0.967, test=0.953) total time=   0.0s\n",
      "[CV 1/3] END algorithm=brute, n_neighbors=4, p=2, weights=distance;, score=(train=1.000, test=0.950) total time=   0.1s\n",
      "[CV 2/3] END algorithm=brute, n_neighbors=4, p=2, weights=distance;, score=(train=1.000, test=0.959) total time=   0.0s\n",
      "[CV 3/3] END algorithm=brute, n_neighbors=4, p=2, weights=distance;, score=(train=1.000, test=0.949) total time=   0.0s\n",
      "[CV 1/3] END algorithm=brute, n_neighbors=4, p=3, weights=uniform;, score=(train=0.963, test=0.953) total time=   0.8s\n",
      "[CV 2/3] END algorithm=brute, n_neighbors=4, p=3, weights=uniform;, score=(train=0.963, test=0.959) total time=   0.9s\n",
      "[CV 3/3] END algorithm=brute, n_neighbors=4, p=3, weights=uniform;, score=(train=0.968, test=0.951) total time=   0.7s\n",
      "[CV 1/3] END algorithm=brute, n_neighbors=4, p=3, weights=distance;, score=(train=1.000, test=0.952) total time=   0.7s\n",
      "[CV 2/3] END algorithm=brute, n_neighbors=4, p=3, weights=distance;, score=(train=1.000, test=0.961) total time=   0.6s\n",
      "[CV 3/3] END algorithm=brute, n_neighbors=4, p=3, weights=distance;, score=(train=1.000, test=0.949) total time=   0.6s\n",
      "[CV 1/3] END algorithm=brute, n_neighbors=5, p=1, weights=uniform;, score=(train=0.964, test=0.956) total time=   0.0s\n",
      "[CV 2/3] END algorithm=brute, n_neighbors=5, p=1, weights=uniform;, score=(train=0.962, test=0.962) total time=   0.0s\n",
      "[CV 3/3] END algorithm=brute, n_neighbors=5, p=1, weights=uniform;, score=(train=0.965, test=0.956) total time=   0.0s\n",
      "[CV 1/3] END algorithm=brute, n_neighbors=5, p=1, weights=distance;, score=(train=1.000, test=0.955) total time=   0.0s\n",
      "[CV 2/3] END algorithm=brute, n_neighbors=5, p=1, weights=distance;, score=(train=1.000, test=0.962) total time=   0.0s\n",
      "[CV 3/3] END algorithm=brute, n_neighbors=5, p=1, weights=distance;, score=(train=1.000, test=0.953) total time=   0.0s\n",
      "[CV 1/3] END algorithm=brute, n_neighbors=5, p=2, weights=uniform;, score=(train=0.963, test=0.955) total time=   0.0s\n",
      "[CV 2/3] END algorithm=brute, n_neighbors=5, p=2, weights=uniform;, score=(train=0.961, test=0.963) total time=   0.0s\n",
      "[CV 3/3] END algorithm=brute, n_neighbors=5, p=2, weights=uniform;, score=(train=0.964, test=0.953) total time=   0.0s\n",
      "[CV 1/3] END algorithm=brute, n_neighbors=5, p=2, weights=distance;, score=(train=1.000, test=0.953) total time=   0.0s\n",
      "[CV 2/3] END algorithm=brute, n_neighbors=5, p=2, weights=distance;, score=(train=1.000, test=0.961) total time=   0.0s\n",
      "[CV 3/3] END algorithm=brute, n_neighbors=5, p=2, weights=distance;, score=(train=1.000, test=0.952) total time=   0.0s\n",
      "[CV 1/3] END algorithm=brute, n_neighbors=5, p=3, weights=uniform;, score=(train=0.963, test=0.954) total time=   0.7s\n",
      "[CV 2/3] END algorithm=brute, n_neighbors=5, p=3, weights=uniform;, score=(train=0.962, test=0.961) total time=   0.6s\n",
      "[CV 3/3] END algorithm=brute, n_neighbors=5, p=3, weights=uniform;, score=(train=0.962, test=0.953) total time=   0.6s\n",
      "[CV 1/3] END algorithm=brute, n_neighbors=5, p=3, weights=distance;, score=(train=1.000, test=0.952) total time=   0.8s\n",
      "[CV 2/3] END algorithm=brute, n_neighbors=5, p=3, weights=distance;, score=(train=1.000, test=0.960) total time=   0.7s\n",
      "[CV 3/3] END algorithm=brute, n_neighbors=5, p=3, weights=distance;, score=(train=1.000, test=0.951) total time=   1.3s\n",
      "[CV 1/3] END algorithm=brute, n_neighbors=6, p=1, weights=uniform;, score=(train=0.963, test=0.956) total time=   0.1s\n",
      "[CV 2/3] END algorithm=brute, n_neighbors=6, p=1, weights=uniform;, score=(train=0.959, test=0.961) total time=   0.0s\n",
      "[CV 3/3] END algorithm=brute, n_neighbors=6, p=1, weights=uniform;, score=(train=0.961, test=0.954) total time=   0.1s\n",
      "[CV 1/3] END algorithm=brute, n_neighbors=6, p=1, weights=distance;, score=(train=1.000, test=0.954) total time=   0.0s\n",
      "[CV 2/3] END algorithm=brute, n_neighbors=6, p=1, weights=distance;, score=(train=1.000, test=0.963) total time=   0.0s\n",
      "[CV 3/3] END algorithm=brute, n_neighbors=6, p=1, weights=distance;, score=(train=1.000, test=0.956) total time=   0.0s\n",
      "[CV 1/3] END algorithm=brute, n_neighbors=6, p=2, weights=uniform;, score=(train=0.963, test=0.954) total time=   0.0s\n",
      "[CV 2/3] END algorithm=brute, n_neighbors=6, p=2, weights=uniform;, score=(train=0.959, test=0.963) total time=   0.0s\n",
      "[CV 3/3] END algorithm=brute, n_neighbors=6, p=2, weights=uniform;, score=(train=0.963, test=0.953) total time=   0.0s\n",
      "[CV 1/3] END algorithm=brute, n_neighbors=6, p=2, weights=distance;, score=(train=1.000, test=0.953) total time=   0.0s\n",
      "[CV 2/3] END algorithm=brute, n_neighbors=6, p=2, weights=distance;, score=(train=1.000, test=0.964) total time=   0.0s\n",
      "[CV 3/3] END algorithm=brute, n_neighbors=6, p=2, weights=distance;, score=(train=1.000, test=0.950) total time=   0.0s\n",
      "[CV 1/3] END algorithm=brute, n_neighbors=6, p=3, weights=uniform;, score=(train=0.961, test=0.956) total time=   0.8s\n",
      "[CV 2/3] END algorithm=brute, n_neighbors=6, p=3, weights=uniform;, score=(train=0.961, test=0.963) total time=   0.6s\n",
      "[CV 3/3] END algorithm=brute, n_neighbors=6, p=3, weights=uniform;, score=(train=0.962, test=0.953) total time=   0.6s\n",
      "[CV 1/3] END algorithm=brute, n_neighbors=6, p=3, weights=distance;, score=(train=1.000, test=0.955) total time=   0.8s\n",
      "[CV 2/3] END algorithm=brute, n_neighbors=6, p=3, weights=distance;, score=(train=1.000, test=0.962) total time=   1.1s\n",
      "[CV 3/3] END algorithm=brute, n_neighbors=6, p=3, weights=distance;, score=(train=1.000, test=0.951) total time=   0.8s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "model = KNeighborsClassifier()\n",
    "params = {\n",
    "    'n_neighbors': [4,5,6],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    'p': [1,2,3]\n",
    "}\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=params, cv=3, verbose=5, return_train_score=True,refit=True)\n",
    "grid_model = grid_search.fit(X_train_meta,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_test = grid_model.predict(X_test_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'algorithm': 'auto', 'n_neighbors': 5, 'p': 1, 'weights': 'uniform'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9579618656264578"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_model.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay,multilabel_confusion_matrix,f1_score,precision_score,accuracy_score,recall_score,precision_recall_fscore_support\n",
    "def evaluation_test(y,y_pred):\n",
    "    cm = confusion_matrix(y,y_pred)\n",
    "    disp = ConfusionMatrixDisplay(cm,display_labels=['AFIB','SB','SR','GSVT'])\n",
    "    disp.plot()\n",
    "    plt.show()\n",
    "    n_classes = len(cm)\n",
    "    result = []\n",
    "    for c in range(n_classes):\n",
    "        tp = cm[c,c]\n",
    "        fp = sum(cm[:,c]) - cm[c,c]\n",
    "        fn = sum(cm[c,:]) - cm[c,c]\n",
    "        tn = sum(np.delete(sum(cm)-cm[c,:],c))\n",
    "        acc = (tp+tn) / (tp+fn+tn+fp)\n",
    "        recall = tp/(tp+fn)\n",
    "        precision = tp/(tp+fp)\n",
    "        specificity = tn/(tn+fp)\n",
    "        f1_score = 2*((precision*recall)/(precision+recall))\n",
    "        if c+1 == 1:\n",
    "            Rhythm = 'AFIB'\n",
    "        elif c+1 == 2:\n",
    "            Rhythm = 'SB'\n",
    "        elif c+1 == 3:\n",
    "            Rhythm = 'SR'\n",
    "        else:\n",
    "            Rhythm = 'GSVT'\n",
    "        result.append([Rhythm,acc,recall,precision,f1_score,specificity])\n",
    "    p_macro,r_macro,f_macro,support_macro = precision_recall_fscore_support(y,y_pred,average='macro')\n",
    "    p_micro,r_micro,f_micro,support_micro = precision_recall_fscore_support(y,y_pred,average='micro')\n",
    "    p_weighted,r_weighted,f_weighted,support_weighted = precision_recall_fscore_support(y,y_pred,average='weighted')\n",
    "    result.append(['macro avg',None,f_macro,p_macro,r_macro,None])\n",
    "    result.append(['micro avg',None,f_micro,p_micro,r_micro,None])\n",
    "    result.append(['weighted avg',None,f_weighted,p_weighted,r_weighted,None])\n",
    "    return result\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rhythm Group</th>\n",
       "      <th>ACC</th>\n",
       "      <th>F1-score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>specificity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AFIB</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.928090</td>\n",
       "      <td>0.913717</td>\n",
       "      <td>0.920847</td>\n",
       "      <td>0.976855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SB</td>\n",
       "      <td>0.991080</td>\n",
       "      <td>0.991003</td>\n",
       "      <td>0.984674</td>\n",
       "      <td>0.987828</td>\n",
       "      <td>0.991124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SR</td>\n",
       "      <td>0.987324</td>\n",
       "      <td>0.970787</td>\n",
       "      <td>0.968610</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.991691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GSVT</td>\n",
       "      <td>0.970423</td>\n",
       "      <td>0.917749</td>\n",
       "      <td>0.944321</td>\n",
       "      <td>0.930845</td>\n",
       "      <td>0.985012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>macro avg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.952304</td>\n",
       "      <td>0.952830</td>\n",
       "      <td>0.951907</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>micro avg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.957746</td>\n",
       "      <td>0.957746</td>\n",
       "      <td>0.957746</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>weighted avg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.957687</td>\n",
       "      <td>0.957741</td>\n",
       "      <td>0.957746</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rhythm Group       ACC  F1-score  Precision    Recall  specificity\n",
       "0          AFIB  0.966667  0.928090   0.913717  0.920847     0.976855\n",
       "1            SB  0.991080  0.991003   0.984674  0.987828     0.991124\n",
       "2            SR  0.987324  0.970787   0.968610  0.969697     0.991691\n",
       "3          GSVT  0.970423  0.917749   0.944321  0.930845     0.985012\n",
       "4     macro avg       NaN  0.952304   0.952830  0.951907          NaN\n",
       "5     micro avg       NaN  0.957746   0.957746  0.957746          NaN\n",
       "6  weighted avg       NaN  0.957687   0.957741  0.957746          NaN"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_test = evaluation_test(y_test,result_test)\n",
    "df_evaluation_test = pd.DataFrame(data=evaluation_test,columns=[\"Rhythm Group\",\"ACC\",\"F1-score\",\"Precision\",\"Recall\",\"specificity\"])\n",
    "df_evaluation_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_evaluation_test.to_csv(\"./Result/Blending_KNN.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testdatasets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
