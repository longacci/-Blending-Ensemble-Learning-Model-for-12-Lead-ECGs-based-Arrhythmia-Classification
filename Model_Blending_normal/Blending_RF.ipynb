{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>204</th>\n",
       "      <th>205</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>950.000000</td>\n",
       "      <td>1074.0</td>\n",
       "      <td>274.986868</td>\n",
       "      <td>782.0</td>\n",
       "      <td>-0.319753</td>\n",
       "      <td>-1.432466</td>\n",
       "      <td>325.821586</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>172.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>252.222222</td>\n",
       "      <td>10656.395062</td>\n",
       "      <td>87.777778</td>\n",
       "      <td>10339.061728</td>\n",
       "      <td>135.800000</td>\n",
       "      <td>4315.560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>574.500000</td>\n",
       "      <td>582.0</td>\n",
       "      <td>104.913059</td>\n",
       "      <td>378.0</td>\n",
       "      <td>0.158313</td>\n",
       "      <td>-0.696295</td>\n",
       "      <td>336.569414</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>-15.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>158.000000</td>\n",
       "      <td>3944.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>6555.000000</td>\n",
       "      <td>-1.066667</td>\n",
       "      <td>697.528889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>593.600000</td>\n",
       "      <td>594.0</td>\n",
       "      <td>4.687572</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.396421</td>\n",
       "      <td>-0.312612</td>\n",
       "      <td>94.909877</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>122.400000</td>\n",
       "      <td>2058.773333</td>\n",
       "      <td>12.533333</td>\n",
       "      <td>1360.782222</td>\n",
       "      <td>95.500000</td>\n",
       "      <td>68.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>420.090909</td>\n",
       "      <td>420.0</td>\n",
       "      <td>3.591772</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-0.021014</td>\n",
       "      <td>-0.856142</td>\n",
       "      <td>254.059787</td>\n",
       "      <td>0.826087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.739130</td>\n",
       "      <td>-9.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>40.666667</td>\n",
       "      <td>1120.888889</td>\n",
       "      <td>5.333333</td>\n",
       "      <td>1504.888889</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>1464.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1068.750000</td>\n",
       "      <td>1075.0</td>\n",
       "      <td>25.118469</td>\n",
       "      <td>76.0</td>\n",
       "      <td>-0.276816</td>\n",
       "      <td>-1.271399</td>\n",
       "      <td>461.130814</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>671.000000</td>\n",
       "      <td>19.750000</td>\n",
       "      <td>569.437500</td>\n",
       "      <td>136.444444</td>\n",
       "      <td>43.358025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8511</th>\n",
       "      <td>3.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>615.733333</td>\n",
       "      <td>596.0</td>\n",
       "      <td>51.114860</td>\n",
       "      <td>152.0</td>\n",
       "      <td>2.153820</td>\n",
       "      <td>2.645687</td>\n",
       "      <td>365.256750</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003757</td>\n",
       "      <td>0.022262</td>\n",
       "      <td>0.003757</td>\n",
       "      <td>0.003757</td>\n",
       "      <td>0.044242</td>\n",
       "      <td>0.044242</td>\n",
       "      <td>0.043021</td>\n",
       "      <td>0.043021</td>\n",
       "      <td>0.037385</td>\n",
       "      <td>0.037385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8512</th>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1091.500000</td>\n",
       "      <td>1093.0</td>\n",
       "      <td>5.894913</td>\n",
       "      <td>18.0</td>\n",
       "      <td>-0.311206</td>\n",
       "      <td>-1.184514</td>\n",
       "      <td>358.414529</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>81.428571</td>\n",
       "      <td>1294.530612</td>\n",
       "      <td>-40.000000</td>\n",
       "      <td>1746.285714</td>\n",
       "      <td>155.333333</td>\n",
       "      <td>4722.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8513</th>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>654.428571</td>\n",
       "      <td>648.0</td>\n",
       "      <td>107.653355</td>\n",
       "      <td>458.0</td>\n",
       "      <td>0.475616</td>\n",
       "      <td>0.784000</td>\n",
       "      <td>180.045117</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>77.142857</td>\n",
       "      <td>2213.551020</td>\n",
       "      <td>-1.714286</td>\n",
       "      <td>2686.204082</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>3602.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8514</th>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1075.000000</td>\n",
       "      <td>1083.0</td>\n",
       "      <td>24.535688</td>\n",
       "      <td>66.0</td>\n",
       "      <td>-0.263431</td>\n",
       "      <td>-1.567800</td>\n",
       "      <td>251.455499</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>101.142857</td>\n",
       "      <td>4933.551020</td>\n",
       "      <td>-10.750000</td>\n",
       "      <td>7259.937500</td>\n",
       "      <td>88.222222</td>\n",
       "      <td>202.172840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8515</th>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1041.250000</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>8.242421</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.214800</td>\n",
       "      <td>-1.575835</td>\n",
       "      <td>505.203302</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>350.000000</td>\n",
       "      <td>-20.000000</td>\n",
       "      <td>588.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8516 rows × 213 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        1     2            3       4           5      6         7         8  \\\n",
       "0     0.0  10.0   950.000000  1074.0  274.986868  782.0 -0.319753 -1.432466   \n",
       "1     0.0  17.0   574.500000   582.0  104.913059  378.0  0.158313 -0.696295   \n",
       "2     3.0  16.0   593.600000   594.0    4.687572   18.0  0.396421 -0.312612   \n",
       "3     3.0  23.0   420.090909   420.0    3.591772   12.0 -0.021014 -0.856142   \n",
       "4     1.0   9.0  1068.750000  1075.0   25.118469   76.0 -0.276816 -1.271399   \n",
       "...   ...   ...          ...     ...         ...    ...       ...       ...   \n",
       "8511  3.0  16.0   615.733333   596.0   51.114860  152.0  2.153820  2.645687   \n",
       "8512  1.0   9.0  1091.500000  1093.0    5.894913   18.0 -0.311206 -1.184514   \n",
       "8513  2.0  15.0   654.428571   648.0  107.653355  458.0  0.475616  0.784000   \n",
       "8514  1.0   9.0  1075.000000  1083.0   24.535688   66.0 -0.263431 -1.567800   \n",
       "8515  1.0   9.0  1041.250000  1040.0    8.242421   22.0  0.214800 -1.575835   \n",
       "\n",
       "               9        10  ...       204         205        206        207  \\\n",
       "0     325.821586  1.000000  ...  1.000000  172.000000  10.000000   9.000000   \n",
       "1     336.569414  1.000000  ...  0.882353  -15.000000  15.000000   4.000000   \n",
       "2      94.909877  1.000000  ...  1.000000   -4.000000  16.000000  15.000000   \n",
       "3     254.059787  0.826087  ...  0.739130   -9.000000   6.000000   4.000000   \n",
       "4     461.130814  1.000000  ...  1.000000    2.000000   9.000000   8.000000   \n",
       "...          ...       ...  ...       ...         ...        ...        ...   \n",
       "8511  365.256750  1.000000  ...  0.003757    0.022262   0.003757   0.003757   \n",
       "8512  358.414529  1.000000  ...  0.888889   -3.000000   9.000000   8.000000   \n",
       "8513  180.045117  1.000000  ...  1.000000   -4.000000  15.000000  14.000000   \n",
       "8514  251.455499  1.000000  ...  1.000000   14.000000   9.000000   8.000000   \n",
       "8515  505.203302  1.000000  ...  1.000000    0.000000   9.000000   8.000000   \n",
       "\n",
       "             208           209        210           211         212  \\\n",
       "0     252.222222  10656.395062  87.777778  10339.061728  135.800000   \n",
       "1     158.000000   3944.000000  73.000000   6555.000000   -1.066667   \n",
       "2     122.400000   2058.773333  12.533333   1360.782222   95.500000   \n",
       "3      40.666667   1120.888889   5.333333   1504.888889   12.000000   \n",
       "4     122.000000    671.000000  19.750000    569.437500  136.444444   \n",
       "...          ...           ...        ...           ...         ...   \n",
       "8511    0.044242      0.044242   0.043021      0.043021    0.037385   \n",
       "8512   81.428571   1294.530612 -40.000000   1746.285714  155.333333   \n",
       "8513   77.142857   2213.551020  -1.714286   2686.204082  104.000000   \n",
       "8514  101.142857   4933.551020 -10.750000   7259.937500   88.222222   \n",
       "8515  102.000000    350.000000 -20.000000    588.000000  150.000000   \n",
       "\n",
       "              213  \n",
       "0     4315.560000  \n",
       "1      697.528889  \n",
       "2       68.750000  \n",
       "3     1464.000000  \n",
       "4       43.358025  \n",
       "...           ...  \n",
       "8511     0.037385  \n",
       "8512  4722.666667  \n",
       "8513  3602.666667  \n",
       "8514   202.172840  \n",
       "8515     0.000000  \n",
       "\n",
       "[8516 rows x 213 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"../data_train_frequency.csv\")\n",
    "df_train.drop(columns=[\"Unnamed: 0\"],inplace=True)\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = df_train.iloc[:,1:].values\n",
    "y_train = df_train.iloc[:,0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = MinMaxScaler()\n",
    "x_train = scale.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>203</th>\n",
       "      <th>204</th>\n",
       "      <th>205</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>710.769231</td>\n",
       "      <td>628.0</td>\n",
       "      <td>153.204817</td>\n",
       "      <td>556.0</td>\n",
       "      <td>0.996355</td>\n",
       "      <td>0.207174</td>\n",
       "      <td>459.037295</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>-10.000000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>729.000000</td>\n",
       "      <td>78.250000</td>\n",
       "      <td>3140.437500</td>\n",
       "      <td>127.600000</td>\n",
       "      <td>1041.440000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>968.666667</td>\n",
       "      <td>894.0</td>\n",
       "      <td>266.399867</td>\n",
       "      <td>932.0</td>\n",
       "      <td>0.979352</td>\n",
       "      <td>0.388359</td>\n",
       "      <td>398.464564</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>140.500000</td>\n",
       "      <td>15314.750000</td>\n",
       "      <td>-27.000000</td>\n",
       "      <td>5249.000000</td>\n",
       "      <td>112.285714</td>\n",
       "      <td>8081.632653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>797.000000</td>\n",
       "      <td>780.0</td>\n",
       "      <td>251.329664</td>\n",
       "      <td>794.0</td>\n",
       "      <td>0.260470</td>\n",
       "      <td>-1.002325</td>\n",
       "      <td>340.802438</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>154.285714</td>\n",
       "      <td>1944.489796</td>\n",
       "      <td>18.571429</td>\n",
       "      <td>8070.530612</td>\n",
       "      <td>131.111111</td>\n",
       "      <td>1078.320988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>757.500000</td>\n",
       "      <td>755.0</td>\n",
       "      <td>8.986100</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.048579</td>\n",
       "      <td>-1.449012</td>\n",
       "      <td>412.324324</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>108.500000</td>\n",
       "      <td>6122.750000</td>\n",
       "      <td>46.500000</td>\n",
       "      <td>7081.416667</td>\n",
       "      <td>121.833333</td>\n",
       "      <td>264.305556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>413.909091</td>\n",
       "      <td>409.0</td>\n",
       "      <td>82.344017</td>\n",
       "      <td>426.0</td>\n",
       "      <td>3.023659</td>\n",
       "      <td>10.404884</td>\n",
       "      <td>168.041577</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.022262</td>\n",
       "      <td>11.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.044242</td>\n",
       "      <td>0.044242</td>\n",
       "      <td>-50.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>45.818182</td>\n",
       "      <td>832.330579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2125</th>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1071.250000</td>\n",
       "      <td>1062.0</td>\n",
       "      <td>36.509417</td>\n",
       "      <td>118.0</td>\n",
       "      <td>1.263183</td>\n",
       "      <td>0.543003</td>\n",
       "      <td>364.303573</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>342.857143</td>\n",
       "      <td>2843.265306</td>\n",
       "      <td>205.142857</td>\n",
       "      <td>11207.836735</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>2281.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2126</th>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1196.000000</td>\n",
       "      <td>1202.0</td>\n",
       "      <td>33.839959</td>\n",
       "      <td>102.0</td>\n",
       "      <td>-0.454057</td>\n",
       "      <td>-1.036905</td>\n",
       "      <td>181.876516</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-26.000000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>137.666667</td>\n",
       "      <td>228.555556</td>\n",
       "      <td>87.714286</td>\n",
       "      <td>14282.775510</td>\n",
       "      <td>169.142857</td>\n",
       "      <td>46.693878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2127</th>\n",
       "      <td>3.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>595.600000</td>\n",
       "      <td>590.0</td>\n",
       "      <td>23.734082</td>\n",
       "      <td>82.0</td>\n",
       "      <td>0.371174</td>\n",
       "      <td>-0.657132</td>\n",
       "      <td>137.696567</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-8.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>102.714286</td>\n",
       "      <td>1270.061224</td>\n",
       "      <td>7.285714</td>\n",
       "      <td>361.489796</td>\n",
       "      <td>90.400000</td>\n",
       "      <td>2186.240000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2128</th>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1080.285714</td>\n",
       "      <td>996.0</td>\n",
       "      <td>180.470587</td>\n",
       "      <td>448.0</td>\n",
       "      <td>0.587475</td>\n",
       "      <td>-1.363827</td>\n",
       "      <td>561.988537</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>62.400000</td>\n",
       "      <td>51.840000</td>\n",
       "      <td>-45.200000</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>5002.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2129</th>\n",
       "      <td>3.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>391.250000</td>\n",
       "      <td>390.0</td>\n",
       "      <td>2.569857</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.605786</td>\n",
       "      <td>-0.869886</td>\n",
       "      <td>654.123072</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.044242</td>\n",
       "      <td>0.044242</td>\n",
       "      <td>0.043021</td>\n",
       "      <td>0.043021</td>\n",
       "      <td>0.037385</td>\n",
       "      <td>0.037385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2130 rows × 213 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0     1            2       3           4      5         6          7  \\\n",
       "0     0.0  14.0   710.769231   628.0  153.204817  556.0  0.996355   0.207174   \n",
       "1     0.0  10.0   968.666667   894.0  266.399867  932.0  0.979352   0.388359   \n",
       "2     0.0  11.0   797.000000   780.0  251.329664  794.0  0.260470  -1.002325   \n",
       "3     2.0  13.0   757.500000   755.0    8.986100   26.0  0.048579  -1.449012   \n",
       "4     0.0  23.0   413.909091   409.0   82.344017  426.0  3.023659  10.404884   \n",
       "...   ...   ...          ...     ...         ...    ...       ...        ...   \n",
       "2125  1.0   9.0  1071.250000  1062.0   36.509417  118.0  1.263183   0.543003   \n",
       "2126  1.0   8.0  1196.000000  1202.0   33.839959  102.0 -0.454057  -1.036905   \n",
       "2127  3.0  16.0   595.600000   590.0   23.734082   82.0  0.371174  -0.657132   \n",
       "2128  1.0   8.0  1080.285714   996.0  180.470587  448.0  0.587475  -1.363827   \n",
       "2129  3.0  25.0   391.250000   390.0    2.569857    8.0  0.605786  -0.869886   \n",
       "\n",
       "               8         9  ...       203        204   205   206         207  \\\n",
       "0     459.037295  1.000000  ...  0.928571 -10.000000  10.0   9.0  146.000000   \n",
       "1     398.464564  1.000000  ...  0.600000  64.000000   7.0   7.0  140.500000   \n",
       "2     340.802438  1.000000  ...  1.000000  26.000000   9.0   7.0  154.285714   \n",
       "3     412.324324  1.000000  ...  1.000000  -4.000000  12.0  12.0  108.500000   \n",
       "4     168.041577  0.956522  ...  0.083333   0.022262  11.0  12.0    0.044242   \n",
       "...          ...       ...  ...       ...        ...   ...   ...         ...   \n",
       "2125  364.303573  0.888889  ...  0.777778   0.000000   9.0   8.0  342.857143   \n",
       "2126  181.876516  1.000000  ...  1.000000 -26.000000   8.0   7.0  137.666667   \n",
       "2127  137.696567  1.000000  ...  1.000000  -8.000000  16.0  14.0  102.714286   \n",
       "2128  561.988537  1.000000  ...  1.000000  18.000000   8.0   5.0   62.400000   \n",
       "2129  654.123072  0.400000  ...  0.240000   4.000000   0.0   0.0    0.044242   \n",
       "\n",
       "               208         209           210         211          212  \n",
       "0       729.000000   78.250000   3140.437500  127.600000  1041.440000  \n",
       "1     15314.750000  -27.000000   5249.000000  112.285714  8081.632653  \n",
       "2      1944.489796   18.571429   8070.530612  131.111111  1078.320988  \n",
       "3      6122.750000   46.500000   7081.416667  121.833333   264.305556  \n",
       "4         0.044242  -50.000000      0.000000   45.818182   832.330579  \n",
       "...            ...         ...           ...         ...          ...  \n",
       "2125   2843.265306  205.142857  11207.836735   96.000000  2281.142857  \n",
       "2126    228.555556   87.714286  14282.775510  169.142857    46.693878  \n",
       "2127   1270.061224    7.285714    361.489796   90.400000  2186.240000  \n",
       "2128     51.840000  -45.200000      0.960000  101.000000  5002.000000  \n",
       "2129      0.044242    0.043021      0.043021    0.037385     0.037385  \n",
       "\n",
       "[2130 rows x 213 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv(\"../data_test_frequency.csv\")\n",
    "df_test.drop(columns=[\"Unnamed: 0\"],inplace=True)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = df_test.iloc[:,1:].values\n",
    "y_test = df_test.iloc[:,0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = scale.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (4258, 212)\n",
      "Vallidation: (4258, 212)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train , test_size=0.5, shuffle=True, stratify=y_train, random_state=119)\n",
    "print(f\"Train: {x_train.shape}\")\n",
    "print(f\"Vallidation: {x_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.81205691e-01, 5.91696502e-01, 5.89185724e-01, ...,\n",
       "        1.26903300e-02, 4.65904451e-01, 1.15640194e-03],\n",
       "       [4.06213397e-01, 4.19438368e-01, 4.13600247e-01, ...,\n",
       "        7.01662380e-03, 4.23438138e-01, 9.57455155e-03],\n",
       "       [3.12457618e-01, 5.39122904e-01, 5.42920345e-01, ...,\n",
       "        0.00000000e+00, 2.44239631e-01, 9.01164403e-02],\n",
       "       ...,\n",
       "       [6.24976882e-01, 2.70970165e-01, 2.50835297e-01, ...,\n",
       "        6.93757972e-02, 4.13638220e-01, 1.36942334e-04],\n",
       "       [2.81205691e-01, 6.15739023e-01, 6.08695222e-01, ...,\n",
       "        4.13544324e-04, 4.98162515e-01, 2.86616027e-03],\n",
       "       [2.49953765e-01, 6.23347162e-01, 6.22073163e-01, ...,\n",
       "        1.77154195e-03, 4.98979175e-01, 4.35433829e-04]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf_clf = RandomForestClassifier(criterion= 'log_loss', max_depth= 5, max_features= 'sqrt', n_estimators= 1000)\n",
    "ab_clf = AdaBoostClassifier(algorithm= 'SAMME.R', learning_rate= 0.1, n_estimators= 50)\n",
    "knn_clf = KNeighborsClassifier(algorithm= 'auto', n_neighbors= 5, p= 1, weights= 'uniform')\n",
    "svc_clf = SVC(C= 100, gamma= 'scale', kernel= 'rbf', probability= True)\n",
    "xgb_clf = XGBClassifier(gamma= 0,learning_rate= 0.1,max_depth= 5,min_child_weight= 1,n_estimators= 1000)\n",
    "dt_clf = DecisionTreeClassifier(criterion= 'entropy',max_depth= 5,max_features= 'sqrt',splitter= 'best')\n",
    "lr_clf = LogisticRegression(C= 0.14, max_iter= 20, multi_class= 'auto', n_jobs= -1, penalty= 'l2', solver= 'sag', tol= 0.015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=0.14, max_iter=20, n_jobs=-1, solver=&#x27;sag&#x27;, tol=0.015)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;LogisticRegression<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression(C=0.14, max_iter=20, n_jobs=-1, solver=&#x27;sag&#x27;, tol=0.015)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(C=0.14, max_iter=20, n_jobs=-1, solver='sag', tol=0.015)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Huấn luyện các mô hình con\n",
    "# rf_clf.fit(x_train,y_train)\n",
    "ab_clf.fit(x_train, y_train)\n",
    "knn_clf.fit(x_train, y_train)\n",
    "svc_clf.fit(x_train, y_train)\n",
    "xgb_clf.fit(x_train, y_train)\n",
    "dt_clf.fit(x_train,y_train)\n",
    "lr_clf.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dự đoán trên tập huấn luyện để tạo đặc trưng mới cho mô hình blending\n",
    "X_train_meta = np.column_stack((\n",
    "    # rf_clf.predict_proba(x_val),\n",
    "    ab_clf.predict_proba(x_val),\n",
    "    knn_clf.predict_proba(x_val),\n",
    "    svc_clf.predict_proba(x_val),\n",
    "    xgb_clf.predict_proba(x_val),\n",
    "    dt_clf.predict_proba(x_val),\n",
    "    lr_clf.predict_proba(x_val),\n",
    "))\n",
    "# Dự đoán trên tập kiểm tra để tạo đặc trưng mới cho mô hình blending\n",
    "X_test_meta = np.column_stack((\n",
    "    # rf_clf.predict_proba(x_test),\n",
    "    ab_clf.predict_proba(x_test),\n",
    "    knn_clf.predict_proba(x_test),\n",
    "    svc_clf.predict_proba(x_test),\n",
    "    xgb_clf.predict_proba(x_test),\n",
    "    dt_clf.predict_proba(x_test),\n",
    "    lr_clf.predict_proba(x_test)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_meta:(4258, 24)\n",
      "X_test_meta:(2130, 24)\n"
     ]
    }
   ],
   "source": [
    "print(f\"X_train_meta:{X_train_meta.shape}\")\n",
    "print(f\"X_test_meta:{X_test_meta.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 72 candidates, totalling 216 fits\n",
      "[CV 1/3] END criterion=gini, max_depth=3, max_features=sqrt, n_estimators=10;, score=(train=0.962, test=0.958) total time=   0.0s\n",
      "[CV 2/3] END criterion=gini, max_depth=3, max_features=sqrt, n_estimators=10;, score=(train=0.961, test=0.964) total time=   0.0s\n",
      "[CV 3/3] END criterion=gini, max_depth=3, max_features=sqrt, n_estimators=10;, score=(train=0.960, test=0.956) total time=   0.0s\n",
      "[CV 1/3] END criterion=gini, max_depth=3, max_features=sqrt, n_estimators=50;, score=(train=0.962, test=0.958) total time=   0.1s\n",
      "[CV 2/3] END criterion=gini, max_depth=3, max_features=sqrt, n_estimators=50;, score=(train=0.955, test=0.964) total time=   0.0s\n",
      "[CV 3/3] END criterion=gini, max_depth=3, max_features=sqrt, n_estimators=50;, score=(train=0.962, test=0.955) total time=   0.0s\n",
      "[CV 1/3] END criterion=gini, max_depth=3, max_features=sqrt, n_estimators=100;, score=(train=0.962, test=0.956) total time=   0.2s\n",
      "[CV 2/3] END criterion=gini, max_depth=3, max_features=sqrt, n_estimators=100;, score=(train=0.958, test=0.965) total time=   0.2s\n",
      "[CV 3/3] END criterion=gini, max_depth=3, max_features=sqrt, n_estimators=100;, score=(train=0.962, test=0.953) total time=   0.2s\n",
      "[CV 1/3] END criterion=gini, max_depth=3, max_features=sqrt, n_estimators=1000;, score=(train=0.963, test=0.953) total time=   2.7s\n",
      "[CV 2/3] END criterion=gini, max_depth=3, max_features=sqrt, n_estimators=1000;, score=(train=0.957, test=0.964) total time=   2.8s\n",
      "[CV 3/3] END criterion=gini, max_depth=3, max_features=sqrt, n_estimators=1000;, score=(train=0.960, test=0.955) total time=   2.4s\n",
      "[CV 1/3] END criterion=gini, max_depth=3, max_features=log2, n_estimators=10;, score=(train=0.962, test=0.954) total time=   0.0s\n",
      "[CV 2/3] END criterion=gini, max_depth=3, max_features=log2, n_estimators=10;, score=(train=0.956, test=0.962) total time=   0.0s\n",
      "[CV 3/3] END criterion=gini, max_depth=3, max_features=log2, n_estimators=10;, score=(train=0.961, test=0.951) total time=   0.0s\n",
      "[CV 1/3] END criterion=gini, max_depth=3, max_features=log2, n_estimators=50;, score=(train=0.962, test=0.955) total time=   0.0s\n",
      "[CV 2/3] END criterion=gini, max_depth=3, max_features=log2, n_estimators=50;, score=(train=0.957, test=0.964) total time=   0.0s\n",
      "[CV 3/3] END criterion=gini, max_depth=3, max_features=log2, n_estimators=50;, score=(train=0.960, test=0.954) total time=   0.0s\n",
      "[CV 1/3] END criterion=gini, max_depth=3, max_features=log2, n_estimators=100;, score=(train=0.962, test=0.954) total time=   0.1s\n",
      "[CV 2/3] END criterion=gini, max_depth=3, max_features=log2, n_estimators=100;, score=(train=0.956, test=0.965) total time=   0.1s\n",
      "[CV 3/3] END criterion=gini, max_depth=3, max_features=log2, n_estimators=100;, score=(train=0.961, test=0.954) total time=   0.1s\n",
      "[CV 1/3] END criterion=gini, max_depth=3, max_features=log2, n_estimators=1000;, score=(train=0.963, test=0.954) total time=   4.1s\n",
      "[CV 2/3] END criterion=gini, max_depth=3, max_features=log2, n_estimators=1000;, score=(train=0.958, test=0.965) total time=   4.3s\n",
      "[CV 3/3] END criterion=gini, max_depth=3, max_features=log2, n_estimators=1000;, score=(train=0.960, test=0.955) total time=   2.6s\n",
      "[CV 1/3] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=10;, score=(train=0.963, test=0.952) total time=   0.0s\n",
      "[CV 2/3] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=10;, score=(train=0.962, test=0.966) total time=   0.0s\n",
      "[CV 3/3] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=10;, score=(train=0.964, test=0.951) total time=   0.0s\n",
      "[CV 1/3] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=50;, score=(train=0.966, test=0.954) total time=   0.0s\n",
      "[CV 2/3] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=50;, score=(train=0.961, test=0.966) total time=   0.0s\n",
      "[CV 3/3] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=50;, score=(train=0.965, test=0.956) total time=   0.0s\n",
      "[CV 1/3] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=100;, score=(train=0.965, test=0.953) total time=   0.2s\n",
      "[CV 2/3] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=100;, score=(train=0.961, test=0.963) total time=   0.2s\n",
      "[CV 3/3] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=100;, score=(train=0.965, test=0.956) total time=   0.2s\n",
      "[CV 1/3] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=1000;, score=(train=0.966, test=0.954) total time=   2.7s\n",
      "[CV 2/3] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=1000;, score=(train=0.961, test=0.964) total time=   2.8s\n",
      "[CV 3/3] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=1000;, score=(train=0.967, test=0.953) total time=   2.8s\n",
      "[CV 1/3] END criterion=gini, max_depth=4, max_features=log2, n_estimators=10;, score=(train=0.964, test=0.955) total time=   0.0s\n",
      "[CV 2/3] END criterion=gini, max_depth=4, max_features=log2, n_estimators=10;, score=(train=0.961, test=0.963) total time=   0.0s\n",
      "[CV 3/3] END criterion=gini, max_depth=4, max_features=log2, n_estimators=10;, score=(train=0.964, test=0.951) total time=   0.0s\n",
      "[CV 1/3] END criterion=gini, max_depth=4, max_features=log2, n_estimators=50;, score=(train=0.965, test=0.956) total time=   0.0s\n",
      "[CV 2/3] END criterion=gini, max_depth=4, max_features=log2, n_estimators=50;, score=(train=0.962, test=0.964) total time=   0.0s\n",
      "[CV 3/3] END criterion=gini, max_depth=4, max_features=log2, n_estimators=50;, score=(train=0.965, test=0.956) total time=   0.0s\n",
      "[CV 1/3] END criterion=gini, max_depth=4, max_features=log2, n_estimators=100;, score=(train=0.965, test=0.956) total time=   0.2s\n",
      "[CV 2/3] END criterion=gini, max_depth=4, max_features=log2, n_estimators=100;, score=(train=0.962, test=0.965) total time=   0.2s\n",
      "[CV 3/3] END criterion=gini, max_depth=4, max_features=log2, n_estimators=100;, score=(train=0.965, test=0.955) total time=   0.2s\n",
      "[CV 1/3] END criterion=gini, max_depth=4, max_features=log2, n_estimators=1000;, score=(train=0.965, test=0.954) total time=   2.6s\n",
      "[CV 2/3] END criterion=gini, max_depth=4, max_features=log2, n_estimators=1000;, score=(train=0.961, test=0.964) total time=   2.8s\n",
      "[CV 3/3] END criterion=gini, max_depth=4, max_features=log2, n_estimators=1000;, score=(train=0.966, test=0.954) total time=   2.7s\n",
      "[CV 1/3] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=10;, score=(train=0.969, test=0.955) total time=   0.0s\n",
      "[CV 2/3] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=10;, score=(train=0.965, test=0.958) total time=   0.0s\n",
      "[CV 3/3] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=10;, score=(train=0.970, test=0.952) total time=   0.0s\n",
      "[CV 1/3] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=50;, score=(train=0.969, test=0.956) total time=   0.1s\n",
      "[CV 2/3] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=50;, score=(train=0.967, test=0.965) total time=   0.1s\n",
      "[CV 3/3] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=50;, score=(train=0.971, test=0.955) total time=   0.1s\n",
      "[CV 1/3] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=100;, score=(train=0.970, test=0.956) total time=   0.2s\n",
      "[CV 2/3] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=100;, score=(train=0.965, test=0.964) total time=   0.2s\n",
      "[CV 3/3] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=100;, score=(train=0.971, test=0.956) total time=   0.2s\n",
      "[CV 1/3] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=1000;, score=(train=0.970, test=0.956) total time=   3.0s\n",
      "[CV 2/3] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=1000;, score=(train=0.966, test=0.965) total time=   3.1s\n",
      "[CV 3/3] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=1000;, score=(train=0.971, test=0.955) total time=   3.0s\n",
      "[CV 1/3] END criterion=gini, max_depth=5, max_features=log2, n_estimators=10;, score=(train=0.969, test=0.956) total time=   0.0s\n",
      "[CV 2/3] END criterion=gini, max_depth=5, max_features=log2, n_estimators=10;, score=(train=0.965, test=0.963) total time=   0.0s\n",
      "[CV 3/3] END criterion=gini, max_depth=5, max_features=log2, n_estimators=10;, score=(train=0.969, test=0.955) total time=   0.0s\n",
      "[CV 1/3] END criterion=gini, max_depth=5, max_features=log2, n_estimators=50;, score=(train=0.968, test=0.958) total time=   0.1s\n",
      "[CV 2/3] END criterion=gini, max_depth=5, max_features=log2, n_estimators=50;, score=(train=0.965, test=0.965) total time=   0.1s\n",
      "[CV 3/3] END criterion=gini, max_depth=5, max_features=log2, n_estimators=50;, score=(train=0.971, test=0.955) total time=   0.1s\n",
      "[CV 1/3] END criterion=gini, max_depth=5, max_features=log2, n_estimators=100;, score=(train=0.971, test=0.957) total time=   0.2s\n",
      "[CV 2/3] END criterion=gini, max_depth=5, max_features=log2, n_estimators=100;, score=(train=0.966, test=0.966) total time=   0.2s\n",
      "[CV 3/3] END criterion=gini, max_depth=5, max_features=log2, n_estimators=100;, score=(train=0.970, test=0.956) total time=   0.2s\n",
      "[CV 1/3] END criterion=gini, max_depth=5, max_features=log2, n_estimators=1000;, score=(train=0.969, test=0.955) total time=   3.1s\n",
      "[CV 2/3] END criterion=gini, max_depth=5, max_features=log2, n_estimators=1000;, score=(train=0.967, test=0.965) total time=   3.1s\n",
      "[CV 3/3] END criterion=gini, max_depth=5, max_features=log2, n_estimators=1000;, score=(train=0.971, test=0.956) total time=   3.0s\n",
      "[CV 1/3] END criterion=entropy, max_depth=3, max_features=sqrt, n_estimators=10;, score=(train=0.961, test=0.954) total time=   0.0s\n",
      "[CV 2/3] END criterion=entropy, max_depth=3, max_features=sqrt, n_estimators=10;, score=(train=0.958, test=0.963) total time=   0.0s\n",
      "[CV 3/3] END criterion=entropy, max_depth=3, max_features=sqrt, n_estimators=10;, score=(train=0.959, test=0.951) total time=   0.0s\n",
      "[CV 1/3] END criterion=entropy, max_depth=3, max_features=sqrt, n_estimators=50;, score=(train=0.962, test=0.954) total time=   0.1s\n",
      "[CV 2/3] END criterion=entropy, max_depth=3, max_features=sqrt, n_estimators=50;, score=(train=0.957, test=0.963) total time=   0.1s\n",
      "[CV 3/3] END criterion=entropy, max_depth=3, max_features=sqrt, n_estimators=50;, score=(train=0.963, test=0.952) total time=   0.1s\n",
      "[CV 1/3] END criterion=entropy, max_depth=3, max_features=sqrt, n_estimators=100;, score=(train=0.964, test=0.956) total time=   0.2s\n",
      "[CV 2/3] END criterion=entropy, max_depth=3, max_features=sqrt, n_estimators=100;, score=(train=0.957, test=0.963) total time=   0.3s\n",
      "[CV 3/3] END criterion=entropy, max_depth=3, max_features=sqrt, n_estimators=100;, score=(train=0.962, test=0.956) total time=   0.4s\n",
      "[CV 1/3] END criterion=entropy, max_depth=3, max_features=sqrt, n_estimators=1000;, score=(train=0.963, test=0.954) total time=   3.1s\n",
      "[CV 2/3] END criterion=entropy, max_depth=3, max_features=sqrt, n_estimators=1000;, score=(train=0.957, test=0.963) total time=   3.1s\n",
      "[CV 3/3] END criterion=entropy, max_depth=3, max_features=sqrt, n_estimators=1000;, score=(train=0.961, test=0.955) total time=   3.2s\n",
      "[CV 1/3] END criterion=entropy, max_depth=3, max_features=log2, n_estimators=10;, score=(train=0.963, test=0.954) total time=   0.0s\n",
      "[CV 2/3] END criterion=entropy, max_depth=3, max_features=log2, n_estimators=10;, score=(train=0.955, test=0.957) total time=   0.0s\n",
      "[CV 3/3] END criterion=entropy, max_depth=3, max_features=log2, n_estimators=10;, score=(train=0.961, test=0.951) total time=   0.0s\n",
      "[CV 1/3] END criterion=entropy, max_depth=3, max_features=log2, n_estimators=50;, score=(train=0.963, test=0.957) total time=   0.1s\n",
      "[CV 2/3] END criterion=entropy, max_depth=3, max_features=log2, n_estimators=50;, score=(train=0.956, test=0.963) total time=   0.1s\n",
      "[CV 3/3] END criterion=entropy, max_depth=3, max_features=log2, n_estimators=50;, score=(train=0.961, test=0.954) total time=   0.1s\n",
      "[CV 1/3] END criterion=entropy, max_depth=3, max_features=log2, n_estimators=100;, score=(train=0.962, test=0.952) total time=   0.3s\n",
      "[CV 2/3] END criterion=entropy, max_depth=3, max_features=log2, n_estimators=100;, score=(train=0.958, test=0.964) total time=   0.2s\n",
      "[CV 3/3] END criterion=entropy, max_depth=3, max_features=log2, n_estimators=100;, score=(train=0.961, test=0.956) total time=   0.2s\n",
      "[CV 1/3] END criterion=entropy, max_depth=3, max_features=log2, n_estimators=1000;, score=(train=0.964, test=0.955) total time=   3.5s\n",
      "[CV 2/3] END criterion=entropy, max_depth=3, max_features=log2, n_estimators=1000;, score=(train=0.958, test=0.964) total time=   3.3s\n",
      "[CV 3/3] END criterion=entropy, max_depth=3, max_features=log2, n_estimators=1000;, score=(train=0.962, test=0.955) total time=   3.2s\n",
      "[CV 1/3] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=10;, score=(train=0.963, test=0.954) total time=   0.0s\n",
      "[CV 2/3] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=10;, score=(train=0.961, test=0.964) total time=   0.0s\n",
      "[CV 3/3] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=10;, score=(train=0.963, test=0.954) total time=   0.0s\n",
      "[CV 1/3] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=50;, score=(train=0.965, test=0.955) total time=   0.1s\n",
      "[CV 2/3] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=50;, score=(train=0.960, test=0.964) total time=   0.1s\n",
      "[CV 3/3] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=50;, score=(train=0.964, test=0.955) total time=   0.1s\n",
      "[CV 1/3] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=100;, score=(train=0.965, test=0.954) total time=   0.3s\n",
      "[CV 2/3] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=100;, score=(train=0.961, test=0.962) total time=   0.3s\n",
      "[CV 3/3] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=100;, score=(train=0.965, test=0.954) total time=   0.3s\n",
      "[CV 1/3] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=1000;, score=(train=0.966, test=0.955) total time=   3.7s\n",
      "[CV 2/3] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=1000;, score=(train=0.962, test=0.965) total time=   3.8s\n",
      "[CV 3/3] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=1000;, score=(train=0.966, test=0.957) total time=   3.6s\n",
      "[CV 1/3] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=10;, score=(train=0.964, test=0.958) total time=   0.0s\n",
      "[CV 2/3] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=10;, score=(train=0.959, test=0.963) total time=   0.0s\n",
      "[CV 3/3] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=10;, score=(train=0.963, test=0.956) total time=   0.0s\n",
      "[CV 1/3] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=50;, score=(train=0.966, test=0.954) total time=   0.1s\n",
      "[CV 2/3] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=50;, score=(train=0.960, test=0.964) total time=   0.1s\n",
      "[CV 3/3] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=50;, score=(train=0.963, test=0.955) total time=   0.1s\n",
      "[CV 1/3] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=100;, score=(train=0.965, test=0.957) total time=   0.3s\n",
      "[CV 2/3] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=100;, score=(train=0.962, test=0.964) total time=   0.3s\n",
      "[CV 3/3] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=100;, score=(train=0.966, test=0.956) total time=   0.3s\n",
      "[CV 1/3] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=1000;, score=(train=0.967, test=0.956) total time=   3.7s\n",
      "[CV 2/3] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=1000;, score=(train=0.962, test=0.964) total time=   3.8s\n",
      "[CV 3/3] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=1000;, score=(train=0.966, test=0.956) total time=   4.0s\n",
      "[CV 1/3] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=10;, score=(train=0.970, test=0.958) total time=   0.0s\n",
      "[CV 2/3] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=10;, score=(train=0.963, test=0.963) total time=   0.0s\n",
      "[CV 3/3] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=10;, score=(train=0.973, test=0.956) total time=   0.0s\n",
      "[CV 1/3] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=50;, score=(train=0.969, test=0.956) total time=   0.1s\n",
      "[CV 2/3] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=50;, score=(train=0.965, test=0.965) total time=   0.1s\n",
      "[CV 3/3] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=50;, score=(train=0.970, test=0.956) total time=   0.1s\n",
      "[CV 1/3] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=100;, score=(train=0.968, test=0.956) total time=   0.3s\n",
      "[CV 2/3] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=100;, score=(train=0.968, test=0.968) total time=   0.3s\n",
      "[CV 3/3] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=100;, score=(train=0.971, test=0.953) total time=   0.3s\n",
      "[CV 1/3] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=1000;, score=(train=0.968, test=0.956) total time=   4.1s\n",
      "[CV 2/3] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=1000;, score=(train=0.966, test=0.965) total time=   4.1s\n",
      "[CV 3/3] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=1000;, score=(train=0.971, test=0.954) total time=   4.3s\n",
      "[CV 1/3] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=10;, score=(train=0.968, test=0.954) total time=   0.0s\n",
      "[CV 2/3] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=10;, score=(train=0.965, test=0.961) total time=   0.0s\n",
      "[CV 3/3] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=10;, score=(train=0.970, test=0.956) total time=   0.0s\n",
      "[CV 1/3] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=50;, score=(train=0.969, test=0.955) total time=   0.1s\n",
      "[CV 2/3] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=50;, score=(train=0.965, test=0.965) total time=   0.1s\n",
      "[CV 3/3] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=50;, score=(train=0.970, test=0.956) total time=   0.1s\n",
      "[CV 1/3] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=100;, score=(train=0.970, test=0.958) total time=   0.3s\n",
      "[CV 2/3] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=100;, score=(train=0.965, test=0.966) total time=   0.4s\n",
      "[CV 3/3] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=100;, score=(train=0.971, test=0.959) total time=   0.3s\n",
      "[CV 1/3] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=1000;, score=(train=0.969, test=0.957) total time=   4.2s\n",
      "[CV 2/3] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=1000;, score=(train=0.966, test=0.965) total time=   4.0s\n",
      "[CV 3/3] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=1000;, score=(train=0.970, test=0.955) total time=   3.9s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=3, max_features=sqrt, n_estimators=10;, score=(train=0.963, test=0.953) total time=   0.0s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=3, max_features=sqrt, n_estimators=10;, score=(train=0.956, test=0.962) total time=   0.0s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=3, max_features=sqrt, n_estimators=10;, score=(train=0.960, test=0.946) total time=   0.0s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=3, max_features=sqrt, n_estimators=50;, score=(train=0.963, test=0.955) total time=   0.1s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=3, max_features=sqrt, n_estimators=50;, score=(train=0.957, test=0.964) total time=   0.1s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=3, max_features=sqrt, n_estimators=50;, score=(train=0.962, test=0.953) total time=   0.1s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=3, max_features=sqrt, n_estimators=100;, score=(train=0.961, test=0.956) total time=   0.2s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=3, max_features=sqrt, n_estimators=100;, score=(train=0.958, test=0.963) total time=   0.2s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=3, max_features=sqrt, n_estimators=100;, score=(train=0.962, test=0.956) total time=   0.2s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=3, max_features=sqrt, n_estimators=1000;, score=(train=0.963, test=0.955) total time=   3.0s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=3, max_features=sqrt, n_estimators=1000;, score=(train=0.957, test=0.963) total time=   3.1s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=3, max_features=sqrt, n_estimators=1000;, score=(train=0.962, test=0.956) total time=   3.5s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=3, max_features=log2, n_estimators=10;, score=(train=0.960, test=0.949) total time=   0.0s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=3, max_features=log2, n_estimators=10;, score=(train=0.957, test=0.961) total time=   0.0s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=3, max_features=log2, n_estimators=10;, score=(train=0.961, test=0.951) total time=   0.0s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=3, max_features=log2, n_estimators=50;, score=(train=0.963, test=0.952) total time=   0.2s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=3, max_features=log2, n_estimators=50;, score=(train=0.958, test=0.963) total time=   0.1s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=3, max_features=log2, n_estimators=50;, score=(train=0.962, test=0.954) total time=   0.1s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=3, max_features=log2, n_estimators=100;, score=(train=0.962, test=0.957) total time=   0.2s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=3, max_features=log2, n_estimators=100;, score=(train=0.958, test=0.963) total time=   0.2s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=3, max_features=log2, n_estimators=100;, score=(train=0.961, test=0.956) total time=   0.2s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=3, max_features=log2, n_estimators=1000;, score=(train=0.963, test=0.955) total time=   3.2s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=3, max_features=log2, n_estimators=1000;, score=(train=0.957, test=0.964) total time=   4.6s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=3, max_features=log2, n_estimators=1000;, score=(train=0.961, test=0.956) total time=   4.9s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=4, max_features=sqrt, n_estimators=10;, score=(train=0.965, test=0.954) total time=   0.0s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=4, max_features=sqrt, n_estimators=10;, score=(train=0.961, test=0.962) total time=   0.0s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=4, max_features=sqrt, n_estimators=10;, score=(train=0.966, test=0.953) total time=   0.0s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=4, max_features=sqrt, n_estimators=50;, score=(train=0.965, test=0.957) total time=   0.1s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=4, max_features=sqrt, n_estimators=50;, score=(train=0.961, test=0.964) total time=   0.1s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=4, max_features=sqrt, n_estimators=50;, score=(train=0.965, test=0.955) total time=   0.1s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=4, max_features=sqrt, n_estimators=100;, score=(train=0.965, test=0.956) total time=   0.3s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=4, max_features=sqrt, n_estimators=100;, score=(train=0.961, test=0.963) total time=   0.3s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=4, max_features=sqrt, n_estimators=100;, score=(train=0.966, test=0.955) total time=   0.3s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=4, max_features=sqrt, n_estimators=1000;, score=(train=0.966, test=0.955) total time=   3.6s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=4, max_features=sqrt, n_estimators=1000;, score=(train=0.961, test=0.965) total time=   3.8s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=4, max_features=sqrt, n_estimators=1000;, score=(train=0.965, test=0.955) total time=   3.6s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=4, max_features=log2, n_estimators=10;, score=(train=0.965, test=0.954) total time=   0.0s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=4, max_features=log2, n_estimators=10;, score=(train=0.960, test=0.963) total time=   0.0s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=4, max_features=log2, n_estimators=10;, score=(train=0.964, test=0.958) total time=   0.0s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=4, max_features=log2, n_estimators=50;, score=(train=0.965, test=0.955) total time=   0.1s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=4, max_features=log2, n_estimators=50;, score=(train=0.961, test=0.965) total time=   0.1s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=4, max_features=log2, n_estimators=50;, score=(train=0.965, test=0.956) total time=   0.1s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=4, max_features=log2, n_estimators=100;, score=(train=0.965, test=0.956) total time=   0.3s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=4, max_features=log2, n_estimators=100;, score=(train=0.960, test=0.966) total time=   0.3s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=4, max_features=log2, n_estimators=100;, score=(train=0.964, test=0.953) total time=   0.3s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=4, max_features=log2, n_estimators=1000;, score=(train=0.966, test=0.956) total time=   3.8s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=4, max_features=log2, n_estimators=1000;, score=(train=0.961, test=0.965) total time=   4.0s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=4, max_features=log2, n_estimators=1000;, score=(train=0.965, test=0.956) total time=   3.7s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=5, max_features=sqrt, n_estimators=10;, score=(train=0.969, test=0.956) total time=   0.0s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=5, max_features=sqrt, n_estimators=10;, score=(train=0.965, test=0.963) total time=   0.0s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=5, max_features=sqrt, n_estimators=10;, score=(train=0.969, test=0.952) total time=   0.0s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=5, max_features=sqrt, n_estimators=50;, score=(train=0.968, test=0.956) total time=   0.1s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=5, max_features=sqrt, n_estimators=50;, score=(train=0.965, test=0.966) total time=   0.2s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=5, max_features=sqrt, n_estimators=50;, score=(train=0.970, test=0.956) total time=   0.2s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=5, max_features=sqrt, n_estimators=100;, score=(train=0.968, test=0.959) total time=   0.6s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=5, max_features=sqrt, n_estimators=100;, score=(train=0.965, test=0.966) total time=   0.6s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=5, max_features=sqrt, n_estimators=100;, score=(train=0.970, test=0.955) total time=   0.6s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=5, max_features=sqrt, n_estimators=1000;, score=(train=0.968, test=0.957) total time=   6.7s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=5, max_features=sqrt, n_estimators=1000;, score=(train=0.966, test=0.966) total time=   6.7s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=5, max_features=sqrt, n_estimators=1000;, score=(train=0.971, test=0.956) total time=   6.5s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=5, max_features=log2, n_estimators=10;, score=(train=0.969, test=0.957) total time=   0.0s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=5, max_features=log2, n_estimators=10;, score=(train=0.966, test=0.964) total time=   0.0s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=5, max_features=log2, n_estimators=10;, score=(train=0.969, test=0.951) total time=   0.0s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=5, max_features=log2, n_estimators=50;, score=(train=0.969, test=0.959) total time=   0.2s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=5, max_features=log2, n_estimators=50;, score=(train=0.967, test=0.964) total time=   0.3s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=5, max_features=log2, n_estimators=50;, score=(train=0.971, test=0.956) total time=   0.2s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=5, max_features=log2, n_estimators=100;, score=(train=0.968, test=0.956) total time=   0.6s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=5, max_features=log2, n_estimators=100;, score=(train=0.964, test=0.967) total time=   0.6s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=5, max_features=log2, n_estimators=100;, score=(train=0.970, test=0.956) total time=   0.6s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=5, max_features=log2, n_estimators=1000;, score=(train=0.968, test=0.956) total time=   6.7s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=5, max_features=log2, n_estimators=1000;, score=(train=0.966, test=0.965) total time=   7.1s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=5, max_features=log2, n_estimators=1000;, score=(train=0.971, test=0.955) total time=   6.7s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "model = RandomForestClassifier()\n",
    "params = {\n",
    "    'n_estimators': [10,50,100,1000],\n",
    "    'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "    'max_depth': [3,4,5],\n",
    "    'max_features':['sqrt', 'log2'],\n",
    "}\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=params, cv=3, verbose=5, return_train_score=True,refit=True)\n",
    "grid_model = grid_search.fit(X_train_meta,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_test = grid_model.predict(X_test_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'entropy',\n",
       " 'max_depth': 5,\n",
       " 'max_features': 'log2',\n",
       " 'n_estimators': 100}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9612500703067358"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_model.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay,multilabel_confusion_matrix,f1_score,precision_score,accuracy_score,recall_score,precision_recall_fscore_support\n",
    "def evaluation_test(y,y_pred):\n",
    "    cm = confusion_matrix(y,y_pred)\n",
    "    disp = ConfusionMatrixDisplay(cm,display_labels=['AFIB','SB','SR','GSVT'])\n",
    "    disp.plot()\n",
    "    plt.show()\n",
    "    n_classes = len(cm)\n",
    "    result = []\n",
    "    for c in range(n_classes):\n",
    "        tp = cm[c,c]\n",
    "        fp = sum(cm[:,c]) - cm[c,c]\n",
    "        fn = sum(cm[c,:]) - cm[c,c]\n",
    "        tn = sum(np.delete(sum(cm)-cm[c,:],c))\n",
    "        acc = (tp+tn) / (tp+fn+tn+fp)\n",
    "        recall = tp/(tp+fn)\n",
    "        precision = tp/(tp+fp)\n",
    "        specificity = tn/(tn+fp)\n",
    "        f1_score = 2*((precision*recall)/(precision+recall))\n",
    "        if c+1 == 1:\n",
    "            Rhythm = 'AFIB'\n",
    "        elif c+1 == 2:\n",
    "            Rhythm = 'SB'\n",
    "        elif c+1 == 3:\n",
    "            Rhythm = 'SR'\n",
    "        else:\n",
    "            Rhythm = 'GSVT'\n",
    "        result.append([Rhythm,acc,recall,precision,f1_score,specificity])\n",
    "    p_macro,r_macro,f_macro,support_macro = precision_recall_fscore_support(y,y_pred,average='macro')\n",
    "    p_micro,r_micro,f_micro,support_micro = precision_recall_fscore_support(y,y_pred,average='micro')\n",
    "    p_weighted,r_weighted,f_weighted,support_weighted = precision_recall_fscore_support(y,y_pred,average='weighted')\n",
    "    result.append(['macro avg',None,f_macro,p_macro,r_macro,None])\n",
    "    result.append(['micro avg',None,f_micro,p_micro,r_micro,None])\n",
    "    result.append(['weighted avg',None,f_weighted,p_weighted,r_weighted,None])\n",
    "    return result\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rhythm Group</th>\n",
       "      <th>ACC</th>\n",
       "      <th>F1-score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>specificity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AFIB</td>\n",
       "      <td>0.969953</td>\n",
       "      <td>0.925843</td>\n",
       "      <td>0.930023</td>\n",
       "      <td>0.927928</td>\n",
       "      <td>0.981602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SB</td>\n",
       "      <td>0.992019</td>\n",
       "      <td>0.992288</td>\n",
       "      <td>0.985951</td>\n",
       "      <td>0.989110</td>\n",
       "      <td>0.991864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SR</td>\n",
       "      <td>0.989671</td>\n",
       "      <td>0.975281</td>\n",
       "      <td>0.975281</td>\n",
       "      <td>0.975281</td>\n",
       "      <td>0.993472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GSVT</td>\n",
       "      <td>0.972300</td>\n",
       "      <td>0.932900</td>\n",
       "      <td>0.938998</td>\n",
       "      <td>0.935939</td>\n",
       "      <td>0.983213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>macro avg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.957064</td>\n",
       "      <td>0.957563</td>\n",
       "      <td>0.956578</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>micro avg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.961972</td>\n",
       "      <td>0.961972</td>\n",
       "      <td>0.961972</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>weighted avg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.961906</td>\n",
       "      <td>0.961853</td>\n",
       "      <td>0.961972</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rhythm Group       ACC  F1-score  Precision    Recall  specificity\n",
       "0          AFIB  0.969953  0.925843   0.930023  0.927928     0.981602\n",
       "1            SB  0.992019  0.992288   0.985951  0.989110     0.991864\n",
       "2            SR  0.989671  0.975281   0.975281  0.975281     0.993472\n",
       "3          GSVT  0.972300  0.932900   0.938998  0.935939     0.983213\n",
       "4     macro avg       NaN  0.957064   0.957563  0.956578          NaN\n",
       "5     micro avg       NaN  0.961972   0.961972  0.961972          NaN\n",
       "6  weighted avg       NaN  0.961906   0.961853  0.961972          NaN"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_test = evaluation_test(y_test,result_test)\n",
    "df_evaluation_test = pd.DataFrame(data=evaluation_test,columns=[\"Rhythm Group\",\"ACC\",\"F1-score\",\"Precision\",\"Recall\",\"specificity\"])\n",
    "df_evaluation_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_evaluation_test.to_csv(\"./Result/Blending_RF.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testdatasets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
