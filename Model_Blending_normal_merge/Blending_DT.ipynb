{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>204</th>\n",
       "      <th>205</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>950.000000</td>\n",
       "      <td>1074.0</td>\n",
       "      <td>274.986868</td>\n",
       "      <td>782.0</td>\n",
       "      <td>-0.319753</td>\n",
       "      <td>-1.432466</td>\n",
       "      <td>325.821586</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>172.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>252.222222</td>\n",
       "      <td>10656.395062</td>\n",
       "      <td>87.777778</td>\n",
       "      <td>10339.061728</td>\n",
       "      <td>135.800000</td>\n",
       "      <td>4315.560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>574.500000</td>\n",
       "      <td>582.0</td>\n",
       "      <td>104.913059</td>\n",
       "      <td>378.0</td>\n",
       "      <td>0.158313</td>\n",
       "      <td>-0.696295</td>\n",
       "      <td>336.569414</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>-15.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>158.000000</td>\n",
       "      <td>3944.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>6555.000000</td>\n",
       "      <td>-1.066667</td>\n",
       "      <td>697.528889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>593.600000</td>\n",
       "      <td>594.0</td>\n",
       "      <td>4.687572</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.396421</td>\n",
       "      <td>-0.312612</td>\n",
       "      <td>94.909877</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>122.400000</td>\n",
       "      <td>2058.773333</td>\n",
       "      <td>12.533333</td>\n",
       "      <td>1360.782222</td>\n",
       "      <td>95.500000</td>\n",
       "      <td>68.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>420.090909</td>\n",
       "      <td>420.0</td>\n",
       "      <td>3.591772</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-0.021014</td>\n",
       "      <td>-0.856142</td>\n",
       "      <td>254.059787</td>\n",
       "      <td>0.826087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.739130</td>\n",
       "      <td>-9.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>40.666667</td>\n",
       "      <td>1120.888889</td>\n",
       "      <td>5.333333</td>\n",
       "      <td>1504.888889</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>1464.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1068.750000</td>\n",
       "      <td>1075.0</td>\n",
       "      <td>25.118469</td>\n",
       "      <td>76.0</td>\n",
       "      <td>-0.276816</td>\n",
       "      <td>-1.271399</td>\n",
       "      <td>461.130814</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>671.000000</td>\n",
       "      <td>19.750000</td>\n",
       "      <td>569.437500</td>\n",
       "      <td>136.444444</td>\n",
       "      <td>43.358025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8511</th>\n",
       "      <td>3.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>615.733333</td>\n",
       "      <td>596.0</td>\n",
       "      <td>51.114860</td>\n",
       "      <td>152.0</td>\n",
       "      <td>2.153820</td>\n",
       "      <td>2.645687</td>\n",
       "      <td>365.256750</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003757</td>\n",
       "      <td>0.022262</td>\n",
       "      <td>0.003757</td>\n",
       "      <td>0.003757</td>\n",
       "      <td>0.044242</td>\n",
       "      <td>0.044242</td>\n",
       "      <td>0.043021</td>\n",
       "      <td>0.043021</td>\n",
       "      <td>0.037385</td>\n",
       "      <td>0.037385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8512</th>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1091.500000</td>\n",
       "      <td>1093.0</td>\n",
       "      <td>5.894913</td>\n",
       "      <td>18.0</td>\n",
       "      <td>-0.311206</td>\n",
       "      <td>-1.184514</td>\n",
       "      <td>358.414529</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>81.428571</td>\n",
       "      <td>1294.530612</td>\n",
       "      <td>-40.000000</td>\n",
       "      <td>1746.285714</td>\n",
       "      <td>155.333333</td>\n",
       "      <td>4722.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8513</th>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>654.428571</td>\n",
       "      <td>648.0</td>\n",
       "      <td>107.653355</td>\n",
       "      <td>458.0</td>\n",
       "      <td>0.475616</td>\n",
       "      <td>0.784000</td>\n",
       "      <td>180.045117</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>77.142857</td>\n",
       "      <td>2213.551020</td>\n",
       "      <td>-1.714286</td>\n",
       "      <td>2686.204082</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>3602.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8514</th>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1075.000000</td>\n",
       "      <td>1083.0</td>\n",
       "      <td>24.535688</td>\n",
       "      <td>66.0</td>\n",
       "      <td>-0.263431</td>\n",
       "      <td>-1.567800</td>\n",
       "      <td>251.455499</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>101.142857</td>\n",
       "      <td>4933.551020</td>\n",
       "      <td>-10.750000</td>\n",
       "      <td>7259.937500</td>\n",
       "      <td>88.222222</td>\n",
       "      <td>202.172840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8515</th>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1041.250000</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>8.242421</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.214800</td>\n",
       "      <td>-1.575835</td>\n",
       "      <td>505.203302</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>350.000000</td>\n",
       "      <td>-20.000000</td>\n",
       "      <td>588.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8516 rows × 213 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        1     2            3       4           5      6         7         8  \\\n",
       "0     0.0  10.0   950.000000  1074.0  274.986868  782.0 -0.319753 -1.432466   \n",
       "1     0.0  17.0   574.500000   582.0  104.913059  378.0  0.158313 -0.696295   \n",
       "2     3.0  16.0   593.600000   594.0    4.687572   18.0  0.396421 -0.312612   \n",
       "3     3.0  23.0   420.090909   420.0    3.591772   12.0 -0.021014 -0.856142   \n",
       "4     1.0   9.0  1068.750000  1075.0   25.118469   76.0 -0.276816 -1.271399   \n",
       "...   ...   ...          ...     ...         ...    ...       ...       ...   \n",
       "8511  3.0  16.0   615.733333   596.0   51.114860  152.0  2.153820  2.645687   \n",
       "8512  1.0   9.0  1091.500000  1093.0    5.894913   18.0 -0.311206 -1.184514   \n",
       "8513  2.0  15.0   654.428571   648.0  107.653355  458.0  0.475616  0.784000   \n",
       "8514  1.0   9.0  1075.000000  1083.0   24.535688   66.0 -0.263431 -1.567800   \n",
       "8515  1.0   9.0  1041.250000  1040.0    8.242421   22.0  0.214800 -1.575835   \n",
       "\n",
       "               9        10  ...       204         205        206        207  \\\n",
       "0     325.821586  1.000000  ...  1.000000  172.000000  10.000000   9.000000   \n",
       "1     336.569414  1.000000  ...  0.882353  -15.000000  15.000000   4.000000   \n",
       "2      94.909877  1.000000  ...  1.000000   -4.000000  16.000000  15.000000   \n",
       "3     254.059787  0.826087  ...  0.739130   -9.000000   6.000000   4.000000   \n",
       "4     461.130814  1.000000  ...  1.000000    2.000000   9.000000   8.000000   \n",
       "...          ...       ...  ...       ...         ...        ...        ...   \n",
       "8511  365.256750  1.000000  ...  0.003757    0.022262   0.003757   0.003757   \n",
       "8512  358.414529  1.000000  ...  0.888889   -3.000000   9.000000   8.000000   \n",
       "8513  180.045117  1.000000  ...  1.000000   -4.000000  15.000000  14.000000   \n",
       "8514  251.455499  1.000000  ...  1.000000   14.000000   9.000000   8.000000   \n",
       "8515  505.203302  1.000000  ...  1.000000    0.000000   9.000000   8.000000   \n",
       "\n",
       "             208           209        210           211         212  \\\n",
       "0     252.222222  10656.395062  87.777778  10339.061728  135.800000   \n",
       "1     158.000000   3944.000000  73.000000   6555.000000   -1.066667   \n",
       "2     122.400000   2058.773333  12.533333   1360.782222   95.500000   \n",
       "3      40.666667   1120.888889   5.333333   1504.888889   12.000000   \n",
       "4     122.000000    671.000000  19.750000    569.437500  136.444444   \n",
       "...          ...           ...        ...           ...         ...   \n",
       "8511    0.044242      0.044242   0.043021      0.043021    0.037385   \n",
       "8512   81.428571   1294.530612 -40.000000   1746.285714  155.333333   \n",
       "8513   77.142857   2213.551020  -1.714286   2686.204082  104.000000   \n",
       "8514  101.142857   4933.551020 -10.750000   7259.937500   88.222222   \n",
       "8515  102.000000    350.000000 -20.000000    588.000000  150.000000   \n",
       "\n",
       "              213  \n",
       "0     4315.560000  \n",
       "1      697.528889  \n",
       "2       68.750000  \n",
       "3     1464.000000  \n",
       "4       43.358025  \n",
       "...           ...  \n",
       "8511     0.037385  \n",
       "8512  4722.666667  \n",
       "8513  3602.666667  \n",
       "8514   202.172840  \n",
       "8515     0.000000  \n",
       "\n",
       "[8516 rows x 213 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"../data_train_frequency.csv\")\n",
    "df_train.drop(columns=[\"Unnamed: 0\"],inplace=True)\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = df_train.iloc[:,1:].values\n",
    "y_train = df_train.iloc[:,0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = MinMaxScaler()\n",
    "x_train = scale.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>203</th>\n",
       "      <th>204</th>\n",
       "      <th>205</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>710.769231</td>\n",
       "      <td>628.0</td>\n",
       "      <td>153.204817</td>\n",
       "      <td>556.0</td>\n",
       "      <td>0.996355</td>\n",
       "      <td>0.207174</td>\n",
       "      <td>459.037295</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>-10.000000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>729.000000</td>\n",
       "      <td>78.250000</td>\n",
       "      <td>3140.437500</td>\n",
       "      <td>127.600000</td>\n",
       "      <td>1041.440000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>968.666667</td>\n",
       "      <td>894.0</td>\n",
       "      <td>266.399867</td>\n",
       "      <td>932.0</td>\n",
       "      <td>0.979352</td>\n",
       "      <td>0.388359</td>\n",
       "      <td>398.464564</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>140.500000</td>\n",
       "      <td>15314.750000</td>\n",
       "      <td>-27.000000</td>\n",
       "      <td>5249.000000</td>\n",
       "      <td>112.285714</td>\n",
       "      <td>8081.632653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>797.000000</td>\n",
       "      <td>780.0</td>\n",
       "      <td>251.329664</td>\n",
       "      <td>794.0</td>\n",
       "      <td>0.260470</td>\n",
       "      <td>-1.002325</td>\n",
       "      <td>340.802438</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>154.285714</td>\n",
       "      <td>1944.489796</td>\n",
       "      <td>18.571429</td>\n",
       "      <td>8070.530612</td>\n",
       "      <td>131.111111</td>\n",
       "      <td>1078.320988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>757.500000</td>\n",
       "      <td>755.0</td>\n",
       "      <td>8.986100</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.048579</td>\n",
       "      <td>-1.449012</td>\n",
       "      <td>412.324324</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>108.500000</td>\n",
       "      <td>6122.750000</td>\n",
       "      <td>46.500000</td>\n",
       "      <td>7081.416667</td>\n",
       "      <td>121.833333</td>\n",
       "      <td>264.305556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>413.909091</td>\n",
       "      <td>409.0</td>\n",
       "      <td>82.344017</td>\n",
       "      <td>426.0</td>\n",
       "      <td>3.023659</td>\n",
       "      <td>10.404884</td>\n",
       "      <td>168.041577</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.022262</td>\n",
       "      <td>11.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.044242</td>\n",
       "      <td>0.044242</td>\n",
       "      <td>-50.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>45.818182</td>\n",
       "      <td>832.330579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2125</th>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1071.250000</td>\n",
       "      <td>1062.0</td>\n",
       "      <td>36.509417</td>\n",
       "      <td>118.0</td>\n",
       "      <td>1.263183</td>\n",
       "      <td>0.543003</td>\n",
       "      <td>364.303573</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>342.857143</td>\n",
       "      <td>2843.265306</td>\n",
       "      <td>205.142857</td>\n",
       "      <td>11207.836735</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>2281.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2126</th>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1196.000000</td>\n",
       "      <td>1202.0</td>\n",
       "      <td>33.839959</td>\n",
       "      <td>102.0</td>\n",
       "      <td>-0.454057</td>\n",
       "      <td>-1.036905</td>\n",
       "      <td>181.876516</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-26.000000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>137.666667</td>\n",
       "      <td>228.555556</td>\n",
       "      <td>87.714286</td>\n",
       "      <td>14282.775510</td>\n",
       "      <td>169.142857</td>\n",
       "      <td>46.693878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2127</th>\n",
       "      <td>3.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>595.600000</td>\n",
       "      <td>590.0</td>\n",
       "      <td>23.734082</td>\n",
       "      <td>82.0</td>\n",
       "      <td>0.371174</td>\n",
       "      <td>-0.657132</td>\n",
       "      <td>137.696567</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-8.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>102.714286</td>\n",
       "      <td>1270.061224</td>\n",
       "      <td>7.285714</td>\n",
       "      <td>361.489796</td>\n",
       "      <td>90.400000</td>\n",
       "      <td>2186.240000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2128</th>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1080.285714</td>\n",
       "      <td>996.0</td>\n",
       "      <td>180.470587</td>\n",
       "      <td>448.0</td>\n",
       "      <td>0.587475</td>\n",
       "      <td>-1.363827</td>\n",
       "      <td>561.988537</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>62.400000</td>\n",
       "      <td>51.840000</td>\n",
       "      <td>-45.200000</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>5002.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2129</th>\n",
       "      <td>3.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>391.250000</td>\n",
       "      <td>390.0</td>\n",
       "      <td>2.569857</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.605786</td>\n",
       "      <td>-0.869886</td>\n",
       "      <td>654.123072</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.044242</td>\n",
       "      <td>0.044242</td>\n",
       "      <td>0.043021</td>\n",
       "      <td>0.043021</td>\n",
       "      <td>0.037385</td>\n",
       "      <td>0.037385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2130 rows × 213 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0     1            2       3           4      5         6          7  \\\n",
       "0     0.0  14.0   710.769231   628.0  153.204817  556.0  0.996355   0.207174   \n",
       "1     0.0  10.0   968.666667   894.0  266.399867  932.0  0.979352   0.388359   \n",
       "2     0.0  11.0   797.000000   780.0  251.329664  794.0  0.260470  -1.002325   \n",
       "3     2.0  13.0   757.500000   755.0    8.986100   26.0  0.048579  -1.449012   \n",
       "4     0.0  23.0   413.909091   409.0   82.344017  426.0  3.023659  10.404884   \n",
       "...   ...   ...          ...     ...         ...    ...       ...        ...   \n",
       "2125  1.0   9.0  1071.250000  1062.0   36.509417  118.0  1.263183   0.543003   \n",
       "2126  1.0   8.0  1196.000000  1202.0   33.839959  102.0 -0.454057  -1.036905   \n",
       "2127  3.0  16.0   595.600000   590.0   23.734082   82.0  0.371174  -0.657132   \n",
       "2128  1.0   8.0  1080.285714   996.0  180.470587  448.0  0.587475  -1.363827   \n",
       "2129  3.0  25.0   391.250000   390.0    2.569857    8.0  0.605786  -0.869886   \n",
       "\n",
       "               8         9  ...       203        204   205   206         207  \\\n",
       "0     459.037295  1.000000  ...  0.928571 -10.000000  10.0   9.0  146.000000   \n",
       "1     398.464564  1.000000  ...  0.600000  64.000000   7.0   7.0  140.500000   \n",
       "2     340.802438  1.000000  ...  1.000000  26.000000   9.0   7.0  154.285714   \n",
       "3     412.324324  1.000000  ...  1.000000  -4.000000  12.0  12.0  108.500000   \n",
       "4     168.041577  0.956522  ...  0.083333   0.022262  11.0  12.0    0.044242   \n",
       "...          ...       ...  ...       ...        ...   ...   ...         ...   \n",
       "2125  364.303573  0.888889  ...  0.777778   0.000000   9.0   8.0  342.857143   \n",
       "2126  181.876516  1.000000  ...  1.000000 -26.000000   8.0   7.0  137.666667   \n",
       "2127  137.696567  1.000000  ...  1.000000  -8.000000  16.0  14.0  102.714286   \n",
       "2128  561.988537  1.000000  ...  1.000000  18.000000   8.0   5.0   62.400000   \n",
       "2129  654.123072  0.400000  ...  0.240000   4.000000   0.0   0.0    0.044242   \n",
       "\n",
       "               208         209           210         211          212  \n",
       "0       729.000000   78.250000   3140.437500  127.600000  1041.440000  \n",
       "1     15314.750000  -27.000000   5249.000000  112.285714  8081.632653  \n",
       "2      1944.489796   18.571429   8070.530612  131.111111  1078.320988  \n",
       "3      6122.750000   46.500000   7081.416667  121.833333   264.305556  \n",
       "4         0.044242  -50.000000      0.000000   45.818182   832.330579  \n",
       "...            ...         ...           ...         ...          ...  \n",
       "2125   2843.265306  205.142857  11207.836735   96.000000  2281.142857  \n",
       "2126    228.555556   87.714286  14282.775510  169.142857    46.693878  \n",
       "2127   1270.061224    7.285714    361.489796   90.400000  2186.240000  \n",
       "2128     51.840000  -45.200000      0.960000  101.000000  5002.000000  \n",
       "2129      0.044242    0.043021      0.043021    0.037385     0.037385  \n",
       "\n",
       "[2130 rows x 213 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv(\"../data_test_frequency.csv\")\n",
    "df_test.drop(columns=[\"Unnamed: 0\"],inplace=True)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = df_test.iloc[:,1:].values\n",
    "y_test = df_test.iloc[:,0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = scale.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (4258, 212)\n",
      "Vallidation: (4258, 212)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train , test_size=0.5, shuffle=True, stratify=y_train, random_state=119)\n",
    "print(f\"Train: {x_train.shape}\")\n",
    "print(f\"Vallidation: {x_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf_clf = RandomForestClassifier(criterion= 'log_loss', max_depth= 5, max_features= 'sqrt', n_estimators= 1000)\n",
    "# ab_clf = AdaBoostClassifier(algorithm= 'SAMME.R', learning_rate= 0.1, n_estimators= 50)\n",
    "# knn_clf = KNeighborsClassifier(algorithm= 'auto', n_neighbors= 5, p= 1, weights= 'uniform')\n",
    "# svc_clf = SVC(C= 100, gamma= 'scale', kernel= 'rbf', probability= True)\n",
    "# xgb_clf = XGBClassifier(gamma= 0,learning_rate= 0.1,max_depth= 5,min_child_weight= 1,n_estimators= 1000)\n",
    "# # dt_clf = DecisionTreeClassifier(criterion= 'entropy',max_depth= 5,max_features= 'sqrt',splitter= 'best')\n",
    "\n",
    "rf_clf = RandomForestClassifier(criterion= 'log_loss', max_depth= 15, max_features= 'sqrt', n_estimators= 40)\n",
    "ab_clf = AdaBoostClassifier(algorithm= 'SAMME.R', learning_rate= 0.1, n_estimators= 61)\n",
    "knn_clf = KNeighborsClassifier(algorithm= 'auto', n_neighbors= 8, p= 1, weights= 'uniform')\n",
    "svc_clf = SVC(C= 15, gamma= 'scale', kernel= 'rbf', probability= True)\n",
    "xgb_clf = XGBClassifier(gamma= 0,learning_rate= 0.05,max_depth= 5,min_child_weight= 1,n_estimators= 1000)\n",
    "lr_clf = LogisticRegression(C= 0.14, max_iter= 20, multi_class= 'auto', n_jobs= -1, penalty= 'l2', solver= 'sag', tol= 0.015)\n",
    "# dt_clf = DecisionTreeClassifier(criterion= 'gini',max_depth= 6,max_features= 'sqrt',splitter= 'best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=0.14, max_iter=20, n_jobs=-1, solver=&#x27;sag&#x27;, tol=0.015)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;LogisticRegression<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression(C=0.14, max_iter=20, n_jobs=-1, solver=&#x27;sag&#x27;, tol=0.015)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(C=0.14, max_iter=20, n_jobs=-1, solver='sag', tol=0.015)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Huấn luyện các mô hình con\n",
    "rf_clf.fit(x_train,y_train)\n",
    "ab_clf.fit(x_train, y_train)\n",
    "knn_clf.fit(x_train, y_train)\n",
    "svc_clf.fit(x_train, y_train)\n",
    "xgb_clf.fit(x_train, y_train)\n",
    "lr_clf.fit(x_train, y_train)\n",
    "# dt_clf.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dự đoán trên tập huấn luyện để tạo đặc trưng mới cho mô hình blending\n",
    "X_train_meta = np.column_stack((\n",
    "    rf_clf.predict_proba(x_val),\n",
    "    ab_clf.predict_proba(x_val),\n",
    "    knn_clf.predict_proba(x_val),\n",
    "    svc_clf.predict_proba(x_val),\n",
    "    xgb_clf.predict_proba(x_val),\n",
    "    lr_clf.predict_proba(x_val),\n",
    "    # dt_clf.predict_proba(x_val)\n",
    "))\n",
    "# Dự đoán trên tập kiểm tra để tạo đặc trưng mới cho mô hình blending\n",
    "X_test_meta = np.column_stack((\n",
    "    rf_clf.predict_proba(x_test),\n",
    "    ab_clf.predict_proba(x_test),\n",
    "    knn_clf.predict_proba(x_test),\n",
    "    svc_clf.predict_proba(x_test),\n",
    "    xgb_clf.predict_proba(x_test),\n",
    "    lr_clf.predict_proba(x_test),\n",
    "    # dt_clf.predict_proba(x_test)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_meta:(4258, 24)\n",
      "X_test_meta:(2130, 24)\n"
     ]
    }
   ],
   "source": [
    "print(f\"X_train_meta:{X_train_meta.shape}\")\n",
    "print(f\"X_test_meta:{X_test_meta.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RF_AFIB',\n",
       " 'RF_SB',\n",
       " 'RF_SR',\n",
       " 'RF_GSVT',\n",
       " 'AB_AFIB',\n",
       " 'AB_SB',\n",
       " 'AB_SR',\n",
       " 'AB_GSVT',\n",
       " 'KNN_AFIB',\n",
       " 'KNN_SB',\n",
       " 'KNN_SR',\n",
       " 'KNN_GSVT',\n",
       " 'SVM_AFIB',\n",
       " 'SVM_SB',\n",
       " 'SVM_SR',\n",
       " 'SVM_GSVT',\n",
       " 'XGB_AFIB',\n",
       " 'XGB_SB',\n",
       " 'XGB_SR',\n",
       " 'XGB_GSVT',\n",
       " 'LR_AFIB',\n",
       " 'LR_SB',\n",
       " 'LR_SR',\n",
       " 'LR_GSVT',\n",
       " 'Rhythm']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_rhythm = ['AFIB','SB','SR','GSVT']\n",
    "name_model = ['RF', 'AB','KNN', 'SVM', 'XGB', 'LR']\n",
    "name_columns = []\n",
    "for i in range(len(name_model)):\n",
    "    for j in range(len(name_rhythm)):\n",
    "        name_columns.append(f\"{name_model[i]}_{name_rhythm[j]}\")\n",
    "name_columns.append(\"Rhythm\")\n",
    "name_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RF_AFIB</th>\n",
       "      <th>RF_SB</th>\n",
       "      <th>RF_SR</th>\n",
       "      <th>RF_GSVT</th>\n",
       "      <th>AB_AFIB</th>\n",
       "      <th>AB_SB</th>\n",
       "      <th>AB_SR</th>\n",
       "      <th>AB_GSVT</th>\n",
       "      <th>KNN_AFIB</th>\n",
       "      <th>KNN_SB</th>\n",
       "      <th>...</th>\n",
       "      <th>SVM_GSVT</th>\n",
       "      <th>XGB_AFIB</th>\n",
       "      <th>XGB_SB</th>\n",
       "      <th>XGB_SR</th>\n",
       "      <th>XGB_GSVT</th>\n",
       "      <th>LR_AFIB</th>\n",
       "      <th>LR_SB</th>\n",
       "      <th>LR_SR</th>\n",
       "      <th>LR_GSVT</th>\n",
       "      <th>Rhythm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.925000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.342754</td>\n",
       "      <td>0.162022</td>\n",
       "      <td>0.295954</td>\n",
       "      <td>0.199271</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001770</td>\n",
       "      <td>9.999334e-01</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>2.535906e-05</td>\n",
       "      <td>0.887538</td>\n",
       "      <td>0.019085</td>\n",
       "      <td>0.082841</td>\n",
       "      <td>0.010536</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.185802</td>\n",
       "      <td>0.453266</td>\n",
       "      <td>0.225418</td>\n",
       "      <td>0.135515</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000409</td>\n",
       "      <td>2.972089e-07</td>\n",
       "      <td>0.999991</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>8.302450e-07</td>\n",
       "      <td>0.000344</td>\n",
       "      <td>0.970523</td>\n",
       "      <td>0.029028</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.267233</td>\n",
       "      <td>0.255194</td>\n",
       "      <td>0.310343</td>\n",
       "      <td>0.167229</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004874</td>\n",
       "      <td>4.766035e-05</td>\n",
       "      <td>0.009696</td>\n",
       "      <td>0.990108</td>\n",
       "      <td>1.483133e-04</td>\n",
       "      <td>0.011471</td>\n",
       "      <td>0.723026</td>\n",
       "      <td>0.259602</td>\n",
       "      <td>0.005901</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.185802</td>\n",
       "      <td>0.453266</td>\n",
       "      <td>0.225418</td>\n",
       "      <td>0.135515</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000865</td>\n",
       "      <td>3.694026e-07</td>\n",
       "      <td>0.999971</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>1.501695e-05</td>\n",
       "      <td>0.000788</td>\n",
       "      <td>0.902295</td>\n",
       "      <td>0.096337</td>\n",
       "      <td>0.000580</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.389210</td>\n",
       "      <td>0.074523</td>\n",
       "      <td>0.184868</td>\n",
       "      <td>0.351399</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031602</td>\n",
       "      <td>9.999797e-01</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>1.739992e-05</td>\n",
       "      <td>0.763083</td>\n",
       "      <td>0.000279</td>\n",
       "      <td>0.017935</td>\n",
       "      <td>0.218703</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4253</th>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.303409</td>\n",
       "      <td>0.084845</td>\n",
       "      <td>0.217602</td>\n",
       "      <td>0.394143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.994393</td>\n",
       "      <td>9.578939e-05</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>9.998820e-01</td>\n",
       "      <td>0.022291</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.030211</td>\n",
       "      <td>0.947425</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4254</th>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.313780</td>\n",
       "      <td>0.086193</td>\n",
       "      <td>0.218636</td>\n",
       "      <td>0.381390</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.974624</td>\n",
       "      <td>3.260745e-03</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.000389</td>\n",
       "      <td>9.962105e-01</td>\n",
       "      <td>0.119370</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.033460</td>\n",
       "      <td>0.847155</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4255</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.186192</td>\n",
       "      <td>0.444480</td>\n",
       "      <td>0.229289</td>\n",
       "      <td>0.140039</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.875</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000896</td>\n",
       "      <td>2.218669e-06</td>\n",
       "      <td>0.999975</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>2.238016e-06</td>\n",
       "      <td>0.002777</td>\n",
       "      <td>0.823877</td>\n",
       "      <td>0.171704</td>\n",
       "      <td>0.001642</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4256</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.194676</td>\n",
       "      <td>0.437216</td>\n",
       "      <td>0.224288</td>\n",
       "      <td>0.143820</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002803</td>\n",
       "      <td>6.379285e-06</td>\n",
       "      <td>0.999828</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>1.539085e-05</td>\n",
       "      <td>0.002614</td>\n",
       "      <td>0.935160</td>\n",
       "      <td>0.061780</td>\n",
       "      <td>0.000446</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4257</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.185802</td>\n",
       "      <td>0.453266</td>\n",
       "      <td>0.225418</td>\n",
       "      <td>0.135515</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001202</td>\n",
       "      <td>1.517404e-07</td>\n",
       "      <td>0.999989</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>1.127951e-06</td>\n",
       "      <td>0.001589</td>\n",
       "      <td>0.785315</td>\n",
       "      <td>0.211150</td>\n",
       "      <td>0.001946</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4258 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       RF_AFIB  RF_SB  RF_SR   RF_GSVT   AB_AFIB     AB_SB     AB_SR  \\\n",
       "0     0.925000  0.000  0.050  0.025000  0.342754  0.162022  0.295954   \n",
       "1     0.000000  1.000  0.000  0.000000  0.185802  0.453266  0.225418   \n",
       "2     0.000000  0.400  0.600  0.000000  0.267233  0.255194  0.310343   \n",
       "3     0.000000  1.000  0.000  0.000000  0.185802  0.453266  0.225418   \n",
       "4     0.975000  0.000  0.000  0.025000  0.389210  0.074523  0.184868   \n",
       "...        ...    ...    ...       ...       ...       ...       ...   \n",
       "4253  0.033333  0.000  0.000  0.966667  0.303409  0.084845  0.217602   \n",
       "4254  0.125000  0.000  0.025  0.850000  0.313780  0.086193  0.218636   \n",
       "4255  0.000000  0.975  0.025  0.000000  0.186192  0.444480  0.229289   \n",
       "4256  0.000000  1.000  0.000  0.000000  0.194676  0.437216  0.224288   \n",
       "4257  0.000000  1.000  0.000  0.000000  0.185802  0.453266  0.225418   \n",
       "\n",
       "       AB_GSVT  KNN_AFIB  KNN_SB  ...  SVM_GSVT      XGB_AFIB    XGB_SB  \\\n",
       "0     0.199271       1.0   0.000  ...  0.001770  9.999334e-01  0.000020   \n",
       "1     0.135515       0.0   1.000  ...  0.000409  2.972089e-07  0.999991   \n",
       "2     0.167229       0.0   0.250  ...  0.004874  4.766035e-05  0.009696   \n",
       "3     0.135515       0.0   1.000  ...  0.000865  3.694026e-07  0.999971   \n",
       "4     0.351399       1.0   0.000  ...  0.031602  9.999797e-01  0.000001   \n",
       "...        ...       ...     ...  ...       ...           ...       ...   \n",
       "4253  0.394143       0.0   0.000  ...  0.994393  9.578939e-05  0.000008   \n",
       "4254  0.381390       0.0   0.000  ...  0.974624  3.260745e-03  0.000140   \n",
       "4255  0.140039       0.0   0.875  ...  0.000896  2.218669e-06  0.999975   \n",
       "4256  0.143820       0.0   1.000  ...  0.002803  6.379285e-06  0.999828   \n",
       "4257  0.135515       0.0   1.000  ...  0.001202  1.517404e-07  0.999989   \n",
       "\n",
       "        XGB_SR      XGB_GSVT   LR_AFIB     LR_SB     LR_SR   LR_GSVT  Rhythm  \n",
       "0     0.000022  2.535906e-05  0.887538  0.019085  0.082841  0.010536     1.0  \n",
       "1     0.000008  8.302450e-07  0.000344  0.970523  0.029028  0.000105     2.0  \n",
       "2     0.990108  1.483133e-04  0.011471  0.723026  0.259602  0.005901     0.0  \n",
       "3     0.000014  1.501695e-05  0.000788  0.902295  0.096337  0.000580     3.0  \n",
       "4     0.000002  1.739992e-05  0.763083  0.000279  0.017935  0.218703     2.0  \n",
       "...        ...           ...       ...       ...       ...       ...     ...  \n",
       "4253  0.000014  9.998820e-01  0.022291  0.000073  0.030211  0.947425     2.0  \n",
       "4254  0.000389  9.962105e-01  0.119370  0.000015  0.033460  0.847155     3.0  \n",
       "4255  0.000021  2.238016e-06  0.002777  0.823877  0.171704  0.001642     0.0  \n",
       "4256  0.000151  1.539085e-05  0.002614  0.935160  0.061780  0.000446     1.0  \n",
       "4257  0.000010  1.127951e-06  0.001589  0.785315  0.211150  0.001946     1.0  \n",
       "\n",
       "[4258 rows x 25 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_connect_train_meta = pd.concat([pd.DataFrame(X_train_meta), pd.DataFrame(y_train)],axis=1)\n",
    "df_connect_test_meta = pd.concat([pd.DataFrame(X_test_meta), pd.DataFrame(y_test)],axis=1)\n",
    "df_connect_train_meta.columns = name_columns\n",
    "df_connect_test_meta.columns = name_columns\n",
    "df_connect_train_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 54 candidates, totalling 162 fits\n",
      "[CV 1/3] END criterion=gini, max_depth=3, max_features=auto, splitter=best;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=gini, max_depth=3, max_features=auto, splitter=best;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=gini, max_depth=3, max_features=auto, splitter=best;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=gini, max_depth=3, max_features=auto, splitter=random;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=gini, max_depth=3, max_features=auto, splitter=random;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=gini, max_depth=3, max_features=auto, splitter=random;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=gini, max_depth=3, max_features=sqrt, splitter=best;, score=(train=0.963, test=0.951) total time=   0.0s\n",
      "[CV 2/3] END criterion=gini, max_depth=3, max_features=sqrt, splitter=best;, score=(train=0.953, test=0.953) total time=   0.0s\n",
      "[CV 3/3] END criterion=gini, max_depth=3, max_features=sqrt, splitter=best;, score=(train=0.953, test=0.937) total time=   0.0s\n",
      "[CV 1/3] END criterion=gini, max_depth=3, max_features=sqrt, splitter=random;, score=(train=0.938, test=0.935) total time=   0.0s\n",
      "[CV 2/3] END criterion=gini, max_depth=3, max_features=sqrt, splitter=random;, score=(train=0.928, test=0.942) total time=   0.0s\n",
      "[CV 3/3] END criterion=gini, max_depth=3, max_features=sqrt, splitter=random;, score=(train=0.893, test=0.891) total time=   0.0s\n",
      "[CV 1/3] END criterion=gini, max_depth=3, max_features=log2, splitter=best;, score=(train=0.950, test=0.940) total time=   0.0s\n",
      "[CV 2/3] END criterion=gini, max_depth=3, max_features=log2, splitter=best;, score=(train=0.934, test=0.929) total time=   0.0s\n",
      "[CV 3/3] END criterion=gini, max_depth=3, max_features=log2, splitter=best;, score=(train=0.955, test=0.951) total time=   0.0s\n",
      "[CV 1/3] END criterion=gini, max_depth=3, max_features=log2, splitter=random;, score=(train=0.938, test=0.934) total time=   0.0s\n",
      "[CV 2/3] END criterion=gini, max_depth=3, max_features=log2, splitter=random;, score=(train=0.953, test=0.959) total time=   0.0s\n",
      "[CV 3/3] END criterion=gini, max_depth=3, max_features=log2, splitter=random;, score=(train=0.946, test=0.936) total time=   0.0s\n",
      "[CV 1/3] END criterion=gini, max_depth=4, max_features=auto, splitter=best;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=gini, max_depth=4, max_features=auto, splitter=best;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=gini, max_depth=4, max_features=auto, splitter=best;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=gini, max_depth=4, max_features=auto, splitter=random;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=gini, max_depth=4, max_features=auto, splitter=random;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=gini, max_depth=4, max_features=auto, splitter=random;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=gini, max_depth=4, max_features=sqrt, splitter=best;, score=(train=0.947, test=0.934) total time=   0.0s\n",
      "[CV 2/3] END criterion=gini, max_depth=4, max_features=sqrt, splitter=best;, score=(train=0.960, test=0.963) total time=   0.0s\n",
      "[CV 3/3] END criterion=gini, max_depth=4, max_features=sqrt, splitter=best;, score=(train=0.956, test=0.940) total time=   0.0s\n",
      "[CV 1/3] END criterion=gini, max_depth=4, max_features=sqrt, splitter=random;, score=(train=0.958, test=0.954) total time=   0.0s\n",
      "[CV 2/3] END criterion=gini, max_depth=4, max_features=sqrt, splitter=random;, score=(train=0.923, test=0.932) total time=   0.0s\n",
      "[CV 3/3] END criterion=gini, max_depth=4, max_features=sqrt, splitter=random;, score=(train=0.954, test=0.939) total time=   0.0s\n",
      "[CV 1/3] END criterion=gini, max_depth=4, max_features=log2, splitter=best;, score=(train=0.961, test=0.937) total time=   0.0s\n",
      "[CV 2/3] END criterion=gini, max_depth=4, max_features=log2, splitter=best;, score=(train=0.958, test=0.957) total time=   0.0s\n",
      "[CV 3/3] END criterion=gini, max_depth=4, max_features=log2, splitter=best;, score=(train=0.956, test=0.935) total time=   0.0s\n",
      "[CV 1/3] END criterion=gini, max_depth=4, max_features=log2, splitter=random;, score=(train=0.949, test=0.945) total time=   0.0s\n",
      "[CV 2/3] END criterion=gini, max_depth=4, max_features=log2, splitter=random;, score=(train=0.953, test=0.953) total time=   0.0s\n",
      "[CV 3/3] END criterion=gini, max_depth=4, max_features=log2, splitter=random;, score=(train=0.957, test=0.946) total time=   0.0s\n",
      "[CV 1/3] END criterion=gini, max_depth=5, max_features=auto, splitter=best;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=gini, max_depth=5, max_features=auto, splitter=best;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=gini, max_depth=5, max_features=auto, splitter=best;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=gini, max_depth=5, max_features=auto, splitter=random;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=gini, max_depth=5, max_features=auto, splitter=random;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=gini, max_depth=5, max_features=auto, splitter=random;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=gini, max_depth=5, max_features=sqrt, splitter=best;, score=(train=0.969, test=0.945) total time=   0.0s\n",
      "[CV 2/3] END criterion=gini, max_depth=5, max_features=sqrt, splitter=best;, score=(train=0.959, test=0.951) total time=   0.0s\n",
      "[CV 3/3] END criterion=gini, max_depth=5, max_features=sqrt, splitter=best;, score=(train=0.970, test=0.946) total time=   0.0s\n",
      "[CV 1/3] END criterion=gini, max_depth=5, max_features=sqrt, splitter=random;, score=(train=0.958, test=0.947) total time=   0.0s\n",
      "[CV 2/3] END criterion=gini, max_depth=5, max_features=sqrt, splitter=random;, score=(train=0.956, test=0.959) total time=   0.0s\n",
      "[CV 3/3] END criterion=gini, max_depth=5, max_features=sqrt, splitter=random;, score=(train=0.905, test=0.895) total time=   0.0s\n",
      "[CV 1/3] END criterion=gini, max_depth=5, max_features=log2, splitter=best;, score=(train=0.968, test=0.948) total time=   0.0s\n",
      "[CV 2/3] END criterion=gini, max_depth=5, max_features=log2, splitter=best;, score=(train=0.962, test=0.965) total time=   0.0s\n",
      "[CV 3/3] END criterion=gini, max_depth=5, max_features=log2, splitter=best;, score=(train=0.962, test=0.949) total time=   0.0s\n",
      "[CV 1/3] END criterion=gini, max_depth=5, max_features=log2, splitter=random;, score=(train=0.950, test=0.942) total time=   0.0s\n",
      "[CV 2/3] END criterion=gini, max_depth=5, max_features=log2, splitter=random;, score=(train=0.951, test=0.946) total time=   0.0s\n",
      "[CV 3/3] END criterion=gini, max_depth=5, max_features=log2, splitter=random;, score=(train=0.942, test=0.940) total time=   0.0s\n",
      "[CV 1/3] END criterion=entropy, max_depth=3, max_features=auto, splitter=best;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=entropy, max_depth=3, max_features=auto, splitter=best;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=entropy, max_depth=3, max_features=auto, splitter=best;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=entropy, max_depth=3, max_features=auto, splitter=random;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=entropy, max_depth=3, max_features=auto, splitter=random;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=entropy, max_depth=3, max_features=auto, splitter=random;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=entropy, max_depth=3, max_features=sqrt, splitter=best;, score=(train=0.894, test=0.873) total time=   0.0s\n",
      "[CV 2/3] END criterion=entropy, max_depth=3, max_features=sqrt, splitter=best;, score=(train=0.948, test=0.953) total time=   0.0s\n",
      "[CV 3/3] END criterion=entropy, max_depth=3, max_features=sqrt, splitter=best;, score=(train=0.956, test=0.942) total time=   0.0s\n",
      "[CV 1/3] END criterion=entropy, max_depth=3, max_features=sqrt, splitter=random;, score=(train=0.956, test=0.952) total time=   0.0s\n",
      "[CV 2/3] END criterion=entropy, max_depth=3, max_features=sqrt, splitter=random;, score=(train=0.916, test=0.928) total time=   0.0s\n",
      "[CV 3/3] END criterion=entropy, max_depth=3, max_features=sqrt, splitter=random;, score=(train=0.916, test=0.911) total time=   0.0s\n",
      "[CV 1/3] END criterion=entropy, max_depth=3, max_features=log2, splitter=best;, score=(train=0.954, test=0.953) total time=   0.0s\n",
      "[CV 2/3] END criterion=entropy, max_depth=3, max_features=log2, splitter=best;, score=(train=0.955, test=0.961) total time=   0.0s\n",
      "[CV 3/3] END criterion=entropy, max_depth=3, max_features=log2, splitter=best;, score=(train=0.945, test=0.935) total time=   0.0s\n",
      "[CV 1/3] END criterion=entropy, max_depth=3, max_features=log2, splitter=random;, score=(train=0.932, test=0.934) total time=   0.0s\n",
      "[CV 2/3] END criterion=entropy, max_depth=3, max_features=log2, splitter=random;, score=(train=0.910, test=0.912) total time=   0.0s\n",
      "[CV 3/3] END criterion=entropy, max_depth=3, max_features=log2, splitter=random;, score=(train=0.893, test=0.879) total time=   0.0s\n",
      "[CV 1/3] END criterion=entropy, max_depth=4, max_features=auto, splitter=best;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=entropy, max_depth=4, max_features=auto, splitter=best;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=entropy, max_depth=4, max_features=auto, splitter=best;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=entropy, max_depth=4, max_features=auto, splitter=random;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=entropy, max_depth=4, max_features=auto, splitter=random;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=entropy, max_depth=4, max_features=auto, splitter=random;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=entropy, max_depth=4, max_features=sqrt, splitter=best;, score=(train=0.961, test=0.950) total time=   0.0s\n",
      "[CV 2/3] END criterion=entropy, max_depth=4, max_features=sqrt, splitter=best;, score=(train=0.955, test=0.959) total time=   0.0s\n",
      "[CV 3/3] END criterion=entropy, max_depth=4, max_features=sqrt, splitter=best;, score=(train=0.958, test=0.940) total time=   0.0s\n",
      "[CV 1/3] END criterion=entropy, max_depth=4, max_features=sqrt, splitter=random;, score=(train=0.939, test=0.938) total time=   0.0s\n",
      "[CV 2/3] END criterion=entropy, max_depth=4, max_features=sqrt, splitter=random;, score=(train=0.946, test=0.953) total time=   0.0s\n",
      "[CV 3/3] END criterion=entropy, max_depth=4, max_features=sqrt, splitter=random;, score=(train=0.958, test=0.952) total time=   0.0s\n",
      "[CV 1/3] END criterion=entropy, max_depth=4, max_features=log2, splitter=best;, score=(train=0.957, test=0.944) total time=   0.0s\n",
      "[CV 2/3] END criterion=entropy, max_depth=4, max_features=log2, splitter=best;, score=(train=0.958, test=0.962) total time=   0.0s\n",
      "[CV 3/3] END criterion=entropy, max_depth=4, max_features=log2, splitter=best;, score=(train=0.956, test=0.939) total time=   0.0s\n",
      "[CV 1/3] END criterion=entropy, max_depth=4, max_features=log2, splitter=random;, score=(train=0.936, test=0.929) total time=   0.0s\n",
      "[CV 2/3] END criterion=entropy, max_depth=4, max_features=log2, splitter=random;, score=(train=0.954, test=0.958) total time=   0.0s\n",
      "[CV 3/3] END criterion=entropy, max_depth=4, max_features=log2, splitter=random;, score=(train=0.895, test=0.876) total time=   0.0s\n",
      "[CV 1/3] END criterion=entropy, max_depth=5, max_features=auto, splitter=best;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=entropy, max_depth=5, max_features=auto, splitter=best;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=entropy, max_depth=5, max_features=auto, splitter=best;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=entropy, max_depth=5, max_features=auto, splitter=random;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=entropy, max_depth=5, max_features=auto, splitter=random;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=entropy, max_depth=5, max_features=auto, splitter=random;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=entropy, max_depth=5, max_features=sqrt, splitter=best;, score=(train=0.960, test=0.944) total time=   0.0s\n",
      "[CV 2/3] END criterion=entropy, max_depth=5, max_features=sqrt, splitter=best;, score=(train=0.955, test=0.958) total time=   0.0s\n",
      "[CV 3/3] END criterion=entropy, max_depth=5, max_features=sqrt, splitter=best;, score=(train=0.961, test=0.939) total time=   0.0s\n",
      "[CV 1/3] END criterion=entropy, max_depth=5, max_features=sqrt, splitter=random;, score=(train=0.955, test=0.945) total time=   0.0s\n",
      "[CV 2/3] END criterion=entropy, max_depth=5, max_features=sqrt, splitter=random;, score=(train=0.951, test=0.958) total time=   0.0s\n",
      "[CV 3/3] END criterion=entropy, max_depth=5, max_features=sqrt, splitter=random;, score=(train=0.952, test=0.939) total time=   0.0s\n",
      "[CV 1/3] END criterion=entropy, max_depth=5, max_features=log2, splitter=best;, score=(train=0.968, test=0.947) total time=   0.0s\n",
      "[CV 2/3] END criterion=entropy, max_depth=5, max_features=log2, splitter=best;, score=(train=0.961, test=0.963) total time=   0.0s\n",
      "[CV 3/3] END criterion=entropy, max_depth=5, max_features=log2, splitter=best;, score=(train=0.960, test=0.946) total time=   0.0s\n",
      "[CV 1/3] END criterion=entropy, max_depth=5, max_features=log2, splitter=random;, score=(train=0.957, test=0.942) total time=   0.0s\n",
      "[CV 2/3] END criterion=entropy, max_depth=5, max_features=log2, splitter=random;, score=(train=0.955, test=0.957) total time=   0.0s\n",
      "[CV 3/3] END criterion=entropy, max_depth=5, max_features=log2, splitter=random;, score=(train=0.953, test=0.942) total time=   0.0s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=3, max_features=auto, splitter=best;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=3, max_features=auto, splitter=best;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=3, max_features=auto, splitter=best;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=3, max_features=auto, splitter=random;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=3, max_features=auto, splitter=random;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=3, max_features=auto, splitter=random;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=3, max_features=sqrt, splitter=best;, score=(train=0.943, test=0.935) total time=   0.0s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=3, max_features=sqrt, splitter=best;, score=(train=0.942, test=0.946) total time=   0.0s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=3, max_features=sqrt, splitter=best;, score=(train=0.946, test=0.926) total time=   0.0s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=3, max_features=sqrt, splitter=random;, score=(train=0.954, test=0.948) total time=   0.0s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=3, max_features=sqrt, splitter=random;, score=(train=0.759, test=0.762) total time=   0.0s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=3, max_features=sqrt, splitter=random;, score=(train=0.920, test=0.910) total time=   0.0s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=3, max_features=log2, splitter=best;, score=(train=0.949, test=0.950) total time=   0.0s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=3, max_features=log2, splitter=best;, score=(train=0.944, test=0.951) total time=   0.0s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=3, max_features=log2, splitter=best;, score=(train=0.955, test=0.949) total time=   0.0s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=3, max_features=log2, splitter=random;, score=(train=0.958, test=0.954) total time=   0.0s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=3, max_features=log2, splitter=random;, score=(train=0.932, test=0.940) total time=   0.0s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=3, max_features=log2, splitter=random;, score=(train=0.869, test=0.856) total time=   0.0s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=4, max_features=auto, splitter=best;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=4, max_features=auto, splitter=best;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=4, max_features=auto, splitter=best;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=4, max_features=auto, splitter=random;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=4, max_features=auto, splitter=random;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=4, max_features=auto, splitter=random;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=4, max_features=sqrt, splitter=best;, score=(train=0.953, test=0.944) total time=   0.0s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=4, max_features=sqrt, splitter=best;, score=(train=0.952, test=0.946) total time=   0.0s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=4, max_features=sqrt, splitter=best;, score=(train=0.956, test=0.934) total time=   0.0s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=4, max_features=sqrt, splitter=random;, score=(train=0.929, test=0.934) total time=   0.0s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=4, max_features=sqrt, splitter=random;, score=(train=0.954, test=0.961) total time=   0.0s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=4, max_features=sqrt, splitter=random;, score=(train=0.944, test=0.939) total time=   0.0s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=4, max_features=log2, splitter=best;, score=(train=0.958, test=0.944) total time=   0.0s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=4, max_features=log2, splitter=best;, score=(train=0.946, test=0.948) total time=   0.0s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=4, max_features=log2, splitter=best;, score=(train=0.956, test=0.947) total time=   0.0s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=4, max_features=log2, splitter=random;, score=(train=0.949, test=0.946) total time=   0.0s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=4, max_features=log2, splitter=random;, score=(train=0.955, test=0.963) total time=   0.0s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=4, max_features=log2, splitter=random;, score=(train=0.885, test=0.856) total time=   0.0s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=5, max_features=auto, splitter=best;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=5, max_features=auto, splitter=best;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=5, max_features=auto, splitter=best;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=5, max_features=auto, splitter=random;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=5, max_features=auto, splitter=random;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=5, max_features=auto, splitter=random;, score=(train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=5, max_features=sqrt, splitter=best;, score=(train=0.965, test=0.948) total time=   0.0s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=5, max_features=sqrt, splitter=best;, score=(train=0.952, test=0.946) total time=   0.0s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=5, max_features=sqrt, splitter=best;, score=(train=0.957, test=0.939) total time=   0.0s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=5, max_features=sqrt, splitter=random;, score=(train=0.961, test=0.958) total time=   0.0s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=5, max_features=sqrt, splitter=random;, score=(train=0.957, test=0.958) total time=   0.0s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=5, max_features=sqrt, splitter=random;, score=(train=0.959, test=0.952) total time=   0.0s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=5, max_features=log2, splitter=best;, score=(train=0.965, test=0.944) total time=   0.0s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=5, max_features=log2, splitter=best;, score=(train=0.961, test=0.954) total time=   0.0s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=5, max_features=log2, splitter=best;, score=(train=0.958, test=0.929) total time=   0.0s\n",
      "[CV 1/3] END criterion=log_loss, max_depth=5, max_features=log2, splitter=random;, score=(train=0.949, test=0.941) total time=   0.0s\n",
      "[CV 2/3] END criterion=log_loss, max_depth=5, max_features=log2, splitter=random;, score=(train=0.943, test=0.948) total time=   0.0s\n",
      "[CV 3/3] END criterion=log_loss, max_depth=5, max_features=log2, splitter=random;, score=(train=0.952, test=0.943) total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_validation.py:542: FitFailedWarning: \n",
      "54 fits failed out of a total of 162.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "54 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_validation.py\", line 890, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\base.py\", line 1344, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of DecisionTreeClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.94692255 0.922496   0.93987782 0.9429331\n",
      "        nan        nan 0.94575198 0.94151902 0.94293244 0.94833332\n",
      "        nan        nan 0.94739369 0.93376857 0.95373552 0.94293111\n",
      "        nan        nan 0.92227566 0.93024381 0.94950603 0.90840174\n",
      "        nan        nan 0.9497416  0.94763025 0.94833348 0.92108789\n",
      "        nan        nan 0.9469242  0.94739369 0.95185643 0.94668979\n",
      "        nan        nan 0.93565081 0.87316235 0.94997651 0.9168536\n",
      "        nan        nan 0.94128643 0.94481236 0.94621948 0.92155373\n",
      "        nan        nan 0.94457414 0.95584737 0.9424608  0.94387107]\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the train scores are non-finite: [       nan        nan 0.95620085 0.91980016 0.94621936 0.94574822\n",
      "        nan        nan 0.95420302 0.94504619 0.9580792  0.9529117\n",
      "        nan        nan 0.96582936 0.93976264 0.96406809 0.94762827\n",
      "        nan        nan 0.93247551 0.92919525 0.95138596 0.9115806\n",
      "        nan        nan 0.95807928 0.94809668 0.9572569  0.92848846\n",
      "        nan        nan 0.95866622 0.95291244 0.96301134 0.95502611\n",
      "        nan        nan 0.94351797 0.87765107 0.94962419 0.91956765\n",
      "        nan        nan 0.95361667 0.94245965 0.95338246 0.92978153\n",
      "        nan        nan 0.95819719 0.95890108 0.96124983 0.94774552]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "model = DecisionTreeClassifier()\n",
    "params = {\n",
    "    'criterion':['gini', 'entropy', 'log_loss'],\n",
    "    'max_depth': [3,4,5],\n",
    "    'splitter':['best', 'random'],\n",
    "    'max_features':['auto', 'sqrt', 'log2']\n",
    "}\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=params, cv=3, verbose=5, return_train_score=True,refit=True)\n",
    "grid_model = grid_search.fit(X_train_meta,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_test = grid_model.predict(X_test_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'log_loss',\n",
       " 'max_depth': 5,\n",
       " 'max_features': 'sqrt',\n",
       " 'splitter': 'random'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9558473698663675"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_model.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay,multilabel_confusion_matrix,f1_score,precision_score,accuracy_score,recall_score,precision_recall_fscore_support\n",
    "def evaluation_test(y,y_pred):\n",
    "    cm = confusion_matrix(y,y_pred)\n",
    "    disp = ConfusionMatrixDisplay(cm,display_labels=['AFIB','SB','SR','GSVT'])\n",
    "    disp.plot()\n",
    "    plt.show()\n",
    "    n_classes = len(cm)\n",
    "    result = []\n",
    "    for c in range(n_classes):\n",
    "        tp = cm[c,c]\n",
    "        fp = sum(cm[:,c]) - cm[c,c]\n",
    "        fn = sum(cm[c,:]) - cm[c,c]\n",
    "        tn = sum(np.delete(sum(cm)-cm[c,:],c))\n",
    "        acc = (tp+tn) / (tp+fn+tn+fp)\n",
    "        recall = tp/(tp+fn)\n",
    "        precision = tp/(tp+fp)\n",
    "        specificity = tn/(tn+fp)\n",
    "        f1_score = 2*((precision*recall)/(precision+recall))\n",
    "        if c+1 == 1:\n",
    "            Rhythm = 'AFIB'\n",
    "        elif c+1 == 2:\n",
    "            Rhythm = 'SB'\n",
    "        elif c+1 == 3:\n",
    "            Rhythm = 'SR'\n",
    "        else:\n",
    "            Rhythm = 'GSVT'\n",
    "        result.append([Rhythm,acc,recall,precision,f1_score,specificity])\n",
    "    print(f\"Accuracy: {accuracy_score(y,y_pred)}\")\n",
    "    p_macro,r_macro,f_macro,support_macro = precision_recall_fscore_support(y,y_pred,average='macro')\n",
    "    p_micro,r_micro,f_micro,support_micro = precision_recall_fscore_support(y,y_pred,average='micro')\n",
    "    p_weighted,r_weighted,f_weighted,support_weighted = precision_recall_fscore_support(y,y_pred,average='weighted')\n",
    "    result.append(['macro avg',None,f_macro,p_macro,r_macro,None])\n",
    "    result.append(['micro avg',None,f_micro,p_micro,r_micro,None])\n",
    "    result.append(['weighted avg',None,f_weighted,p_weighted,r_weighted,None])\n",
    "    return result\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9549295774647887\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rhythm Group</th>\n",
       "      <th>ACC</th>\n",
       "      <th>F1-score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>specificity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AFIB</td>\n",
       "      <td>0.963380</td>\n",
       "      <td>0.907865</td>\n",
       "      <td>0.916100</td>\n",
       "      <td>0.911964</td>\n",
       "      <td>0.978042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SB</td>\n",
       "      <td>0.989671</td>\n",
       "      <td>0.985861</td>\n",
       "      <td>0.985861</td>\n",
       "      <td>0.985861</td>\n",
       "      <td>0.991864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SR</td>\n",
       "      <td>0.988732</td>\n",
       "      <td>0.977528</td>\n",
       "      <td>0.968820</td>\n",
       "      <td>0.973154</td>\n",
       "      <td>0.991691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GSVT</td>\n",
       "      <td>0.968075</td>\n",
       "      <td>0.926407</td>\n",
       "      <td>0.926407</td>\n",
       "      <td>0.926407</td>\n",
       "      <td>0.979616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>macro avg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.949347</td>\n",
       "      <td>0.949297</td>\n",
       "      <td>0.949415</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>micro avg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.954930</td>\n",
       "      <td>0.954930</td>\n",
       "      <td>0.954930</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>weighted avg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.954872</td>\n",
       "      <td>0.954831</td>\n",
       "      <td>0.954930</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rhythm Group       ACC  F1-score  Precision    Recall  specificity\n",
       "0          AFIB  0.963380  0.907865   0.916100  0.911964     0.978042\n",
       "1            SB  0.989671  0.985861   0.985861  0.985861     0.991864\n",
       "2            SR  0.988732  0.977528   0.968820  0.973154     0.991691\n",
       "3          GSVT  0.968075  0.926407   0.926407  0.926407     0.979616\n",
       "4     macro avg       NaN  0.949347   0.949297  0.949415          NaN\n",
       "5     micro avg       NaN  0.954930   0.954930  0.954930          NaN\n",
       "6  weighted avg       NaN  0.954872   0.954831  0.954930          NaN"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_test = evaluation_test(y_test,result_test)\n",
    "df_evaluation_test = pd.DataFrame(data=evaluation_test,columns=[\"Rhythm Group\",\"ACC\",\"F1-score\",\"Precision\",\"Recall\",\"specificity\"])\n",
    "df_evaluation_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_evaluation_test.to_csv(\"./Result/Blending_DT.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
